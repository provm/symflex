diff -rupN /home/prafull/Desktop/qemu-2.9.0/balloon.c /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/balloon.c
--- /home/prafull/Desktop/qemu-2.9.0/balloon.c	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/balloon.c	2018-05-28 13:06:05.000000000 +0530
@@ -35,6 +35,11 @@
 #include "qapi/qmp/qjson.h"
 
 static QEMUBalloonEvent *balloon_event_fn;
+/* Added by Bhavesh Singh. 2017.06.02. Begin add */
+static QEMUSSDBalloonEvent *ssd_balloon_event_fn;
+static void *ssd_balloon_opaque;
+/* Added by Bhavesh Singh. 2017.06.02. End add */
+
 static QEMUBalloonStatus *balloon_stat_fn;
 static void *balloon_opaque;
 static bool balloon_inhibited;
@@ -116,3 +121,55 @@ void qmp_balloon(int64_t target, Error *
     trace_balloon_event(balloon_opaque, target);
     balloon_event_fn(balloon_opaque, target);
 }
+
+
+/* Added by Bhavesh Singh. 2017.06.02. Begin add */
+void qmp_ssd_balloon(int64_t target, Error **errp)
+{
+//    if (!have_balloon(errp)) {
+//        return;
+//    }
+
+/*  Added by Muhammed Unais P.  2017-07-25.  Begin add.  */  
+    printf("\n\tFn callled : qmp_ssd_balloon [ballon.c]\n");
+    printf("\tIt Checks 'ssd_balloon_event_fn' is declared. \n");
+    printf("\tIf it OK, it calls . ssd_balloon_event_fn(ssd_balloon_opaque, target);\n");
+/*  Added by Muhammed Unais P.  2017-07-25.  End add.    */
+
+    if(!ssd_balloon_event_fn)
+    {
+        printf("SSD ballon event not registered \n");
+        return;
+    }
+    ssd_balloon_event_fn(ssd_balloon_opaque, target);
+}
+int qemu_add_ssd_balloon_handler(QEMUSSDBalloonEvent *event_func, void *opaque)
+{
+
+/*  Added by Muhammed Unais P.  2017-07-25.  Begin add.  */  
+    printf("\nFn callled : qemu_add_ssd_balloon_handler\n");
+/*  Added by Muhammed Unais P.  2017-07-25.  End add.    */
+
+    if (ssd_balloon_event_fn || ssd_balloon_opaque) {
+        /* We're already registered one balloon handler.  How many can
+         * a guest really have?
+         */
+        return -1;
+    }
+    ssd_balloon_event_fn = event_func;
+    //balloon_stat_fn = stat_func;
+   ssd_balloon_opaque = opaque;
+    return 0;
+}
+
+void qemu_remove_ssd_balloon_handler(void *opaque)
+{
+    if (ssd_balloon_opaque != opaque) {
+        return;
+    }
+    ssd_balloon_event_fn = NULL;
+    //balloon_stat_fn = NULL;
+    ssd_balloon_opaque = NULL;
+}
+/* Added by Bhavesh Singh. 2017.06.02. End add */
+
diff -rupN /home/prafull/Desktop/qemu-2.9.0/balloon.c.orig /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/balloon.c.orig
--- /home/prafull/Desktop/qemu-2.9.0/balloon.c.orig	1970-01-01 05:30:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/balloon.c.orig	2018-05-28 13:06:05.000000000 +0530
@@ -0,0 +1,118 @@
+/*
+ * Generic Balloon handlers and management
+ *
+ * Copyright (c) 2003-2008 Fabrice Bellard
+ * Copyright (C) 2011 Red Hat, Inc.
+ * Copyright (C) 2011 Amit Shah <amit.shah@redhat.com>
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ * THE SOFTWARE.
+ */
+
+#include "qemu/osdep.h"
+#include "qemu-common.h"
+#include "exec/cpu-common.h"
+#include "sysemu/kvm.h"
+#include "sysemu/balloon.h"
+#include "trace-root.h"
+#include "qmp-commands.h"
+#include "qapi/qmp/qerror.h"
+#include "qapi/qmp/qjson.h"
+
+static QEMUBalloonEvent *balloon_event_fn;
+static QEMUBalloonStatus *balloon_stat_fn;
+static void *balloon_opaque;
+static bool balloon_inhibited;
+
+bool qemu_balloon_is_inhibited(void)
+{
+    return balloon_inhibited;
+}
+
+void qemu_balloon_inhibit(bool state)
+{
+    balloon_inhibited = state;
+}
+
+static bool have_balloon(Error **errp)
+{
+    if (kvm_enabled() && !kvm_has_sync_mmu()) {
+        error_set(errp, ERROR_CLASS_KVM_MISSING_CAP,
+                  "Using KVM without synchronous MMU, balloon unavailable");
+        return false;
+    }
+    if (!balloon_event_fn) {
+        error_set(errp, ERROR_CLASS_DEVICE_NOT_ACTIVE,
+                  "No balloon device has been activated");
+        return false;
+    }
+    return true;
+}
+
+int qemu_add_balloon_handler(QEMUBalloonEvent *event_func,
+                             QEMUBalloonStatus *stat_func, void *opaque)
+{
+    if (balloon_event_fn || balloon_stat_fn || balloon_opaque) {
+        /* We're already registered one balloon handler.  How many can
+         * a guest really have?
+         */
+        return -1;
+    }
+    balloon_event_fn = event_func;
+    balloon_stat_fn = stat_func;
+    balloon_opaque = opaque;
+    return 0;
+}
+
+void qemu_remove_balloon_handler(void *opaque)
+{
+    if (balloon_opaque != opaque) {
+        return;
+    }
+    balloon_event_fn = NULL;
+    balloon_stat_fn = NULL;
+    balloon_opaque = NULL;
+}
+
+BalloonInfo *qmp_query_balloon(Error **errp)
+{
+    BalloonInfo *info;
+
+    if (!have_balloon(errp)) {
+        return NULL;
+    }
+
+    info = g_malloc0(sizeof(*info));
+    balloon_stat_fn(balloon_opaque, info);
+    return info;
+}
+
+void qmp_balloon(int64_t target, Error **errp)
+{
+    if (!have_balloon(errp)) {
+        return;
+    }
+
+    if (target <= 0) {
+        error_setg(errp, QERR_INVALID_PARAMETER_VALUE, "target", "a size");
+        return;
+    }
+
+    trace_balloon_event(balloon_opaque, target);
+    balloon_event_fn(balloon_opaque, target);
+}
diff -rupN /home/prafull/Desktop/qemu-2.9.0/balloon.c.rej /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/balloon.c.rej
--- /home/prafull/Desktop/qemu-2.9.0/balloon.c.rej	1970-01-01 05:30:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/balloon.c.rej	2018-05-28 13:06:05.000000000 +0530
@@ -0,0 +1,18 @@
+--- balloon.c	2016-12-20 21:46:42.000000000 +0530
++++ qemu/qemu-2.9.0/balloon.c	2017-06-06 06:04:51.399762074 +0530
+@@ -33,10 +33,15 @@
+ #include "qmp-commands.h"
+ #include "qapi/qmp/qerror.h"
+ #include "qapi/qmp/qjson.h"
+ 
+ static QEMUBalloonEvent *balloon_event_fn;
++/* Added by Bhavesh Singh. 2017.06.02. Begin add */
++static QEMUSSDBalloonEvent *ssd_balloon_event_fn;
++static void *ssd_balloon_opaque;
++/* Added by Bhavesh Singh. 2017.06.02. End add */
++
+ static QEMUBalloonStatus *balloon_stat_fn;
+ static void *balloon_opaque;
+ static bool balloon_inhibited;f
+ 
+ bool qemu_balloon_is_inhibited(void)
diff -rupN /home/prafull/Desktop/qemu-2.9.0/block/block-backend.c /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/block/block-backend.c
--- /home/prafull/Desktop/qemu-2.9.0/block/block-backend.c	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/block/block-backend.c	2018-05-28 13:06:05.000000000 +0530
@@ -188,6 +188,11 @@ BlockBackend *blk_new(uint64_t perm, uin
 BlockBackend *blk_new_open(const char *filename, const char *reference,
                            QDict *options, int flags, Error **errp)
 {
+
+    printf("virtio: blk_new_open ()\n");
+    printf("\tfilename : %s\n", filename);
+
+    
     BlockBackend *blk;
     BlockDriverState *bs;
     uint64_t perm;
@@ -212,8 +217,10 @@ BlockBackend *blk_new_open(const char *f
     bs = bdrv_open(filename, reference, options, flags, errp);
     if (!bs) {
         blk_unref(blk);
+        printf("virtio: Error Block drive state is empty \n");
         return NULL;
     }
+    
 
     blk->root = bdrv_root_attach_child(bs, "root", &child_root,
                                        perm, BLK_PERM_ALL, blk, errp);
diff -rupN /home/prafull/Desktop/qemu-2.9.0/blockdev.c /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/blockdev.c
--- /home/prafull/Desktop/qemu-2.9.0/blockdev.c	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/blockdev.c	2018-05-28 13:06:05.000000000 +0530
@@ -470,6 +470,10 @@ static BlockBackend *blockdev_init(const
         BLOCKDEV_DETECT_ZEROES_OPTIONS_OFF;
     const char *throttling_group = NULL;
 
+    /*Unais Start*/
+    const char *vssd;
+
+    /*End*/
     /* Check common options by copying from bs_opts to opts, all other options
      * stay in bs_opts for processing by bdrv_open(). */
     id = qdict_get_try_str(bs_opts, "id");
@@ -489,6 +493,20 @@ static BlockBackend *blockdev_init(const
         qdict_del(bs_opts, "id");
     }
 
+    /*Added by Unais*/
+    vssd = qdict_get_try_str(bs_opts, "vssd");
+    if(vssd)
+    {   
+        // printf("\tvssd is specified. delete it \n");
+        qdict_del(bs_opts, "vssd");
+        qdict_del(bs_opts, "vm-id");
+        qdict_del(bs_opts, "vm-name");
+        qdict_del(bs_opts, "size");
+        qdict_del(bs_opts, "allocate");
+        qdict_del(bs_opts, "persist");
+    }
+    /*End Add*/
+
     /* extract parameters */
     snapshot = qemu_opt_get_bool(opts, "snapshot", 0);
 
@@ -798,6 +816,11 @@ DriveInfo *drive_new(QemuOpts *all_opts,
     Error *local_err = NULL;
     int i;
 
+    /*Unais Added*/
+    int vssd_device=0;
+    /* End*/
+
+    printf("\nDriver new : \n");
     /* Change legacy command line options into QMP ones */
     static const struct {
         const char *from;
@@ -835,6 +858,74 @@ DriveInfo *drive_new(QemuOpts *all_opts,
         }
     }
 
+
+    /*Unais Added*/
+    value = qemu_opt_get(all_opts, "vssd");
+    if(value)
+    {
+        vssd_device = 1;
+        printf("\tIt a vssd device \n");
+
+        value = qemu_opt_get(all_opts, "vm-id");
+        if(!value)
+        {
+            printf("VSSD-ERROR: VM id is not specified \n");
+            exit(1);
+        }
+        vssd_vm_id = atoi(value);
+        
+        
+        value = qemu_opt_get(all_opts, "vm-name");
+        if(!value)
+        {
+            printf("VSSD-ERROR: VM name is not specified \n");
+            exit(1);
+        }       
+       strcpy(vssd_vm_name, value);
+       printf("VSSD: \vvm-name : %s. (%s) \n", vssd_vm_name,value);
+
+
+       value = qemu_opt_get(all_opts, "size");
+        if(!value)
+        {
+            printf("VSSD-ERROR: VSSD size is not specified \n");
+            exit(1);
+        }       
+       vssd_size = atoi(value);
+
+       value = qemu_opt_get(all_opts, "allocate");
+        if(!value)
+        {
+            printf("VSSD-ERROR: VSSD current alllocation size is not specified \n");
+            exit(1);
+        }       
+       vssd_current_allocate = atoi(value);
+
+       value = qemu_opt_get(all_opts, "persist");
+        if(!value)
+        {
+            printf("VSSD-ERROR: VSSD current persist size is not specified \n");
+            exit(1);
+        }
+        if(strcmp(value, "FULL") == 0)
+        {
+            vssd_persist_full = true;
+            vssd_current_persist = 0;
+        }
+        else if(strcmp(value, "NONE") == 0) 
+        {
+            vssd_persist_full = false;
+            vssd_current_persist = 0;
+        }
+        else
+        {
+            vssd_persist_full = false;
+            vssd_current_persist = atoi(value);
+        } 
+
+    }
+    /*End*/
+
     value = qemu_opt_get(all_opts, "cache");
     if (value) {
         int flags = 0;
@@ -860,6 +951,7 @@ DriveInfo *drive_new(QemuOpts *all_opts,
         }
         qemu_opt_unset(all_opts, "cache");
     }
+   
 
     /* Get a QDict for processing the options */
     bs_opts = qdict_new();
@@ -1047,8 +1139,19 @@ DriveInfo *drive_new(QemuOpts *all_opts,
                                    &error_abort);
         if (arch_type == QEMU_ARCH_S390X) {
             qemu_opt_set(devopts, "driver", "virtio-blk-ccw", &error_abort);
-        } else {
-            qemu_opt_set(devopts, "driver", "virtio-blk-pci", &error_abort);
+        } 
+        else 
+        {
+            /**Modified by Unais */
+            if(vssd_device)
+            {
+                printf("Setting driver = virtio-vssd-pci \n");
+                qemu_opt_set(devopts, "driver", "virtio-vssd-pci", &error_abort);
+            }
+            else
+                qemu_opt_set(devopts, "driver", "virtio-blk-pci", &error_abort);
+
+            /*End Modified*/
         }
         qemu_opt_set(devopts, "drive", qdict_get_str(bs_opts, "id"),
                      &error_abort);
@@ -1081,6 +1184,7 @@ DriveInfo *drive_new(QemuOpts *all_opts,
     }
 
     /* Actual block device init: Functionality shared with blockdev-add */
+    printf("\tInitializing blockdev \n");
     blk = blockdev_init(filename, bs_opts, &local_err);
     bs_opts = NULL;
     if (!blk) {
diff -rupN /home/prafull/Desktop/qemu-2.9.0/configure /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/configure
--- /home/prafull/Desktop/qemu-2.9.0/configure	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/configure	2018-05-28 13:06:17.000000000 +0530
@@ -5110,9 +5110,9 @@ echo "vhost-net support $vhost_net"
 echo "vhost-scsi support $vhost_scsi"
 echo "vhost-vsock support $vhost_vsock"
 echo "Trace backends    $trace_backends"
-if have_backend "simple"; then
+#if have_backend "simple"; then
 echo "Trace output file $trace_file-<pid>"
-fi
+#fi
 echo "spice support     $spice $(echo_version $spice $spice_protocol_version/$spice_server_version)"
 echo "rbd support       $rbd"
 echo "xfsctl support    $xfs"
diff -rupN /home/prafull/Desktop/qemu-2.9.0/.gitignore.orig /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/.gitignore.orig
--- /home/prafull/Desktop/qemu-2.9.0/.gitignore.orig	1970-01-01 05:30:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/.gitignore.orig	2018-05-28 13:06:05.000000000 +0530
@@ -0,0 +1,133 @@
+/config-devices.*
+/config-all-devices.*
+/config-all-disas.*
+/config-host.*
+/config-target.*
+/config.status
+/config-temp
+/trace-events-all
+/trace/generated-events.h
+/trace/generated-events.c
+/trace/generated-helpers-wrappers.h
+/trace/generated-helpers.h
+/trace/generated-helpers.c
+/trace/generated-tcg-tracers.h
+/ui/shader/texture-blit-frag.h
+/ui/shader/texture-blit-vert.h
+*-timestamp
+/*-softmmu
+/*-darwin-user
+/*-linux-user
+/*-bsd-user
+/ivshmem-client
+/ivshmem-server
+/libdis*
+/libuser
+/linux-headers/asm
+/qga/qapi-generated
+/qapi-generated
+/qapi-types.[ch]
+/qapi-visit.[ch]
+/qapi-event.[ch]
+/qmp-commands.h
+/qmp-introspect.[ch]
+/qmp-marshal.c
+/qemu-doc.html
+/qemu-doc.info
+/qemu-doc.txt
+/qemu-img
+/qemu-nbd
+/qemu-options.def
+/qemu-options.texi
+/qemu-img-cmds.texi
+/qemu-img-cmds.h
+/qemu-io
+/qemu-ga
+/qemu-bridge-helper
+/qemu-monitor.texi
+/qemu-monitor-info.texi
+/qemu-version.h
+/qemu-version.h.tmp
+/module_block.h
+/vscclient
+/fsdev/virtfs-proxy-helper
+*.[1-9]
+*.a
+*.aux
+*.cp
+*.exe
+*.msi
+*.dll
+*.so
+*.mo
+*.fn
+*.ky
+*.log
+*.pdf
+*.pod
+*.cps
+*.fns
+*.kys
+*.pg
+*.pyc
+*.toc
+*.tp
+*.vr
+*.d
+!/scripts/qemu-guest-agent/fsfreeze-hook.d
+*.o
+.sdk
+*.gcda
+*.gcno
+/pc-bios/bios-pq/status
+/pc-bios/vgabios-pq/status
+/pc-bios/optionrom/linuxboot.asm
+/pc-bios/optionrom/linuxboot.bin
+/pc-bios/optionrom/linuxboot.raw
+/pc-bios/optionrom/linuxboot.img
+/pc-bios/optionrom/linuxboot_dma.asm
+/pc-bios/optionrom/linuxboot_dma.bin
+/pc-bios/optionrom/linuxboot_dma.raw
+/pc-bios/optionrom/linuxboot_dma.img
+/pc-bios/optionrom/multiboot.asm
+/pc-bios/optionrom/multiboot.bin
+/pc-bios/optionrom/multiboot.raw
+/pc-bios/optionrom/multiboot.img
+/pc-bios/optionrom/kvmvapic.asm
+/pc-bios/optionrom/kvmvapic.bin
+/pc-bios/optionrom/kvmvapic.raw
+/pc-bios/optionrom/kvmvapic.img
+/pc-bios/s390-ccw/s390-ccw.elf
+/pc-bios/s390-ccw/s390-ccw.img
+/docs/qemu-ga-qapi.texi
+/docs/qemu-ga-ref.html
+/docs/qemu-ga-ref.info*
+/docs/qemu-ga-ref.txt
+/docs/qemu-qmp-qapi.texi
+/docs/qemu-qmp-ref.html
+/docs/qemu-qmp-ref.info*
+/docs/qemu-qmp-ref.txt
+/docs/version.texi
+*.tps
+.stgit-*
+cscope.*
+tags
+TAGS
+docker-src.*
+*~
+trace.h
+trace.c
+trace-ust.h
+trace-ust.h
+trace-dtrace.h
+trace-dtrace.dtrace
+trace-root.h
+trace-root.c
+trace-ust-root.h
+trace-ust-root.h
+trace-ust-all.h
+trace-ust-all.c
+trace-dtrace-root.h
+trace-dtrace-root.dtrace
+trace-ust-all.h
+trace-ust-all.c
diff -rupN /home/prafull/Desktop/qemu-2.9.0/.gitignore.rej /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/.gitignore.rej
--- /home/prafull/Desktop/qemu-2.9.0/.gitignore.rej	1970-01-01 05:30:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/.gitignore.rej	2018-05-28 13:06:05.000000000 +0530
@@ -0,0 +1,14 @@
+--- .gitignore	2016-12-20 21:46:42.000000000 +0530
++++ qemu/qemu-2.9.0/.gitignore	2017-04-06 16:13:53.245972640 +0530
+@@ -3,10 +3,11 @@
+ /config-all-disas.*
+ /config-host.*
+ /config-target.*
+ /config.status
+ /config-temp
++/build
+ /trace-events-all
+ /trace/generated-tracers.h
+ /trace/generated-tracers.c
+ /trace/generated-tracers-dtrace.h
+ /trace/generated-tracers.dtrace
diff -rupN /home/prafull/Desktop/qemu-2.9.0/hmp.c /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hmp.c
--- /home/prafull/Desktop/qemu-2.9.0/hmp.c	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hmp.c	2018-05-28 13:06:05.000000000 +0530
@@ -1154,6 +1154,23 @@ void hmp_balloon(Monitor *mon, const QDi
     }
 }
 
+/* Added by Bhavesh Singh. 2017.06.02. Begin add */
+void hmp_ssd_balloon(Monitor *mon, const QDict *qdict)
+{
+    int64_t value = qdict_get_int(qdict, "value");
+    Error *err = NULL;
+
+/*  Added by Muhammed Unais P.  2017-07-25.  Begin add.  */  
+    printf("\nFn callled : hmp_ssd_balloon [hmp.c]\n");
+/*  Added by Muhammed Unais P.  2017-07-25.  End add.    */
+
+    qmp_ssd_balloon(value, &err);
+    if (err) {
+        error_report_err(err);
+    }
+}
+/* Added by Bhavesh Singh. 2017.06.02. End add */
+
 void hmp_block_resize(Monitor *mon, const QDict *qdict)
 {
     const char *device = qdict_get_str(qdict, "device");
diff -rupN /home/prafull/Desktop/qemu-2.9.0/hmp-commands.hx /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hmp-commands.hx
--- /home/prafull/Desktop/qemu-2.9.0/hmp-commands.hx	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hmp-commands.hx	2018-05-28 13:06:05.000000000 +0530
@@ -1766,6 +1766,19 @@ Set QOM property @var{property} of objec
 ETEXI
 
     {
+        .name       = "ssd-balloon",
+        .args_type  = "value:l",
+        .params     = "target",
+        .help       = "request VM to change its balloon size by given number of sectors",
+        .cmd        = hmp_ssd_balloon,
+    },
+
+STEXI
+@item ssd-balloon @var{target} 
+Change vssd size
+ETEXI
+
+    {
         .name       = "info",
         .args_type  = "item:s?",
         .params     = "[subcommand]",
@@ -1776,4 +1789,4 @@ ETEXI
 
 STEXI
 @end table
-ETEXI
+ETEXI
\ No newline at end of file
diff -rupN /home/prafull/Desktop/qemu-2.9.0/hmp.c.orig /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hmp.c.orig
--- /home/prafull/Desktop/qemu-2.9.0/hmp.c.orig	1970-01-01 05:30:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hmp.c.orig	2018-05-28 13:06:05.000000000 +0530
@@ -0,0 +1,2618 @@
+/*
+ * Human Monitor Interface
+ *
+ * Copyright IBM, Corp. 2011
+ *
+ * Authors:
+ *  Anthony Liguori   <aliguori@us.ibm.com>
+ *
+ * This work is licensed under the terms of the GNU GPL, version 2.  See
+ * the COPYING file in the top-level directory.
+ *
+ * Contributions after 2012-01-13 are licensed under the terms of the
+ * GNU GPL, version 2 or (at your option) any later version.
+ */
+
+#include "qemu/osdep.h"
+#include "hmp.h"
+#include "net/net.h"
+#include "net/eth.h"
+#include "sysemu/char.h"
+#include "sysemu/block-backend.h"
+#include "qemu/config-file.h"
+#include "qemu/option.h"
+#include "qemu/timer.h"
+#include "qmp-commands.h"
+#include "qemu/sockets.h"
+#include "monitor/monitor.h"
+#include "monitor/qdev.h"
+#include "qapi/opts-visitor.h"
+#include "qapi/qmp/qerror.h"
+#include "qapi/string-output-visitor.h"
+#include "qapi/util.h"
+#include "qapi-visit.h"
+#include "qom/object_interfaces.h"
+#include "ui/console.h"
+#include "block/qapi.h"
+#include "qemu-io.h"
+#include "qemu/cutils.h"
+#include "qemu/error-report.h"
+#include "hw/intc/intc.h"
+
+#ifdef CONFIG_SPICE
+#include <spice/enums.h>
+#endif
+
+static void hmp_handle_error(Monitor *mon, Error **errp)
+{
+    assert(errp);
+    if (*errp) {
+        error_report_err(*errp);
+    }
+}
+
+void hmp_info_name(Monitor *mon, const QDict *qdict)
+{
+    NameInfo *info;
+
+    info = qmp_query_name(NULL);
+    if (info->has_name) {
+        monitor_printf(mon, "%s\n", info->name);
+    }
+    qapi_free_NameInfo(info);
+}
+
+void hmp_info_version(Monitor *mon, const QDict *qdict)
+{
+    VersionInfo *info;
+
+    info = qmp_query_version(NULL);
+
+    monitor_printf(mon, "%" PRId64 ".%" PRId64 ".%" PRId64 "%s\n",
+                   info->qemu->major, info->qemu->minor, info->qemu->micro,
+                   info->package);
+
+    qapi_free_VersionInfo(info);
+}
+
+void hmp_info_kvm(Monitor *mon, const QDict *qdict)
+{
+    KvmInfo *info;
+
+    info = qmp_query_kvm(NULL);
+    monitor_printf(mon, "kvm support: ");
+    if (info->present) {
+        monitor_printf(mon, "%s\n", info->enabled ? "enabled" : "disabled");
+    } else {
+        monitor_printf(mon, "not compiled\n");
+    }
+
+    qapi_free_KvmInfo(info);
+}
+
+void hmp_info_status(Monitor *mon, const QDict *qdict)
+{
+    StatusInfo *info;
+
+    info = qmp_query_status(NULL);
+
+    monitor_printf(mon, "VM status: %s%s",
+                   info->running ? "running" : "paused",
+                   info->singlestep ? " (single step mode)" : "");
+
+    if (!info->running && info->status != RUN_STATE_PAUSED) {
+        monitor_printf(mon, " (%s)", RunState_lookup[info->status]);
+    }
+
+    monitor_printf(mon, "\n");
+
+    qapi_free_StatusInfo(info);
+}
+
+void hmp_info_uuid(Monitor *mon, const QDict *qdict)
+{
+    UuidInfo *info;
+
+    info = qmp_query_uuid(NULL);
+    monitor_printf(mon, "%s\n", info->UUID);
+    qapi_free_UuidInfo(info);
+}
+
+void hmp_info_chardev(Monitor *mon, const QDict *qdict)
+{
+    ChardevInfoList *char_info, *info;
+
+    char_info = qmp_query_chardev(NULL);
+    for (info = char_info; info; info = info->next) {
+        monitor_printf(mon, "%s: filename=%s\n", info->value->label,
+                                                 info->value->filename);
+    }
+
+    qapi_free_ChardevInfoList(char_info);
+}
+
+void hmp_info_mice(Monitor *mon, const QDict *qdict)
+{
+    MouseInfoList *mice_list, *mouse;
+
+    mice_list = qmp_query_mice(NULL);
+    if (!mice_list) {
+        monitor_printf(mon, "No mouse devices connected\n");
+        return;
+    }
+
+    for (mouse = mice_list; mouse; mouse = mouse->next) {
+        monitor_printf(mon, "%c Mouse #%" PRId64 ": %s%s\n",
+                       mouse->value->current ? '*' : ' ',
+                       mouse->value->index, mouse->value->name,
+                       mouse->value->absolute ? " (absolute)" : "");
+    }
+
+    qapi_free_MouseInfoList(mice_list);
+}
+
+void hmp_info_migrate(Monitor *mon, const QDict *qdict)
+{
+    MigrationInfo *info;
+    MigrationCapabilityStatusList *caps, *cap;
+
+    info = qmp_query_migrate(NULL);
+    caps = qmp_query_migrate_capabilities(NULL);
+
+    /* do not display parameters during setup */
+    if (info->has_status && caps) {
+        monitor_printf(mon, "capabilities: ");
+        for (cap = caps; cap; cap = cap->next) {
+            monitor_printf(mon, "%s: %s ",
+                           MigrationCapability_lookup[cap->value->capability],
+                           cap->value->state ? "on" : "off");
+        }
+        monitor_printf(mon, "\n");
+    }
+
+    if (info->has_status) {
+        monitor_printf(mon, "Migration status: %s",
+                       MigrationStatus_lookup[info->status]);
+        if (info->status == MIGRATION_STATUS_FAILED &&
+            info->has_error_desc) {
+            monitor_printf(mon, " (%s)\n", info->error_desc);
+        } else {
+            monitor_printf(mon, "\n");
+        }
+
+        monitor_printf(mon, "total time: %" PRIu64 " milliseconds\n",
+                       info->total_time);
+        if (info->has_expected_downtime) {
+            monitor_printf(mon, "expected downtime: %" PRIu64 " milliseconds\n",
+                           info->expected_downtime);
+        }
+        if (info->has_downtime) {
+            monitor_printf(mon, "downtime: %" PRIu64 " milliseconds\n",
+                           info->downtime);
+        }
+        if (info->has_setup_time) {
+            monitor_printf(mon, "setup: %" PRIu64 " milliseconds\n",
+                           info->setup_time);
+        }
+    }
+
+    if (info->has_ram) {
+        monitor_printf(mon, "transferred ram: %" PRIu64 " kbytes\n",
+                       info->ram->transferred >> 10);
+        monitor_printf(mon, "throughput: %0.2f mbps\n",
+                       info->ram->mbps);
+        monitor_printf(mon, "remaining ram: %" PRIu64 " kbytes\n",
+                       info->ram->remaining >> 10);
+        monitor_printf(mon, "total ram: %" PRIu64 " kbytes\n",
+                       info->ram->total >> 10);
+        monitor_printf(mon, "duplicate: %" PRIu64 " pages\n",
+                       info->ram->duplicate);
+        monitor_printf(mon, "skipped: %" PRIu64 " pages\n",
+                       info->ram->skipped);
+        monitor_printf(mon, "normal: %" PRIu64 " pages\n",
+                       info->ram->normal);
+        monitor_printf(mon, "normal bytes: %" PRIu64 " kbytes\n",
+                       info->ram->normal_bytes >> 10);
+        monitor_printf(mon, "dirty sync count: %" PRIu64 "\n",
+                       info->ram->dirty_sync_count);
+        if (info->ram->dirty_pages_rate) {
+            monitor_printf(mon, "dirty pages rate: %" PRIu64 " pages\n",
+                           info->ram->dirty_pages_rate);
+        }
+        if (info->ram->postcopy_requests) {
+            monitor_printf(mon, "postcopy request count: %" PRIu64 "\n",
+                           info->ram->postcopy_requests);
+        }
+    }
+
+    if (info->has_disk) {
+        monitor_printf(mon, "transferred disk: %" PRIu64 " kbytes\n",
+                       info->disk->transferred >> 10);
+        monitor_printf(mon, "remaining disk: %" PRIu64 " kbytes\n",
+                       info->disk->remaining >> 10);
+        monitor_printf(mon, "total disk: %" PRIu64 " kbytes\n",
+                       info->disk->total >> 10);
+    }
+
+    if (info->has_xbzrle_cache) {
+        monitor_printf(mon, "cache size: %" PRIu64 " bytes\n",
+                       info->xbzrle_cache->cache_size);
+        monitor_printf(mon, "xbzrle transferred: %" PRIu64 " kbytes\n",
+                       info->xbzrle_cache->bytes >> 10);
+        monitor_printf(mon, "xbzrle pages: %" PRIu64 " pages\n",
+                       info->xbzrle_cache->pages);
+        monitor_printf(mon, "xbzrle cache miss: %" PRIu64 "\n",
+                       info->xbzrle_cache->cache_miss);
+        monitor_printf(mon, "xbzrle cache miss rate: %0.2f\n",
+                       info->xbzrle_cache->cache_miss_rate);
+        monitor_printf(mon, "xbzrle overflow : %" PRIu64 "\n",
+                       info->xbzrle_cache->overflow);
+    }
+
+    if (info->has_cpu_throttle_percentage) {
+        monitor_printf(mon, "cpu throttle percentage: %" PRIu64 "\n",
+                       info->cpu_throttle_percentage);
+    }
+
+    qapi_free_MigrationInfo(info);
+    qapi_free_MigrationCapabilityStatusList(caps);
+}
+
+void hmp_info_migrate_capabilities(Monitor *mon, const QDict *qdict)
+{
+    MigrationCapabilityStatusList *caps, *cap;
+
+    caps = qmp_query_migrate_capabilities(NULL);
+
+    if (caps) {
+        monitor_printf(mon, "capabilities: ");
+        for (cap = caps; cap; cap = cap->next) {
+            monitor_printf(mon, "%s: %s ",
+                           MigrationCapability_lookup[cap->value->capability],
+                           cap->value->state ? "on" : "off");
+        }
+        monitor_printf(mon, "\n");
+    }
+
+    qapi_free_MigrationCapabilityStatusList(caps);
+}
+
+void hmp_info_migrate_parameters(Monitor *mon, const QDict *qdict)
+{
+    MigrationParameters *params;
+
+    params = qmp_query_migrate_parameters(NULL);
+
+    if (params) {
+        monitor_printf(mon, "parameters:");
+        assert(params->has_compress_level);
+        monitor_printf(mon, " %s: %" PRId64,
+            MigrationParameter_lookup[MIGRATION_PARAMETER_COMPRESS_LEVEL],
+            params->compress_level);
+        assert(params->has_compress_threads);
+        monitor_printf(mon, " %s: %" PRId64,
+            MigrationParameter_lookup[MIGRATION_PARAMETER_COMPRESS_THREADS],
+            params->compress_threads);
+        assert(params->has_decompress_threads);
+        monitor_printf(mon, " %s: %" PRId64,
+            MigrationParameter_lookup[MIGRATION_PARAMETER_DECOMPRESS_THREADS],
+            params->decompress_threads);
+        assert(params->has_cpu_throttle_initial);
+        monitor_printf(mon, " %s: %" PRId64,
+            MigrationParameter_lookup[MIGRATION_PARAMETER_CPU_THROTTLE_INITIAL],
+            params->cpu_throttle_initial);
+        assert(params->has_cpu_throttle_increment);
+        monitor_printf(mon, " %s: %" PRId64,
+            MigrationParameter_lookup[MIGRATION_PARAMETER_CPU_THROTTLE_INCREMENT],
+            params->cpu_throttle_increment);
+        monitor_printf(mon, " %s: '%s'",
+            MigrationParameter_lookup[MIGRATION_PARAMETER_TLS_CREDS],
+            params->has_tls_creds ? params->tls_creds : "");
+        monitor_printf(mon, " %s: '%s'",
+            MigrationParameter_lookup[MIGRATION_PARAMETER_TLS_HOSTNAME],
+            params->has_tls_hostname ? params->tls_hostname : "");
+        assert(params->has_max_bandwidth);
+        monitor_printf(mon, " %s: %" PRId64 " bytes/second",
+            MigrationParameter_lookup[MIGRATION_PARAMETER_MAX_BANDWIDTH],
+            params->max_bandwidth);
+        assert(params->has_downtime_limit);
+        monitor_printf(mon, " %s: %" PRId64 " milliseconds",
+            MigrationParameter_lookup[MIGRATION_PARAMETER_DOWNTIME_LIMIT],
+            params->downtime_limit);
+        assert(params->has_x_checkpoint_delay);
+        monitor_printf(mon, " %s: %" PRId64,
+            MigrationParameter_lookup[MIGRATION_PARAMETER_X_CHECKPOINT_DELAY],
+            params->x_checkpoint_delay);
+        monitor_printf(mon, "\n");
+    }
+
+    qapi_free_MigrationParameters(params);
+}
+
+void hmp_info_migrate_cache_size(Monitor *mon, const QDict *qdict)
+{
+    monitor_printf(mon, "xbzrel cache size: %" PRId64 " kbytes\n",
+                   qmp_query_migrate_cache_size(NULL) >> 10);
+}
+
+void hmp_info_cpus(Monitor *mon, const QDict *qdict)
+{
+    CpuInfoList *cpu_list, *cpu;
+
+    cpu_list = qmp_query_cpus(NULL);
+
+    for (cpu = cpu_list; cpu; cpu = cpu->next) {
+        int active = ' ';
+
+        if (cpu->value->CPU == monitor_get_cpu_index()) {
+            active = '*';
+        }
+
+        monitor_printf(mon, "%c CPU #%" PRId64 ":", active, cpu->value->CPU);
+
+        switch (cpu->value->arch) {
+        case CPU_INFO_ARCH_X86:
+            monitor_printf(mon, " pc=0x%016" PRIx64, cpu->value->u.x86.pc);
+            break;
+        case CPU_INFO_ARCH_PPC:
+            monitor_printf(mon, " nip=0x%016" PRIx64, cpu->value->u.ppc.nip);
+            break;
+        case CPU_INFO_ARCH_SPARC:
+            monitor_printf(mon, " pc=0x%016" PRIx64,
+                           cpu->value->u.q_sparc.pc);
+            monitor_printf(mon, " npc=0x%016" PRIx64,
+                           cpu->value->u.q_sparc.npc);
+            break;
+        case CPU_INFO_ARCH_MIPS:
+            monitor_printf(mon, " PC=0x%016" PRIx64, cpu->value->u.q_mips.PC);
+            break;
+        case CPU_INFO_ARCH_TRICORE:
+            monitor_printf(mon, " PC=0x%016" PRIx64, cpu->value->u.tricore.PC);
+            break;
+        default:
+            break;
+        }
+
+        if (cpu->value->halted) {
+            monitor_printf(mon, " (halted)");
+        }
+
+        monitor_printf(mon, " thread_id=%" PRId64 "\n", cpu->value->thread_id);
+    }
+
+    qapi_free_CpuInfoList(cpu_list);
+}
+
+static void print_block_info(Monitor *mon, BlockInfo *info,
+                             BlockDeviceInfo *inserted, bool verbose)
+{
+    ImageInfo *image_info;
+
+    assert(!info || !info->has_inserted || info->inserted == inserted);
+
+    if (info) {
+        monitor_printf(mon, "%s", info->device);
+        if (inserted && inserted->has_node_name) {
+            monitor_printf(mon, " (%s)", inserted->node_name);
+        }
+    } else {
+        assert(inserted);
+        monitor_printf(mon, "%s",
+                       inserted->has_node_name
+                       ? inserted->node_name
+                       : "<anonymous>");
+    }
+
+    if (inserted) {
+        monitor_printf(mon, ": %s (%s%s%s)\n",
+                       inserted->file,
+                       inserted->drv,
+                       inserted->ro ? ", read-only" : "",
+                       inserted->encrypted ? ", encrypted" : "");
+    } else {
+        monitor_printf(mon, ": [not inserted]\n");
+    }
+
+    if (info) {
+        if (info->has_io_status && info->io_status != BLOCK_DEVICE_IO_STATUS_OK) {
+            monitor_printf(mon, "    I/O status:       %s\n",
+                           BlockDeviceIoStatus_lookup[info->io_status]);
+        }
+
+        if (info->removable) {
+            monitor_printf(mon, "    Removable device: %slocked, tray %s\n",
+                           info->locked ? "" : "not ",
+                           info->tray_open ? "open" : "closed");
+        }
+    }
+
+
+    if (!inserted) {
+        return;
+    }
+
+    monitor_printf(mon, "    Cache mode:       %s%s%s\n",
+                   inserted->cache->writeback ? "writeback" : "writethrough",
+                   inserted->cache->direct ? ", direct" : "",
+                   inserted->cache->no_flush ? ", ignore flushes" : "");
+
+    if (inserted->has_backing_file) {
+        monitor_printf(mon,
+                       "    Backing file:     %s "
+                       "(chain depth: %" PRId64 ")\n",
+                       inserted->backing_file,
+                       inserted->backing_file_depth);
+    }
+
+    if (inserted->detect_zeroes != BLOCKDEV_DETECT_ZEROES_OPTIONS_OFF) {
+        monitor_printf(mon, "    Detect zeroes:    %s\n",
+                       BlockdevDetectZeroesOptions_lookup[inserted->detect_zeroes]);
+    }
+
+    if (inserted->bps  || inserted->bps_rd  || inserted->bps_wr  ||
+        inserted->iops || inserted->iops_rd || inserted->iops_wr)
+    {
+        monitor_printf(mon, "    I/O throttling:   bps=%" PRId64
+                        " bps_rd=%" PRId64  " bps_wr=%" PRId64
+                        " bps_max=%" PRId64
+                        " bps_rd_max=%" PRId64
+                        " bps_wr_max=%" PRId64
+                        " iops=%" PRId64 " iops_rd=%" PRId64
+                        " iops_wr=%" PRId64
+                        " iops_max=%" PRId64
+                        " iops_rd_max=%" PRId64
+                        " iops_wr_max=%" PRId64
+                        " iops_size=%" PRId64
+                        " group=%s\n",
+                        inserted->bps,
+                        inserted->bps_rd,
+                        inserted->bps_wr,
+                        inserted->bps_max,
+                        inserted->bps_rd_max,
+                        inserted->bps_wr_max,
+                        inserted->iops,
+                        inserted->iops_rd,
+                        inserted->iops_wr,
+                        inserted->iops_max,
+                        inserted->iops_rd_max,
+                        inserted->iops_wr_max,
+                        inserted->iops_size,
+                        inserted->group);
+    }
+
+    if (verbose) {
+        monitor_printf(mon, "\nImages:\n");
+        image_info = inserted->image;
+        while (1) {
+                bdrv_image_info_dump((fprintf_function)monitor_printf,
+                                     mon, image_info);
+            if (image_info->has_backing_image) {
+                image_info = image_info->backing_image;
+            } else {
+                break;
+            }
+        }
+    }
+}
+
+void hmp_info_block(Monitor *mon, const QDict *qdict)
+{
+    BlockInfoList *block_list, *info;
+    BlockDeviceInfoList *blockdev_list, *blockdev;
+    const char *device = qdict_get_try_str(qdict, "device");
+    bool verbose = qdict_get_try_bool(qdict, "verbose", false);
+    bool nodes = qdict_get_try_bool(qdict, "nodes", false);
+    bool printed = false;
+
+    /* Print BlockBackend information */
+    if (!nodes) {
+        block_list = qmp_query_block(NULL);
+    } else {
+        block_list = NULL;
+    }
+
+    for (info = block_list; info; info = info->next) {
+        if (device && strcmp(device, info->value->device)) {
+            continue;
+        }
+
+        if (info != block_list) {
+            monitor_printf(mon, "\n");
+        }
+
+        print_block_info(mon, info->value, info->value->has_inserted
+                                           ? info->value->inserted : NULL,
+                         verbose);
+        printed = true;
+    }
+
+    qapi_free_BlockInfoList(block_list);
+
+    if ((!device && !nodes) || printed) {
+        return;
+    }
+
+    /* Print node information */
+    blockdev_list = qmp_query_named_block_nodes(NULL);
+    for (blockdev = blockdev_list; blockdev; blockdev = blockdev->next) {
+        assert(blockdev->value->has_node_name);
+        if (device && strcmp(device, blockdev->value->node_name)) {
+            continue;
+        }
+
+        if (blockdev != blockdev_list) {
+            monitor_printf(mon, "\n");
+        }
+
+        print_block_info(mon, NULL, blockdev->value, verbose);
+    }
+    qapi_free_BlockDeviceInfoList(blockdev_list);
+}
+
+void hmp_info_blockstats(Monitor *mon, const QDict *qdict)
+{
+    BlockStatsList *stats_list, *stats;
+
+    stats_list = qmp_query_blockstats(false, false, NULL);
+
+    for (stats = stats_list; stats; stats = stats->next) {
+        if (!stats->value->has_device) {
+            continue;
+        }
+
+        monitor_printf(mon, "%s:", stats->value->device);
+        monitor_printf(mon, " rd_bytes=%" PRId64
+                       " wr_bytes=%" PRId64
+                       " rd_operations=%" PRId64
+                       " wr_operations=%" PRId64
+                       " flush_operations=%" PRId64
+                       " wr_total_time_ns=%" PRId64
+                       " rd_total_time_ns=%" PRId64
+                       " flush_total_time_ns=%" PRId64
+                       " rd_merged=%" PRId64
+                       " wr_merged=%" PRId64
+                       " idle_time_ns=%" PRId64
+                       "\n",
+                       stats->value->stats->rd_bytes,
+                       stats->value->stats->wr_bytes,
+                       stats->value->stats->rd_operations,
+                       stats->value->stats->wr_operations,
+                       stats->value->stats->flush_operations,
+                       stats->value->stats->wr_total_time_ns,
+                       stats->value->stats->rd_total_time_ns,
+                       stats->value->stats->flush_total_time_ns,
+                       stats->value->stats->rd_merged,
+                       stats->value->stats->wr_merged,
+                       stats->value->stats->idle_time_ns);
+    }
+
+    qapi_free_BlockStatsList(stats_list);
+}
+
+void hmp_info_vnc(Monitor *mon, const QDict *qdict)
+{
+    VncInfo *info;
+    Error *err = NULL;
+    VncClientInfoList *client;
+
+    info = qmp_query_vnc(&err);
+    if (err) {
+        error_report_err(err);
+        return;
+    }
+
+    if (!info->enabled) {
+        monitor_printf(mon, "Server: disabled\n");
+        goto out;
+    }
+
+    monitor_printf(mon, "Server:\n");
+    if (info->has_host && info->has_service) {
+        monitor_printf(mon, "     address: %s:%s\n", info->host, info->service);
+    }
+    if (info->has_auth) {
+        monitor_printf(mon, "        auth: %s\n", info->auth);
+    }
+
+    if (!info->has_clients || info->clients == NULL) {
+        monitor_printf(mon, "Client: none\n");
+    } else {
+        for (client = info->clients; client; client = client->next) {
+            monitor_printf(mon, "Client:\n");
+            monitor_printf(mon, "     address: %s:%s\n",
+                           client->value->host,
+                           client->value->service);
+            monitor_printf(mon, "  x509_dname: %s\n",
+                           client->value->x509_dname ?
+                           client->value->x509_dname : "none");
+            monitor_printf(mon, "    username: %s\n",
+                           client->value->has_sasl_username ?
+                           client->value->sasl_username : "none");
+        }
+    }
+
+out:
+    qapi_free_VncInfo(info);
+}
+
+#ifdef CONFIG_SPICE
+void hmp_info_spice(Monitor *mon, const QDict *qdict)
+{
+    SpiceChannelList *chan;
+    SpiceInfo *info;
+    const char *channel_name;
+    const char * const channel_names[] = {
+        [SPICE_CHANNEL_MAIN] = "main",
+        [SPICE_CHANNEL_DISPLAY] = "display",
+        [SPICE_CHANNEL_INPUTS] = "inputs",
+        [SPICE_CHANNEL_CURSOR] = "cursor",
+        [SPICE_CHANNEL_PLAYBACK] = "playback",
+        [SPICE_CHANNEL_RECORD] = "record",
+        [SPICE_CHANNEL_TUNNEL] = "tunnel",
+        [SPICE_CHANNEL_SMARTCARD] = "smartcard",
+        [SPICE_CHANNEL_USBREDIR] = "usbredir",
+        [SPICE_CHANNEL_PORT] = "port",
+#if 0
+        /* minimum spice-protocol is 0.12.3, webdav was added in 0.12.7,
+         * no easy way to #ifdef (SPICE_CHANNEL_* is a enum).  Disable
+         * as quick fix for build failures with older versions. */
+        [SPICE_CHANNEL_WEBDAV] = "webdav",
+#endif
+    };
+
+    info = qmp_query_spice(NULL);
+
+    if (!info->enabled) {
+        monitor_printf(mon, "Server: disabled\n");
+        goto out;
+    }
+
+    monitor_printf(mon, "Server:\n");
+    if (info->has_port) {
+        monitor_printf(mon, "     address: %s:%" PRId64 "\n",
+                       info->host, info->port);
+    }
+    if (info->has_tls_port) {
+        monitor_printf(mon, "     address: %s:%" PRId64 " [tls]\n",
+                       info->host, info->tls_port);
+    }
+    monitor_printf(mon, "    migrated: %s\n",
+                   info->migrated ? "true" : "false");
+    monitor_printf(mon, "        auth: %s\n", info->auth);
+    monitor_printf(mon, "    compiled: %s\n", info->compiled_version);
+    monitor_printf(mon, "  mouse-mode: %s\n",
+                   SpiceQueryMouseMode_lookup[info->mouse_mode]);
+
+    if (!info->has_channels || info->channels == NULL) {
+        monitor_printf(mon, "Channels: none\n");
+    } else {
+        for (chan = info->channels; chan; chan = chan->next) {
+            monitor_printf(mon, "Channel:\n");
+            monitor_printf(mon, "     address: %s:%s%s\n",
+                           chan->value->host, chan->value->port,
+                           chan->value->tls ? " [tls]" : "");
+            monitor_printf(mon, "     session: %" PRId64 "\n",
+                           chan->value->connection_id);
+            monitor_printf(mon, "     channel: %" PRId64 ":%" PRId64 "\n",
+                           chan->value->channel_type, chan->value->channel_id);
+
+            channel_name = "unknown";
+            if (chan->value->channel_type > 0 &&
+                chan->value->channel_type < ARRAY_SIZE(channel_names) &&
+                channel_names[chan->value->channel_type]) {
+                channel_name = channel_names[chan->value->channel_type];
+            }
+
+            monitor_printf(mon, "     channel name: %s\n", channel_name);
+        }
+    }
+
+out:
+    qapi_free_SpiceInfo(info);
+}
+#endif
+
+void hmp_info_balloon(Monitor *mon, const QDict *qdict)
+{
+    BalloonInfo *info;
+    Error *err = NULL;
+
+    info = qmp_query_balloon(&err);
+    if (err) {
+        error_report_err(err);
+        return;
+    }
+
+    monitor_printf(mon, "balloon: actual=%" PRId64 "\n", info->actual >> 20);
+
+    qapi_free_BalloonInfo(info);
+}
+
+static void hmp_info_pci_device(Monitor *mon, const PciDeviceInfo *dev)
+{
+    PciMemoryRegionList *region;
+
+    monitor_printf(mon, "  Bus %2" PRId64 ", ", dev->bus);
+    monitor_printf(mon, "device %3" PRId64 ", function %" PRId64 ":\n",
+                   dev->slot, dev->function);
+    monitor_printf(mon, "    ");
+
+    if (dev->class_info->has_desc) {
+        monitor_printf(mon, "%s", dev->class_info->desc);
+    } else {
+        monitor_printf(mon, "Class %04" PRId64, dev->class_info->q_class);
+    }
+
+    monitor_printf(mon, ": PCI device %04" PRIx64 ":%04" PRIx64 "\n",
+                   dev->id->vendor, dev->id->device);
+
+    if (dev->has_irq) {
+        monitor_printf(mon, "      IRQ %" PRId64 ".\n", dev->irq);
+    }
+
+    if (dev->has_pci_bridge) {
+        monitor_printf(mon, "      BUS %" PRId64 ".\n",
+                       dev->pci_bridge->bus->number);
+        monitor_printf(mon, "      secondary bus %" PRId64 ".\n",
+                       dev->pci_bridge->bus->secondary);
+        monitor_printf(mon, "      subordinate bus %" PRId64 ".\n",
+                       dev->pci_bridge->bus->subordinate);
+
+        monitor_printf(mon, "      IO range [0x%04"PRIx64", 0x%04"PRIx64"]\n",
+                       dev->pci_bridge->bus->io_range->base,
+                       dev->pci_bridge->bus->io_range->limit);
+
+        monitor_printf(mon,
+                       "      memory range [0x%08"PRIx64", 0x%08"PRIx64"]\n",
+                       dev->pci_bridge->bus->memory_range->base,
+                       dev->pci_bridge->bus->memory_range->limit);
+
+        monitor_printf(mon, "      prefetchable memory range "
+                       "[0x%08"PRIx64", 0x%08"PRIx64"]\n",
+                       dev->pci_bridge->bus->prefetchable_range->base,
+                       dev->pci_bridge->bus->prefetchable_range->limit);
+    }
+
+    for (region = dev->regions; region; region = region->next) {
+        uint64_t addr, size;
+
+        addr = region->value->address;
+        size = region->value->size;
+
+        monitor_printf(mon, "      BAR%" PRId64 ": ", region->value->bar);
+
+        if (!strcmp(region->value->type, "io")) {
+            monitor_printf(mon, "I/O at 0x%04" PRIx64
+                                " [0x%04" PRIx64 "].\n",
+                           addr, addr + size - 1);
+        } else {
+            monitor_printf(mon, "%d bit%s memory at 0x%08" PRIx64
+                               " [0x%08" PRIx64 "].\n",
+                           region->value->mem_type_64 ? 64 : 32,
+                           region->value->prefetch ? " prefetchable" : "",
+                           addr, addr + size - 1);
+        }
+    }
+
+    monitor_printf(mon, "      id \"%s\"\n", dev->qdev_id);
+
+    if (dev->has_pci_bridge) {
+        if (dev->pci_bridge->has_devices) {
+            PciDeviceInfoList *cdev;
+            for (cdev = dev->pci_bridge->devices; cdev; cdev = cdev->next) {
+                hmp_info_pci_device(mon, cdev->value);
+            }
+        }
+    }
+}
+
+static int hmp_info_irq_foreach(Object *obj, void *opaque)
+{
+    InterruptStatsProvider *intc;
+    InterruptStatsProviderClass *k;
+    Monitor *mon = opaque;
+
+    if (object_dynamic_cast(obj, TYPE_INTERRUPT_STATS_PROVIDER)) {
+        intc = INTERRUPT_STATS_PROVIDER(obj);
+        k = INTERRUPT_STATS_PROVIDER_GET_CLASS(obj);
+        uint64_t *irq_counts;
+        unsigned int nb_irqs, i;
+        if (k->get_statistics &&
+            k->get_statistics(intc, &irq_counts, &nb_irqs)) {
+            if (nb_irqs > 0) {
+                monitor_printf(mon, "IRQ statistics for %s:\n",
+                               object_get_typename(obj));
+                for (i = 0; i < nb_irqs; i++) {
+                    if (irq_counts[i] > 0) {
+                        monitor_printf(mon, "%2d: %" PRId64 "\n", i,
+                                       irq_counts[i]);
+                    }
+                }
+            }
+        } else {
+            monitor_printf(mon, "IRQ statistics not available for %s.\n",
+                           object_get_typename(obj));
+        }
+    }
+
+    return 0;
+}
+
+void hmp_info_irq(Monitor *mon, const QDict *qdict)
+{
+    object_child_foreach_recursive(object_get_root(),
+                                   hmp_info_irq_foreach, mon);
+}
+
+static int hmp_info_pic_foreach(Object *obj, void *opaque)
+{
+    InterruptStatsProvider *intc;
+    InterruptStatsProviderClass *k;
+    Monitor *mon = opaque;
+
+    if (object_dynamic_cast(obj, TYPE_INTERRUPT_STATS_PROVIDER)) {
+        intc = INTERRUPT_STATS_PROVIDER(obj);
+        k = INTERRUPT_STATS_PROVIDER_GET_CLASS(obj);
+        if (k->print_info) {
+            k->print_info(intc, mon);
+        } else {
+            monitor_printf(mon, "Interrupt controller information not available for %s.\n",
+                           object_get_typename(obj));
+        }
+    }
+
+    return 0;
+}
+
+void hmp_info_pic(Monitor *mon, const QDict *qdict)
+{
+    object_child_foreach_recursive(object_get_root(),
+                                   hmp_info_pic_foreach, mon);
+}
+
+void hmp_info_pci(Monitor *mon, const QDict *qdict)
+{
+    PciInfoList *info_list, *info;
+    Error *err = NULL;
+
+    info_list = qmp_query_pci(&err);
+    if (err) {
+        monitor_printf(mon, "PCI devices not supported\n");
+        error_free(err);
+        return;
+    }
+
+    for (info = info_list; info; info = info->next) {
+        PciDeviceInfoList *dev;
+
+        for (dev = info->value->devices; dev; dev = dev->next) {
+            hmp_info_pci_device(mon, dev->value);
+        }
+    }
+
+    qapi_free_PciInfoList(info_list);
+}
+
+void hmp_info_block_jobs(Monitor *mon, const QDict *qdict)
+{
+    BlockJobInfoList *list;
+    Error *err = NULL;
+
+    list = qmp_query_block_jobs(&err);
+    assert(!err);
+
+    if (!list) {
+        monitor_printf(mon, "No active jobs\n");
+        return;
+    }
+
+    while (list) {
+        if (strcmp(list->value->type, "stream") == 0) {
+            monitor_printf(mon, "Streaming device %s: Completed %" PRId64
+                           " of %" PRId64 " bytes, speed limit %" PRId64
+                           " bytes/s\n",
+                           list->value->device,
+                           list->value->offset,
+                           list->value->len,
+                           list->value->speed);
+        } else {
+            monitor_printf(mon, "Type %s, device %s: Completed %" PRId64
+                           " of %" PRId64 " bytes, speed limit %" PRId64
+                           " bytes/s\n",
+                           list->value->type,
+                           list->value->device,
+                           list->value->offset,
+                           list->value->len,
+                           list->value->speed);
+        }
+        list = list->next;
+    }
+
+    qapi_free_BlockJobInfoList(list);
+}
+
+void hmp_info_tpm(Monitor *mon, const QDict *qdict)
+{
+    TPMInfoList *info_list, *info;
+    Error *err = NULL;
+    unsigned int c = 0;
+    TPMPassthroughOptions *tpo;
+
+    info_list = qmp_query_tpm(&err);
+    if (err) {
+        monitor_printf(mon, "TPM device not supported\n");
+        error_free(err);
+        return;
+    }
+
+    if (info_list) {
+        monitor_printf(mon, "TPM device:\n");
+    }
+
+    for (info = info_list; info; info = info->next) {
+        TPMInfo *ti = info->value;
+        monitor_printf(mon, " tpm%d: model=%s\n",
+                       c, TpmModel_lookup[ti->model]);
+
+        monitor_printf(mon, "  \\ %s: type=%s",
+                       ti->id, TpmTypeOptionsKind_lookup[ti->options->type]);
+
+        switch (ti->options->type) {
+        case TPM_TYPE_OPTIONS_KIND_PASSTHROUGH:
+            tpo = ti->options->u.passthrough.data;
+            monitor_printf(mon, "%s%s%s%s",
+                           tpo->has_path ? ",path=" : "",
+                           tpo->has_path ? tpo->path : "",
+                           tpo->has_cancel_path ? ",cancel-path=" : "",
+                           tpo->has_cancel_path ? tpo->cancel_path : "");
+            break;
+        case TPM_TYPE_OPTIONS_KIND__MAX:
+            break;
+        }
+        monitor_printf(mon, "\n");
+        c++;
+    }
+    qapi_free_TPMInfoList(info_list);
+}
+
+void hmp_quit(Monitor *mon, const QDict *qdict)
+{
+    monitor_suspend(mon);
+    qmp_quit(NULL);
+}
+
+void hmp_stop(Monitor *mon, const QDict *qdict)
+{
+    qmp_stop(NULL);
+}
+
+void hmp_system_reset(Monitor *mon, const QDict *qdict)
+{
+    qmp_system_reset(NULL);
+}
+
+void hmp_system_powerdown(Monitor *mon, const QDict *qdict)
+{
+    qmp_system_powerdown(NULL);
+}
+
+void hmp_cpu(Monitor *mon, const QDict *qdict)
+{
+    int64_t cpu_index;
+
+    /* XXX: drop the monitor_set_cpu() usage when all HMP commands that
+            use it are converted to the QAPI */
+    cpu_index = qdict_get_int(qdict, "index");
+    if (monitor_set_cpu(cpu_index) < 0) {
+        monitor_printf(mon, "invalid CPU index\n");
+    }
+}
+
+void hmp_memsave(Monitor *mon, const QDict *qdict)
+{
+    uint32_t size = qdict_get_int(qdict, "size");
+    const char *filename = qdict_get_str(qdict, "filename");
+    uint64_t addr = qdict_get_int(qdict, "val");
+    Error *err = NULL;
+    int cpu_index = monitor_get_cpu_index();
+
+    if (cpu_index < 0) {
+        monitor_printf(mon, "No CPU available\n");
+        return;
+    }
+
+    qmp_memsave(addr, size, filename, true, cpu_index, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_pmemsave(Monitor *mon, const QDict *qdict)
+{
+    uint32_t size = qdict_get_int(qdict, "size");
+    const char *filename = qdict_get_str(qdict, "filename");
+    uint64_t addr = qdict_get_int(qdict, "val");
+    Error *err = NULL;
+
+    qmp_pmemsave(addr, size, filename, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_ringbuf_write(Monitor *mon, const QDict *qdict)
+{
+    const char *chardev = qdict_get_str(qdict, "device");
+    const char *data = qdict_get_str(qdict, "data");
+    Error *err = NULL;
+
+    qmp_ringbuf_write(chardev, data, false, 0, &err);
+
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_ringbuf_read(Monitor *mon, const QDict *qdict)
+{
+    uint32_t size = qdict_get_int(qdict, "size");
+    const char *chardev = qdict_get_str(qdict, "device");
+    char *data;
+    Error *err = NULL;
+    int i;
+
+    data = qmp_ringbuf_read(chardev, size, false, 0, &err);
+    if (err) {
+        error_report_err(err);
+        return;
+    }
+
+    for (i = 0; data[i]; i++) {
+        unsigned char ch = data[i];
+
+        if (ch == '\\') {
+            monitor_printf(mon, "\\\\");
+        } else if ((ch < 0x20 && ch != '\n' && ch != '\t') || ch == 0x7F) {
+            monitor_printf(mon, "\\u%04X", ch);
+        } else {
+            monitor_printf(mon, "%c", ch);
+        }
+
+    }
+    monitor_printf(mon, "\n");
+    g_free(data);
+}
+
+static void hmp_cont_cb(void *opaque, int err)
+{
+    if (!err) {
+        qmp_cont(NULL);
+    }
+}
+
+static bool key_is_missing(const BlockInfo *bdev)
+{
+    return (bdev->inserted && bdev->inserted->encryption_key_missing);
+}
+
+void hmp_cont(Monitor *mon, const QDict *qdict)
+{
+    BlockInfoList *bdev_list, *bdev;
+    Error *err = NULL;
+
+    bdev_list = qmp_query_block(NULL);
+    for (bdev = bdev_list; bdev; bdev = bdev->next) {
+        if (key_is_missing(bdev->value)) {
+            monitor_read_block_device_key(mon, bdev->value->device,
+                                          hmp_cont_cb, NULL);
+            goto out;
+        }
+    }
+
+    qmp_cont(&err);
+    hmp_handle_error(mon, &err);
+
+out:
+    qapi_free_BlockInfoList(bdev_list);
+}
+
+void hmp_system_wakeup(Monitor *mon, const QDict *qdict)
+{
+    qmp_system_wakeup(NULL);
+}
+
+void hmp_nmi(Monitor *mon, const QDict *qdict)
+{
+    Error *err = NULL;
+
+    qmp_inject_nmi(&err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_set_link(Monitor *mon, const QDict *qdict)
+{
+    const char *name = qdict_get_str(qdict, "name");
+    bool up = qdict_get_bool(qdict, "up");
+    Error *err = NULL;
+
+    qmp_set_link(name, up, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_block_passwd(Monitor *mon, const QDict *qdict)
+{
+    const char *device = qdict_get_str(qdict, "device");
+    const char *password = qdict_get_str(qdict, "password");
+    Error *err = NULL;
+
+    qmp_block_passwd(true, device, false, NULL, password, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_balloon(Monitor *mon, const QDict *qdict)
+{
+    int64_t value = qdict_get_int(qdict, "value");
+    Error *err = NULL;
+
+    qmp_balloon(value, &err);
+    if (err) {
+        error_report_err(err);
+    }
+}
+
+void hmp_block_resize(Monitor *mon, const QDict *qdict)
+{
+    const char *device = qdict_get_str(qdict, "device");
+    int64_t size = qdict_get_int(qdict, "size");
+    Error *err = NULL;
+
+    qmp_block_resize(true, device, false, NULL, size, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_drive_mirror(Monitor *mon, const QDict *qdict)
+{
+    const char *filename = qdict_get_str(qdict, "target");
+    const char *format = qdict_get_try_str(qdict, "format");
+    bool reuse = qdict_get_try_bool(qdict, "reuse", false);
+    bool full = qdict_get_try_bool(qdict, "full", false);
+    Error *err = NULL;
+    DriveMirror mirror = {
+        .device = (char *)qdict_get_str(qdict, "device"),
+        .target = (char *)filename,
+        .has_format = !!format,
+        .format = (char *)format,
+        .sync = full ? MIRROR_SYNC_MODE_FULL : MIRROR_SYNC_MODE_TOP,
+        .has_mode = true,
+        .mode = reuse ? NEW_IMAGE_MODE_EXISTING : NEW_IMAGE_MODE_ABSOLUTE_PATHS,
+        .unmap = true,
+    };
+
+    if (!filename) {
+        error_setg(&err, QERR_MISSING_PARAMETER, "target");
+        hmp_handle_error(mon, &err);
+        return;
+    }
+    qmp_drive_mirror(&mirror, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_drive_backup(Monitor *mon, const QDict *qdict)
+{
+    const char *device = qdict_get_str(qdict, "device");
+    const char *filename = qdict_get_str(qdict, "target");
+    const char *format = qdict_get_try_str(qdict, "format");
+    bool reuse = qdict_get_try_bool(qdict, "reuse", false);
+    bool full = qdict_get_try_bool(qdict, "full", false);
+    bool compress = qdict_get_try_bool(qdict, "compress", false);
+    Error *err = NULL;
+    DriveBackup backup = {
+        .device = (char *)device,
+        .target = (char *)filename,
+        .has_format = !!format,
+        .format = (char *)format,
+        .sync = full ? MIRROR_SYNC_MODE_FULL : MIRROR_SYNC_MODE_TOP,
+        .has_mode = true,
+        .mode = reuse ? NEW_IMAGE_MODE_EXISTING : NEW_IMAGE_MODE_ABSOLUTE_PATHS,
+        .has_compress = !!compress,
+        .compress = compress,
+    };
+
+    if (!filename) {
+        error_setg(&err, QERR_MISSING_PARAMETER, "target");
+        hmp_handle_error(mon, &err);
+        return;
+    }
+
+    qmp_drive_backup(&backup, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_snapshot_blkdev(Monitor *mon, const QDict *qdict)
+{
+    const char *device = qdict_get_str(qdict, "device");
+    const char *filename = qdict_get_try_str(qdict, "snapshot-file");
+    const char *format = qdict_get_try_str(qdict, "format");
+    bool reuse = qdict_get_try_bool(qdict, "reuse", false);
+    enum NewImageMode mode;
+    Error *err = NULL;
+
+    if (!filename) {
+        /* In the future, if 'snapshot-file' is not specified, the snapshot
+           will be taken internally. Today it's actually required. */
+        error_setg(&err, QERR_MISSING_PARAMETER, "snapshot-file");
+        hmp_handle_error(mon, &err);
+        return;
+    }
+
+    mode = reuse ? NEW_IMAGE_MODE_EXISTING : NEW_IMAGE_MODE_ABSOLUTE_PATHS;
+    qmp_blockdev_snapshot_sync(true, device, false, NULL,
+                               filename, false, NULL,
+                               !!format, format,
+                               true, mode, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_snapshot_blkdev_internal(Monitor *mon, const QDict *qdict)
+{
+    const char *device = qdict_get_str(qdict, "device");
+    const char *name = qdict_get_str(qdict, "name");
+    Error *err = NULL;
+
+    qmp_blockdev_snapshot_internal_sync(device, name, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_snapshot_delete_blkdev_internal(Monitor *mon, const QDict *qdict)
+{
+    const char *device = qdict_get_str(qdict, "device");
+    const char *name = qdict_get_str(qdict, "name");
+    const char *id = qdict_get_try_str(qdict, "id");
+    Error *err = NULL;
+
+    qmp_blockdev_snapshot_delete_internal_sync(device, !!id, id,
+                                               true, name, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_migrate_cancel(Monitor *mon, const QDict *qdict)
+{
+    qmp_migrate_cancel(NULL);
+}
+
+void hmp_migrate_incoming(Monitor *mon, const QDict *qdict)
+{
+    Error *err = NULL;
+    const char *uri = qdict_get_str(qdict, "uri");
+
+    qmp_migrate_incoming(uri, &err);
+
+    hmp_handle_error(mon, &err);
+}
+
+/* Kept for backwards compatibility */
+void hmp_migrate_set_downtime(Monitor *mon, const QDict *qdict)
+{
+    double value = qdict_get_double(qdict, "value");
+    qmp_migrate_set_downtime(value, NULL);
+}
+
+void hmp_migrate_set_cache_size(Monitor *mon, const QDict *qdict)
+{
+    int64_t value = qdict_get_int(qdict, "value");
+    Error *err = NULL;
+
+    qmp_migrate_set_cache_size(value, &err);
+    if (err) {
+        error_report_err(err);
+        return;
+    }
+}
+
+/* Kept for backwards compatibility */
+void hmp_migrate_set_speed(Monitor *mon, const QDict *qdict)
+{
+    int64_t value = qdict_get_int(qdict, "value");
+    qmp_migrate_set_speed(value, NULL);
+}
+
+void hmp_migrate_set_capability(Monitor *mon, const QDict *qdict)
+{
+    const char *cap = qdict_get_str(qdict, "capability");
+    bool state = qdict_get_bool(qdict, "state");
+    Error *err = NULL;
+    MigrationCapabilityStatusList *caps = g_malloc0(sizeof(*caps));
+    int i;
+
+    for (i = 0; i < MIGRATION_CAPABILITY__MAX; i++) {
+        if (strcmp(cap, MigrationCapability_lookup[i]) == 0) {
+            caps->value = g_malloc0(sizeof(*caps->value));
+            caps->value->capability = i;
+            caps->value->state = state;
+            caps->next = NULL;
+            qmp_migrate_set_capabilities(caps, &err);
+            break;
+        }
+    }
+
+    if (i == MIGRATION_CAPABILITY__MAX) {
+        error_setg(&err, QERR_INVALID_PARAMETER, cap);
+    }
+
+    qapi_free_MigrationCapabilityStatusList(caps);
+
+    if (err) {
+        error_report_err(err);
+    }
+}
+
+void hmp_migrate_set_parameter(Monitor *mon, const QDict *qdict)
+{
+    const char *param = qdict_get_str(qdict, "parameter");
+    const char *valuestr = qdict_get_str(qdict, "value");
+    uint64_t valuebw = 0;
+    long valueint = 0;
+    Error *err = NULL;
+    bool use_int_value = false;
+    int i, ret;
+
+    for (i = 0; i < MIGRATION_PARAMETER__MAX; i++) {
+        if (strcmp(param, MigrationParameter_lookup[i]) == 0) {
+            MigrationParameters p = { 0 };
+            switch (i) {
+            case MIGRATION_PARAMETER_COMPRESS_LEVEL:
+                p.has_compress_level = true;
+                use_int_value = true;
+                break;
+            case MIGRATION_PARAMETER_COMPRESS_THREADS:
+                p.has_compress_threads = true;
+                use_int_value = true;
+                break;
+            case MIGRATION_PARAMETER_DECOMPRESS_THREADS:
+                p.has_decompress_threads = true;
+                use_int_value = true;
+                break;
+            case MIGRATION_PARAMETER_CPU_THROTTLE_INITIAL:
+                p.has_cpu_throttle_initial = true;
+                use_int_value = true;
+                break;
+            case MIGRATION_PARAMETER_CPU_THROTTLE_INCREMENT:
+                p.has_cpu_throttle_increment = true;
+                use_int_value = true;
+                break;
+            case MIGRATION_PARAMETER_TLS_CREDS:
+                p.has_tls_creds = true;
+                p.tls_creds = (char *) valuestr;
+                break;
+            case MIGRATION_PARAMETER_TLS_HOSTNAME:
+                p.has_tls_hostname = true;
+                p.tls_hostname = (char *) valuestr;
+                break;
+            case MIGRATION_PARAMETER_MAX_BANDWIDTH:
+                p.has_max_bandwidth = true;
+                ret = qemu_strtosz_MiB(valuestr, NULL, &valuebw);
+                if (ret < 0 || valuebw > INT64_MAX
+                    || (size_t)valuebw != valuebw) {
+                    error_setg(&err, "Invalid size %s", valuestr);
+                    goto cleanup;
+                }
+                p.max_bandwidth = valuebw;
+                break;
+            case MIGRATION_PARAMETER_DOWNTIME_LIMIT:
+                p.has_downtime_limit = true;
+                use_int_value = true;
+                break;
+            case MIGRATION_PARAMETER_X_CHECKPOINT_DELAY:
+                p.has_x_checkpoint_delay = true;
+                use_int_value = true;
+                break;
+            }
+
+            if (use_int_value) {
+                if (qemu_strtol(valuestr, NULL, 10, &valueint) < 0) {
+                    error_setg(&err, "Unable to parse '%s' as an int",
+                               valuestr);
+                    goto cleanup;
+                }
+                /* Set all integers; only one has_FOO will be set, and
+                 * the code ignores the remaining values */
+                p.compress_level = valueint;
+                p.compress_threads = valueint;
+                p.decompress_threads = valueint;
+                p.cpu_throttle_initial = valueint;
+                p.cpu_throttle_increment = valueint;
+                p.downtime_limit = valueint;
+                p.x_checkpoint_delay = valueint;
+            }
+
+            qmp_migrate_set_parameters(&p, &err);
+            break;
+        }
+    }
+
+    if (i == MIGRATION_PARAMETER__MAX) {
+        error_setg(&err, QERR_INVALID_PARAMETER, param);
+    }
+
+ cleanup:
+    if (err) {
+        error_report_err(err);
+    }
+}
+
+void hmp_client_migrate_info(Monitor *mon, const QDict *qdict)
+{
+    Error *err = NULL;
+    const char *protocol = qdict_get_str(qdict, "protocol");
+    const char *hostname = qdict_get_str(qdict, "hostname");
+    bool has_port        = qdict_haskey(qdict, "port");
+    int port             = qdict_get_try_int(qdict, "port", -1);
+    bool has_tls_port    = qdict_haskey(qdict, "tls-port");
+    int tls_port         = qdict_get_try_int(qdict, "tls-port", -1);
+    const char *cert_subject = qdict_get_try_str(qdict, "cert-subject");
+
+    qmp_client_migrate_info(protocol, hostname,
+                            has_port, port, has_tls_port, tls_port,
+                            !!cert_subject, cert_subject, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_migrate_start_postcopy(Monitor *mon, const QDict *qdict)
+{
+    Error *err = NULL;
+    qmp_migrate_start_postcopy(&err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_x_colo_lost_heartbeat(Monitor *mon, const QDict *qdict)
+{
+    Error *err = NULL;
+
+    qmp_x_colo_lost_heartbeat(&err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_set_password(Monitor *mon, const QDict *qdict)
+{
+    const char *protocol  = qdict_get_str(qdict, "protocol");
+    const char *password  = qdict_get_str(qdict, "password");
+    const char *connected = qdict_get_try_str(qdict, "connected");
+    Error *err = NULL;
+
+    qmp_set_password(protocol, password, !!connected, connected, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_expire_password(Monitor *mon, const QDict *qdict)
+{
+    const char *protocol  = qdict_get_str(qdict, "protocol");
+    const char *whenstr = qdict_get_str(qdict, "time");
+    Error *err = NULL;
+
+    qmp_expire_password(protocol, whenstr, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_eject(Monitor *mon, const QDict *qdict)
+{
+    bool force = qdict_get_try_bool(qdict, "force", false);
+    const char *device = qdict_get_str(qdict, "device");
+    Error *err = NULL;
+
+    qmp_eject(true, device, false, NULL, true, force, &err);
+    hmp_handle_error(mon, &err);
+}
+
+static void hmp_change_read_arg(void *opaque, const char *password,
+                                void *readline_opaque)
+{
+    qmp_change_vnc_password(password, NULL);
+    monitor_read_command(opaque, 1);
+}
+
+void hmp_change(Monitor *mon, const QDict *qdict)
+{
+    const char *device = qdict_get_str(qdict, "device");
+    const char *target = qdict_get_str(qdict, "target");
+    const char *arg = qdict_get_try_str(qdict, "arg");
+    const char *read_only = qdict_get_try_str(qdict, "read-only-mode");
+    BlockdevChangeReadOnlyMode read_only_mode = 0;
+    Error *err = NULL;
+
+    if (strcmp(device, "vnc") == 0) {
+        if (read_only) {
+            monitor_printf(mon,
+                           "Parameter 'read-only-mode' is invalid for VNC\n");
+            return;
+        }
+        if (strcmp(target, "passwd") == 0 ||
+            strcmp(target, "password") == 0) {
+            if (!arg) {
+                monitor_read_password(mon, hmp_change_read_arg, NULL);
+                return;
+            }
+        }
+        qmp_change("vnc", target, !!arg, arg, &err);
+    } else {
+        if (read_only) {
+            read_only_mode =
+                qapi_enum_parse(BlockdevChangeReadOnlyMode_lookup,
+                                read_only, BLOCKDEV_CHANGE_READ_ONLY_MODE__MAX,
+                                BLOCKDEV_CHANGE_READ_ONLY_MODE_RETAIN, &err);
+            if (err) {
+                hmp_handle_error(mon, &err);
+                return;
+            }
+        }
+
+        qmp_blockdev_change_medium(true, device, false, NULL, target,
+                                   !!arg, arg, !!read_only, read_only_mode,
+                                   &err);
+        if (err &&
+            error_get_class(err) == ERROR_CLASS_DEVICE_ENCRYPTED) {
+            error_free(err);
+            monitor_read_block_device_key(mon, device, NULL, NULL);
+            return;
+        }
+    }
+
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_block_set_io_throttle(Monitor *mon, const QDict *qdict)
+{
+    Error *err = NULL;
+    BlockIOThrottle throttle = {
+        .has_device = true,
+        .device = (char *) qdict_get_str(qdict, "device"),
+        .bps = qdict_get_int(qdict, "bps"),
+        .bps_rd = qdict_get_int(qdict, "bps_rd"),
+        .bps_wr = qdict_get_int(qdict, "bps_wr"),
+        .iops = qdict_get_int(qdict, "iops"),
+        .iops_rd = qdict_get_int(qdict, "iops_rd"),
+        .iops_wr = qdict_get_int(qdict, "iops_wr"),
+    };
+
+    qmp_block_set_io_throttle(&throttle, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_block_stream(Monitor *mon, const QDict *qdict)
+{
+    Error *error = NULL;
+    const char *device = qdict_get_str(qdict, "device");
+    const char *base = qdict_get_try_str(qdict, "base");
+    int64_t speed = qdict_get_try_int(qdict, "speed", 0);
+
+    qmp_block_stream(true, device, device, base != NULL, base, false, NULL,
+                     false, NULL, qdict_haskey(qdict, "speed"), speed,
+                     true, BLOCKDEV_ON_ERROR_REPORT, &error);
+
+    hmp_handle_error(mon, &error);
+}
+
+void hmp_block_job_set_speed(Monitor *mon, const QDict *qdict)
+{
+    Error *error = NULL;
+    const char *device = qdict_get_str(qdict, "device");
+    int64_t value = qdict_get_int(qdict, "speed");
+
+    qmp_block_job_set_speed(device, value, &error);
+
+    hmp_handle_error(mon, &error);
+}
+
+void hmp_block_job_cancel(Monitor *mon, const QDict *qdict)
+{
+    Error *error = NULL;
+    const char *device = qdict_get_str(qdict, "device");
+    bool force = qdict_get_try_bool(qdict, "force", false);
+
+    qmp_block_job_cancel(device, true, force, &error);
+
+    hmp_handle_error(mon, &error);
+}
+
+void hmp_block_job_pause(Monitor *mon, const QDict *qdict)
+{
+    Error *error = NULL;
+    const char *device = qdict_get_str(qdict, "device");
+
+    qmp_block_job_pause(device, &error);
+
+    hmp_handle_error(mon, &error);
+}
+
+void hmp_block_job_resume(Monitor *mon, const QDict *qdict)
+{
+    Error *error = NULL;
+    const char *device = qdict_get_str(qdict, "device");
+
+    qmp_block_job_resume(device, &error);
+
+    hmp_handle_error(mon, &error);
+}
+
+void hmp_block_job_complete(Monitor *mon, const QDict *qdict)
+{
+    Error *error = NULL;
+    const char *device = qdict_get_str(qdict, "device");
+
+    qmp_block_job_complete(device, &error);
+
+    hmp_handle_error(mon, &error);
+}
+
+typedef struct HMPMigrationStatus
+{
+    QEMUTimer *timer;
+    Monitor *mon;
+    bool is_block_migration;
+} HMPMigrationStatus;
+
+static void hmp_migrate_status_cb(void *opaque)
+{
+    HMPMigrationStatus *status = opaque;
+    MigrationInfo *info;
+
+    info = qmp_query_migrate(NULL);
+    if (!info->has_status || info->status == MIGRATION_STATUS_ACTIVE ||
+        info->status == MIGRATION_STATUS_SETUP) {
+        if (info->has_disk) {
+            int progress;
+
+            if (info->disk->remaining) {
+                progress = info->disk->transferred * 100 / info->disk->total;
+            } else {
+                progress = 100;
+            }
+
+            monitor_printf(status->mon, "Completed %d %%\r", progress);
+            monitor_flush(status->mon);
+        }
+
+        timer_mod(status->timer, qemu_clock_get_ms(QEMU_CLOCK_REALTIME) + 1000);
+    } else {
+        if (status->is_block_migration) {
+            monitor_printf(status->mon, "\n");
+        }
+        if (info->has_error_desc) {
+            error_report("%s", info->error_desc);
+        }
+        monitor_resume(status->mon);
+        timer_del(status->timer);
+        g_free(status);
+    }
+
+    qapi_free_MigrationInfo(info);
+}
+
+void hmp_migrate(Monitor *mon, const QDict *qdict)
+{
+    bool detach = qdict_get_try_bool(qdict, "detach", false);
+    bool blk = qdict_get_try_bool(qdict, "blk", false);
+    bool inc = qdict_get_try_bool(qdict, "inc", false);
+    const char *uri = qdict_get_str(qdict, "uri");
+    Error *err = NULL;
+
+    qmp_migrate(uri, !!blk, blk, !!inc, inc, false, false, &err);
+    if (err) {
+        error_report_err(err);
+        return;
+    }
+
+    if (!detach) {
+        HMPMigrationStatus *status;
+
+        if (monitor_suspend(mon) < 0) {
+            monitor_printf(mon, "terminal does not allow synchronous "
+                           "migration, continuing detached\n");
+            return;
+        }
+
+        status = g_malloc0(sizeof(*status));
+        status->mon = mon;
+        status->is_block_migration = blk || inc;
+        status->timer = timer_new_ms(QEMU_CLOCK_REALTIME, hmp_migrate_status_cb,
+                                          status);
+        timer_mod(status->timer, qemu_clock_get_ms(QEMU_CLOCK_REALTIME));
+    }
+}
+
+void hmp_device_add(Monitor *mon, const QDict *qdict)
+{
+    Error *err = NULL;
+
+    qmp_device_add((QDict *)qdict, NULL, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_device_del(Monitor *mon, const QDict *qdict)
+{
+    const char *id = qdict_get_str(qdict, "id");
+    Error *err = NULL;
+
+    qmp_device_del(id, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_dump_guest_memory(Monitor *mon, const QDict *qdict)
+{
+    Error *err = NULL;
+    bool paging = qdict_get_try_bool(qdict, "paging", false);
+    bool zlib = qdict_get_try_bool(qdict, "zlib", false);
+    bool lzo = qdict_get_try_bool(qdict, "lzo", false);
+    bool snappy = qdict_get_try_bool(qdict, "snappy", false);
+    const char *file = qdict_get_str(qdict, "filename");
+    bool has_begin = qdict_haskey(qdict, "begin");
+    bool has_length = qdict_haskey(qdict, "length");
+    bool has_detach = qdict_haskey(qdict, "detach");
+    int64_t begin = 0;
+    int64_t length = 0;
+    bool detach = false;
+    enum DumpGuestMemoryFormat dump_format = DUMP_GUEST_MEMORY_FORMAT_ELF;
+    char *prot;
+
+    if (zlib + lzo + snappy > 1) {
+        error_setg(&err, "only one of '-z|-l|-s' can be set");
+        hmp_handle_error(mon, &err);
+        return;
+    }
+
+    if (zlib) {
+        dump_format = DUMP_GUEST_MEMORY_FORMAT_KDUMP_ZLIB;
+    }
+
+    if (lzo) {
+        dump_format = DUMP_GUEST_MEMORY_FORMAT_KDUMP_LZO;
+    }
+
+    if (snappy) {
+        dump_format = DUMP_GUEST_MEMORY_FORMAT_KDUMP_SNAPPY;
+    }
+
+    if (has_begin) {
+        begin = qdict_get_int(qdict, "begin");
+    }
+    if (has_length) {
+        length = qdict_get_int(qdict, "length");
+    }
+    if (has_detach) {
+        detach = qdict_get_bool(qdict, "detach");
+    }
+
+    prot = g_strconcat("file:", file, NULL);
+
+    qmp_dump_guest_memory(paging, prot, true, detach, has_begin, begin,
+                          has_length, length, true, dump_format, &err);
+    hmp_handle_error(mon, &err);
+    g_free(prot);
+}
+
+void hmp_netdev_add(Monitor *mon, const QDict *qdict)
+{
+    Error *err = NULL;
+    QemuOpts *opts;
+
+    opts = qemu_opts_from_qdict(qemu_find_opts("netdev"), qdict, &err);
+    if (err) {
+        goto out;
+    }
+
+    netdev_add(opts, &err);
+    if (err) {
+        qemu_opts_del(opts);
+    }
+
+out:
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_netdev_del(Monitor *mon, const QDict *qdict)
+{
+    const char *id = qdict_get_str(qdict, "id");
+    Error *err = NULL;
+
+    qmp_netdev_del(id, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_object_add(Monitor *mon, const QDict *qdict)
+{
+    Error *err = NULL;
+    QemuOpts *opts;
+    Object *obj = NULL;
+
+    opts = qemu_opts_from_qdict(qemu_find_opts("object"), qdict, &err);
+    if (err) {
+        hmp_handle_error(mon, &err);
+        return;
+    }
+
+    obj = user_creatable_add_opts(opts, &err);
+    qemu_opts_del(opts);
+
+    if (err) {
+        hmp_handle_error(mon, &err);
+    }
+    if (obj) {
+        object_unref(obj);
+    }
+}
+
+void hmp_getfd(Monitor *mon, const QDict *qdict)
+{
+    const char *fdname = qdict_get_str(qdict, "fdname");
+    Error *err = NULL;
+
+    qmp_getfd(fdname, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_closefd(Monitor *mon, const QDict *qdict)
+{
+    const char *fdname = qdict_get_str(qdict, "fdname");
+    Error *err = NULL;
+
+    qmp_closefd(fdname, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_sendkey(Monitor *mon, const QDict *qdict)
+{
+    const char *keys = qdict_get_str(qdict, "keys");
+    KeyValueList *keylist, *head = NULL, *tmp = NULL;
+    int has_hold_time = qdict_haskey(qdict, "hold-time");
+    int hold_time = qdict_get_try_int(qdict, "hold-time", -1);
+    Error *err = NULL;
+    char *separator;
+    int keyname_len;
+
+    while (1) {
+        separator = strchr(keys, '-');
+        keyname_len = separator ? separator - keys : strlen(keys);
+
+        /* Be compatible with old interface, convert user inputted "<" */
+        if (keys[0] == '<' && keyname_len == 1) {
+            keys = "less";
+            keyname_len = 4;
+        }
+
+        keylist = g_malloc0(sizeof(*keylist));
+        keylist->value = g_malloc0(sizeof(*keylist->value));
+
+        if (!head) {
+            head = keylist;
+        }
+        if (tmp) {
+            tmp->next = keylist;
+        }
+        tmp = keylist;
+
+        if (strstart(keys, "0x", NULL)) {
+            char *endp;
+            int value = strtoul(keys, &endp, 0);
+            assert(endp <= keys + keyname_len);
+            if (endp != keys + keyname_len) {
+                goto err_out;
+            }
+            keylist->value->type = KEY_VALUE_KIND_NUMBER;
+            keylist->value->u.number.data = value;
+        } else {
+            int idx = index_from_key(keys, keyname_len);
+            if (idx == Q_KEY_CODE__MAX) {
+                goto err_out;
+            }
+            keylist->value->type = KEY_VALUE_KIND_QCODE;
+            keylist->value->u.qcode.data = idx;
+        }
+
+        if (!separator) {
+            break;
+        }
+        keys = separator + 1;
+    }
+
+    qmp_send_key(head, has_hold_time, hold_time, &err);
+    hmp_handle_error(mon, &err);
+
+out:
+    qapi_free_KeyValueList(head);
+    return;
+
+err_out:
+    monitor_printf(mon, "invalid parameter: %.*s\n", keyname_len, keys);
+    goto out;
+}
+
+void hmp_screendump(Monitor *mon, const QDict *qdict)
+{
+    const char *filename = qdict_get_str(qdict, "filename");
+    Error *err = NULL;
+
+    qmp_screendump(filename, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_nbd_server_start(Monitor *mon, const QDict *qdict)
+{
+    const char *uri = qdict_get_str(qdict, "uri");
+    bool writable = qdict_get_try_bool(qdict, "writable", false);
+    bool all = qdict_get_try_bool(qdict, "all", false);
+    Error *local_err = NULL;
+    BlockInfoList *block_list, *info;
+    SocketAddress *addr;
+
+    if (writable && !all) {
+        error_setg(&local_err, "-w only valid together with -a");
+        goto exit;
+    }
+
+    /* First check if the address is valid and start the server.  */
+    addr = socket_parse(uri, &local_err);
+    if (local_err != NULL) {
+        goto exit;
+    }
+
+    qmp_nbd_server_start(addr, false, NULL, &local_err);
+    qapi_free_SocketAddress(addr);
+    if (local_err != NULL) {
+        goto exit;
+    }
+
+    if (!all) {
+        return;
+    }
+
+    /* Then try adding all block devices.  If one fails, close all and
+     * exit.
+     */
+    block_list = qmp_query_block(NULL);
+
+    for (info = block_list; info; info = info->next) {
+        if (!info->value->has_inserted) {
+            continue;
+        }
+
+        qmp_nbd_server_add(info->value->device, true, writable, &local_err);
+
+        if (local_err != NULL) {
+            qmp_nbd_server_stop(NULL);
+            break;
+        }
+    }
+
+    qapi_free_BlockInfoList(block_list);
+
+exit:
+    hmp_handle_error(mon, &local_err);
+}
+
+void hmp_nbd_server_add(Monitor *mon, const QDict *qdict)
+{
+    const char *device = qdict_get_str(qdict, "device");
+    bool writable = qdict_get_try_bool(qdict, "writable", false);
+    Error *local_err = NULL;
+
+    qmp_nbd_server_add(device, true, writable, &local_err);
+
+    if (local_err != NULL) {
+        hmp_handle_error(mon, &local_err);
+    }
+}
+
+void hmp_nbd_server_stop(Monitor *mon, const QDict *qdict)
+{
+    Error *err = NULL;
+
+    qmp_nbd_server_stop(&err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_cpu_add(Monitor *mon, const QDict *qdict)
+{
+    int cpuid;
+    Error *err = NULL;
+
+    cpuid = qdict_get_int(qdict, "id");
+    qmp_cpu_add(cpuid, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_chardev_add(Monitor *mon, const QDict *qdict)
+{
+    const char *args = qdict_get_str(qdict, "args");
+    Error *err = NULL;
+    QemuOpts *opts;
+
+    opts = qemu_opts_parse_noisily(qemu_find_opts("chardev"), args, true);
+    if (opts == NULL) {
+        error_setg(&err, "Parsing chardev args failed");
+    } else {
+        qemu_chr_new_from_opts(opts, &err);
+        qemu_opts_del(opts);
+    }
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_chardev_remove(Monitor *mon, const QDict *qdict)
+{
+    Error *local_err = NULL;
+
+    qmp_chardev_remove(qdict_get_str(qdict, "id"), &local_err);
+    hmp_handle_error(mon, &local_err);
+}
+
+void hmp_qemu_io(Monitor *mon, const QDict *qdict)
+{
+    BlockBackend *blk;
+    BlockBackend *local_blk = NULL;
+    AioContext *aio_context;
+    const char* device = qdict_get_str(qdict, "device");
+    const char* command = qdict_get_str(qdict, "command");
+    Error *err = NULL;
+    int ret;
+
+    blk = blk_by_name(device);
+    if (!blk) {
+        BlockDriverState *bs = bdrv_lookup_bs(NULL, device, &err);
+        if (bs) {
+            blk = local_blk = blk_new(0, BLK_PERM_ALL);
+            ret = blk_insert_bs(blk, bs, &err);
+            if (ret < 0) {
+                goto fail;
+            }
+        } else {
+            goto fail;
+        }
+    }
+
+    aio_context = blk_get_aio_context(blk);
+    aio_context_acquire(aio_context);
+
+    /*
+     * Notably absent: Proper permission management. This is sad, but it seems
+     * almost impossible to achieve without changing the semantics and thereby
+     * limiting the use cases of the qemu-io HMP command.
+     *
+     * In an ideal world we would unconditionally create a new BlockBackend for
+     * qemuio_command(), but we have commands like 'reopen' and want them to
+     * take effect on the exact BlockBackend whose name the user passed instead
+     * of just on a temporary copy of it.
+     *
+     * Another problem is that deleting the temporary BlockBackend involves
+     * draining all requests on it first, but some qemu-iotests cases want to
+     * issue multiple aio_read/write requests and expect them to complete in
+     * the background while the monitor has already returned.
+     *
+     * This is also what prevents us from saving the original permissions and
+     * restoring them later: We can't revoke permissions until all requests
+     * have completed, and we don't know when that is nor can we really let
+     * anything else run before we have revoken them to avoid race conditions.
+     *
+     * What happens now is that command() in qemu-io-cmds.c can extend the
+     * permissions if necessary for the qemu-io command. And they simply stay
+     * extended, possibly resulting in a read-only guest device keeping write
+     * permissions. Ugly, but it appears to be the lesser evil.
+     */
+    qemuio_command(blk, command);
+
+    aio_context_release(aio_context);
+
+fail:
+    blk_unref(local_blk);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_object_del(Monitor *mon, const QDict *qdict)
+{
+    const char *id = qdict_get_str(qdict, "id");
+    Error *err = NULL;
+
+    user_creatable_del(id, &err);
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_info_memdev(Monitor *mon, const QDict *qdict)
+{
+    Error *err = NULL;
+    MemdevList *memdev_list = qmp_query_memdev(&err);
+    MemdevList *m = memdev_list;
+    Visitor *v;
+    char *str;
+
+    while (m) {
+        v = string_output_visitor_new(false, &str);
+        visit_type_uint16List(v, NULL, &m->value->host_nodes, NULL);
+        monitor_printf(mon, "memory backend: %s\n", m->value->id);
+        monitor_printf(mon, "  size:  %" PRId64 "\n", m->value->size);
+        monitor_printf(mon, "  merge: %s\n",
+                       m->value->merge ? "true" : "false");
+        monitor_printf(mon, "  dump: %s\n",
+                       m->value->dump ? "true" : "false");
+        monitor_printf(mon, "  prealloc: %s\n",
+                       m->value->prealloc ? "true" : "false");
+        monitor_printf(mon, "  policy: %s\n",
+                       HostMemPolicy_lookup[m->value->policy]);
+        visit_complete(v, &str);
+        monitor_printf(mon, "  host nodes: %s\n", str);
+
+        g_free(str);
+        visit_free(v);
+        m = m->next;
+    }
+
+    monitor_printf(mon, "\n");
+
+    qapi_free_MemdevList(memdev_list);
+}
+
+void hmp_info_memory_devices(Monitor *mon, const QDict *qdict)
+{
+    Error *err = NULL;
+    MemoryDeviceInfoList *info_list = qmp_query_memory_devices(&err);
+    MemoryDeviceInfoList *info;
+    MemoryDeviceInfo *value;
+    PCDIMMDeviceInfo *di;
+
+    for (info = info_list; info; info = info->next) {
+        value = info->value;
+
+        if (value) {
+            switch (value->type) {
+            case MEMORY_DEVICE_INFO_KIND_DIMM:
+                di = value->u.dimm.data;
+
+                monitor_printf(mon, "Memory device [%s]: \"%s\"\n",
+                               MemoryDeviceInfoKind_lookup[value->type],
+                               di->id ? di->id : "");
+                monitor_printf(mon, "  addr: 0x%" PRIx64 "\n", di->addr);
+                monitor_printf(mon, "  slot: %" PRId64 "\n", di->slot);
+                monitor_printf(mon, "  node: %" PRId64 "\n", di->node);
+                monitor_printf(mon, "  size: %" PRIu64 "\n", di->size);
+                monitor_printf(mon, "  memdev: %s\n", di->memdev);
+                monitor_printf(mon, "  hotplugged: %s\n",
+                               di->hotplugged ? "true" : "false");
+                monitor_printf(mon, "  hotpluggable: %s\n",
+                               di->hotpluggable ? "true" : "false");
+                break;
+            default:
+                break;
+            }
+        }
+    }
+
+    qapi_free_MemoryDeviceInfoList(info_list);
+}
+
+void hmp_info_iothreads(Monitor *mon, const QDict *qdict)
+{
+    IOThreadInfoList *info_list = qmp_query_iothreads(NULL);
+    IOThreadInfoList *info;
+    IOThreadInfo *value;
+
+    for (info = info_list; info; info = info->next) {
+        value = info->value;
+        monitor_printf(mon, "%s:\n", value->id);
+        monitor_printf(mon, "  thread_id=%" PRId64 "\n", value->thread_id);
+        monitor_printf(mon, "  poll-max-ns=%" PRId64 "\n", value->poll_max_ns);
+        monitor_printf(mon, "  poll-grow=%" PRId64 "\n", value->poll_grow);
+        monitor_printf(mon, "  poll-shrink=%" PRId64 "\n", value->poll_shrink);
+    }
+
+    qapi_free_IOThreadInfoList(info_list);
+}
+
+void hmp_qom_list(Monitor *mon, const QDict *qdict)
+{
+    const char *path = qdict_get_try_str(qdict, "path");
+    ObjectPropertyInfoList *list;
+    Error *err = NULL;
+
+    if (path == NULL) {
+        monitor_printf(mon, "/\n");
+        return;
+    }
+
+    list = qmp_qom_list(path, &err);
+    if (err == NULL) {
+        ObjectPropertyInfoList *start = list;
+        while (list != NULL) {
+            ObjectPropertyInfo *value = list->value;
+
+            monitor_printf(mon, "%s (%s)\n",
+                           value->name, value->type);
+            list = list->next;
+        }
+        qapi_free_ObjectPropertyInfoList(start);
+    }
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_qom_set(Monitor *mon, const QDict *qdict)
+{
+    const char *path = qdict_get_str(qdict, "path");
+    const char *property = qdict_get_str(qdict, "property");
+    const char *value = qdict_get_str(qdict, "value");
+    Error *err = NULL;
+    bool ambiguous = false;
+    Object *obj;
+
+    obj = object_resolve_path(path, &ambiguous);
+    if (obj == NULL) {
+        error_set(&err, ERROR_CLASS_DEVICE_NOT_FOUND,
+                  "Device '%s' not found", path);
+    } else {
+        if (ambiguous) {
+            monitor_printf(mon, "Warning: Path '%s' is ambiguous\n", path);
+        }
+        object_property_parse(obj, value, property, &err);
+    }
+    hmp_handle_error(mon, &err);
+}
+
+void hmp_rocker(Monitor *mon, const QDict *qdict)
+{
+    const char *name = qdict_get_str(qdict, "name");
+    RockerSwitch *rocker;
+    Error *err = NULL;
+
+    rocker = qmp_query_rocker(name, &err);
+    if (err != NULL) {
+        hmp_handle_error(mon, &err);
+        return;
+    }
+
+    monitor_printf(mon, "name: %s\n", rocker->name);
+    monitor_printf(mon, "id: 0x%" PRIx64 "\n", rocker->id);
+    monitor_printf(mon, "ports: %d\n", rocker->ports);
+
+    qapi_free_RockerSwitch(rocker);
+}
+
+void hmp_rocker_ports(Monitor *mon, const QDict *qdict)
+{
+    RockerPortList *list, *port;
+    const char *name = qdict_get_str(qdict, "name");
+    Error *err = NULL;
+
+    list = qmp_query_rocker_ports(name, &err);
+    if (err != NULL) {
+        hmp_handle_error(mon, &err);
+        return;
+    }
+
+    monitor_printf(mon, "            ena/    speed/ auto\n");
+    monitor_printf(mon, "      port  link    duplex neg?\n");
+
+    for (port = list; port; port = port->next) {
+        monitor_printf(mon, "%10s  %-4s   %-3s  %2s  %-3s\n",
+                       port->value->name,
+                       port->value->enabled ? port->value->link_up ?
+                       "up" : "down" : "!ena",
+                       port->value->speed == 10000 ? "10G" : "??",
+                       port->value->duplex ? "FD" : "HD",
+                       port->value->autoneg ? "Yes" : "No");
+    }
+
+    qapi_free_RockerPortList(list);
+}
+
+void hmp_rocker_of_dpa_flows(Monitor *mon, const QDict *qdict)
+{
+    RockerOfDpaFlowList *list, *info;
+    const char *name = qdict_get_str(qdict, "name");
+    uint32_t tbl_id = qdict_get_try_int(qdict, "tbl_id", -1);
+    Error *err = NULL;
+
+    list = qmp_query_rocker_of_dpa_flows(name, tbl_id != -1, tbl_id, &err);
+    if (err != NULL) {
+        hmp_handle_error(mon, &err);
+        return;
+    }
+
+    monitor_printf(mon, "prio tbl hits key(mask) --> actions\n");
+
+    for (info = list; info; info = info->next) {
+        RockerOfDpaFlow *flow = info->value;
+        RockerOfDpaFlowKey *key = flow->key;
+        RockerOfDpaFlowMask *mask = flow->mask;
+        RockerOfDpaFlowAction *action = flow->action;
+
+        if (flow->hits) {
+            monitor_printf(mon, "%-4d %-3d %-4" PRIu64,
+                           key->priority, key->tbl_id, flow->hits);
+        } else {
+            monitor_printf(mon, "%-4d %-3d     ",
+                           key->priority, key->tbl_id);
+        }
+
+        if (key->has_in_pport) {
+            monitor_printf(mon, " pport %d", key->in_pport);
+            if (mask->has_in_pport) {
+                monitor_printf(mon, "(0x%x)", mask->in_pport);
+            }
+        }
+
+        if (key->has_vlan_id) {
+            monitor_printf(mon, " vlan %d",
+                           key->vlan_id & VLAN_VID_MASK);
+            if (mask->has_vlan_id) {
+                monitor_printf(mon, "(0x%x)", mask->vlan_id);
+            }
+        }
+
+        if (key->has_tunnel_id) {
+            monitor_printf(mon, " tunnel %d", key->tunnel_id);
+            if (mask->has_tunnel_id) {
+                monitor_printf(mon, "(0x%x)", mask->tunnel_id);
+            }
+        }
+
+        if (key->has_eth_type) {
+            switch (key->eth_type) {
+            case 0x0806:
+                monitor_printf(mon, " ARP");
+                break;
+            case 0x0800:
+                monitor_printf(mon, " IP");
+                break;
+            case 0x86dd:
+                monitor_printf(mon, " IPv6");
+                break;
+            case 0x8809:
+                monitor_printf(mon, " LACP");
+                break;
+            case 0x88cc:
+                monitor_printf(mon, " LLDP");
+                break;
+            default:
+                monitor_printf(mon, " eth type 0x%04x", key->eth_type);
+                break;
+            }
+        }
+
+        if (key->has_eth_src) {
+            if ((strcmp(key->eth_src, "01:00:00:00:00:00") == 0) &&
+                (mask->has_eth_src) &&
+                (strcmp(mask->eth_src, "01:00:00:00:00:00") == 0)) {
+                monitor_printf(mon, " src <any mcast/bcast>");
+            } else if ((strcmp(key->eth_src, "00:00:00:00:00:00") == 0) &&
+                (mask->has_eth_src) &&
+                (strcmp(mask->eth_src, "01:00:00:00:00:00") == 0)) {
+                monitor_printf(mon, " src <any ucast>");
+            } else {
+                monitor_printf(mon, " src %s", key->eth_src);
+                if (mask->has_eth_src) {
+                    monitor_printf(mon, "(%s)", mask->eth_src);
+                }
+            }
+        }
+
+        if (key->has_eth_dst) {
+            if ((strcmp(key->eth_dst, "01:00:00:00:00:00") == 0) &&
+                (mask->has_eth_dst) &&
+                (strcmp(mask->eth_dst, "01:00:00:00:00:00") == 0)) {
+                monitor_printf(mon, " dst <any mcast/bcast>");
+            } else if ((strcmp(key->eth_dst, "00:00:00:00:00:00") == 0) &&
+                (mask->has_eth_dst) &&
+                (strcmp(mask->eth_dst, "01:00:00:00:00:00") == 0)) {
+                monitor_printf(mon, " dst <any ucast>");
+            } else {
+                monitor_printf(mon, " dst %s", key->eth_dst);
+                if (mask->has_eth_dst) {
+                    monitor_printf(mon, "(%s)", mask->eth_dst);
+                }
+            }
+        }
+
+        if (key->has_ip_proto) {
+            monitor_printf(mon, " proto %d", key->ip_proto);
+            if (mask->has_ip_proto) {
+                monitor_printf(mon, "(0x%x)", mask->ip_proto);
+            }
+        }
+
+        if (key->has_ip_tos) {
+            monitor_printf(mon, " TOS %d", key->ip_tos);
+            if (mask->has_ip_tos) {
+                monitor_printf(mon, "(0x%x)", mask->ip_tos);
+            }
+        }
+
+        if (key->has_ip_dst) {
+            monitor_printf(mon, " dst %s", key->ip_dst);
+        }
+
+        if (action->has_goto_tbl || action->has_group_id ||
+            action->has_new_vlan_id) {
+            monitor_printf(mon, " -->");
+        }
+
+        if (action->has_new_vlan_id) {
+            monitor_printf(mon, " apply new vlan %d",
+                           ntohs(action->new_vlan_id));
+        }
+
+        if (action->has_group_id) {
+            monitor_printf(mon, " write group 0x%08x", action->group_id);
+        }
+
+        if (action->has_goto_tbl) {
+            monitor_printf(mon, " goto tbl %d", action->goto_tbl);
+        }
+
+        monitor_printf(mon, "\n");
+    }
+
+    qapi_free_RockerOfDpaFlowList(list);
+}
+
+void hmp_rocker_of_dpa_groups(Monitor *mon, const QDict *qdict)
+{
+    RockerOfDpaGroupList *list, *g;
+    const char *name = qdict_get_str(qdict, "name");
+    uint8_t type = qdict_get_try_int(qdict, "type", 9);
+    Error *err = NULL;
+    bool set = false;
+
+    list = qmp_query_rocker_of_dpa_groups(name, type != 9, type, &err);
+    if (err != NULL) {
+        hmp_handle_error(mon, &err);
+        return;
+    }
+
+    monitor_printf(mon, "id (decode) --> buckets\n");
+
+    for (g = list; g; g = g->next) {
+        RockerOfDpaGroup *group = g->value;
+
+        monitor_printf(mon, "0x%08x", group->id);
+
+        monitor_printf(mon, " (type %s", group->type == 0 ? "L2 interface" :
+                                         group->type == 1 ? "L2 rewrite" :
+                                         group->type == 2 ? "L3 unicast" :
+                                         group->type == 3 ? "L2 multicast" :
+                                         group->type == 4 ? "L2 flood" :
+                                         group->type == 5 ? "L3 interface" :
+                                         group->type == 6 ? "L3 multicast" :
+                                         group->type == 7 ? "L3 ECMP" :
+                                         group->type == 8 ? "L2 overlay" :
+                                         "unknown");
+
+        if (group->has_vlan_id) {
+            monitor_printf(mon, " vlan %d", group->vlan_id);
+        }
+
+        if (group->has_pport) {
+            monitor_printf(mon, " pport %d", group->pport);
+        }
+
+        if (group->has_index) {
+            monitor_printf(mon, " index %d", group->index);
+        }
+
+        monitor_printf(mon, ") -->");
+
+        if (group->has_set_vlan_id && group->set_vlan_id) {
+            set = true;
+            monitor_printf(mon, " set vlan %d",
+                           group->set_vlan_id & VLAN_VID_MASK);
+        }
+
+        if (group->has_set_eth_src) {
+            if (!set) {
+                set = true;
+                monitor_printf(mon, " set");
+            }
+            monitor_printf(mon, " src %s", group->set_eth_src);
+        }
+
+        if (group->has_set_eth_dst) {
+            if (!set) {
+                set = true;
+                monitor_printf(mon, " set");
+            }
+            monitor_printf(mon, " dst %s", group->set_eth_dst);
+        }
+
+        set = false;
+
+        if (group->has_ttl_check && group->ttl_check) {
+            monitor_printf(mon, " check TTL");
+        }
+
+        if (group->has_group_id && group->group_id) {
+            monitor_printf(mon, " group id 0x%08x", group->group_id);
+        }
+
+        if (group->has_pop_vlan && group->pop_vlan) {
+            monitor_printf(mon, " pop vlan");
+        }
+
+        if (group->has_out_pport) {
+            monitor_printf(mon, " out pport %d", group->out_pport);
+        }
+
+        if (group->has_group_ids) {
+            struct uint32List *id;
+
+            monitor_printf(mon, " groups [");
+            for (id = group->group_ids; id; id = id->next) {
+                monitor_printf(mon, "0x%08x", id->value);
+                if (id->next) {
+                    monitor_printf(mon, ",");
+                }
+            }
+            monitor_printf(mon, "]");
+        }
+
+        monitor_printf(mon, "\n");
+    }
+
+    qapi_free_RockerOfDpaGroupList(list);
+}
+
+void hmp_info_dump(Monitor *mon, const QDict *qdict)
+{
+    DumpQueryResult *result = qmp_query_dump(NULL);
+
+    assert(result && result->status < DUMP_STATUS__MAX);
+    monitor_printf(mon, "Status: %s\n", DumpStatus_lookup[result->status]);
+
+    if (result->status == DUMP_STATUS_ACTIVE) {
+        float percent = 0;
+        assert(result->total != 0);
+        percent = 100.0 * result->completed / result->total;
+        monitor_printf(mon, "Finished: %.2f %%\n", percent);
+    }
+
+    qapi_free_DumpQueryResult(result);
+}
+
+void hmp_hotpluggable_cpus(Monitor *mon, const QDict *qdict)
+{
+    Error *err = NULL;
+    HotpluggableCPUList *l = qmp_query_hotpluggable_cpus(&err);
+    HotpluggableCPUList *saved = l;
+    CpuInstanceProperties *c;
+
+    if (err != NULL) {
+        hmp_handle_error(mon, &err);
+        return;
+    }
+
+    monitor_printf(mon, "Hotpluggable CPUs:\n");
+    while (l) {
+        monitor_printf(mon, "  type: \"%s\"\n", l->value->type);
+        monitor_printf(mon, "  vcpus_count: \"%" PRIu64 "\"\n",
+                       l->value->vcpus_count);
+        if (l->value->has_qom_path) {
+            monitor_printf(mon, "  qom_path: \"%s\"\n", l->value->qom_path);
+        }
+
+        c = l->value->props;
+        monitor_printf(mon, "  CPUInstance Properties:\n");
+        if (c->has_node_id) {
+            monitor_printf(mon, "    node-id: \"%" PRIu64 "\"\n", c->node_id);
+        }
+        if (c->has_socket_id) {
+            monitor_printf(mon, "    socket-id: \"%" PRIu64 "\"\n", c->socket_id);
+        }
+        if (c->has_core_id) {
+            monitor_printf(mon, "    core-id: \"%" PRIu64 "\"\n", c->core_id);
+        }
+        if (c->has_thread_id) {
+            monitor_printf(mon, "    thread-id: \"%" PRIu64 "\"\n", c->thread_id);
+        }
+
+        l = l->next;
+    }
+
+    qapi_free_HotpluggableCPUList(saved);
+}
+
+void hmp_info_vm_generation_id(Monitor *mon, const QDict *qdict)
+{
+    Error *err = NULL;
+    GuidInfo *info = qmp_query_vm_generation_id(&err);
+    if (info) {
+        monitor_printf(mon, "%s\n", info->guid);
+    }
+    hmp_handle_error(mon, &err);
+    qapi_free_GuidInfo(info);
+}
diff -rupN /home/prafull/Desktop/qemu-2.9.0/hmp.h /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hmp.h
--- /home/prafull/Desktop/qemu-2.9.0/hmp.h	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hmp.h	2018-05-28 13:06:05.000000000 +0530
@@ -57,6 +57,9 @@ void hmp_nmi(Monitor *mon, const QDict *
 void hmp_set_link(Monitor *mon, const QDict *qdict);
 void hmp_block_passwd(Monitor *mon, const QDict *qdict);
 void hmp_balloon(Monitor *mon, const QDict *qdict);
+/* Added by Bhavesh Singh. 2017.06.02. Begin add */
+void hmp_ssd_balloon(Monitor *mon, const QDict *qdict);
+/* Added by Bhavesh Singh. 2017.06.02. End add */
 void hmp_block_resize(Monitor *mon, const QDict *qdict);
 void hmp_snapshot_blkdev(Monitor *mon, const QDict *qdict);
 void hmp_snapshot_blkdev_internal(Monitor *mon, const QDict *qdict);
diff -rupN /home/prafull/Desktop/qemu-2.9.0/hw/block/dataplane/Makefile.objs /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/block/dataplane/Makefile.objs
--- /home/prafull/Desktop/qemu-2.9.0/hw/block/dataplane/Makefile.objs	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/block/dataplane/Makefile.objs	2018-05-28 13:06:18.000000000 +0530
@@ -1 +1,2 @@
 obj-y += virtio-blk.o
+obj-y += virtio-vssd.o
diff -rupN /home/prafull/Desktop/qemu-2.9.0/hw/block/dataplane/trace-events /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/block/dataplane/trace-events
--- /home/prafull/Desktop/qemu-2.9.0/hw/block/dataplane/trace-events	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/block/dataplane/trace-events	2018-05-28 13:06:18.000000000 +0530
@@ -3,3 +3,5 @@
 # hw/block/dataplane/virtio-blk.c
 virtio_blk_data_plane_start(void *s) "dataplane %p"
 virtio_blk_data_plane_stop(void *s) "dataplane %p"
+virtio_vssd_data_plane_start(void *s) "dataplane %p"
+virtio_vssd_data_plane_stop(void *s) "dataplane %p"
diff -rupN /home/prafull/Desktop/qemu-2.9.0/hw/block/dataplane/virtio-vssd.c /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/block/dataplane/virtio-vssd.c
--- /home/prafull/Desktop/qemu-2.9.0/hw/block/dataplane/virtio-vssd.c	1970-01-01 05:30:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/block/dataplane/virtio-vssd.c	2018-05-28 13:06:18.000000000 +0530
@@ -0,0 +1,277 @@
+/*
+ * Dedicated thread for virtio-blk I/O processing
+ *
+ * Copyright 2012 IBM, Corp.
+ * Copyright 2012 Red Hat, Inc. and/or its affiliates
+ *
+ * Authors:
+ *   Stefan Hajnoczi <stefanha@redhat.com>
+ *
+ * This work is licensed under the terms of the GNU GPL, version 2 or later.
+ * See the COPYING file in the top-level directory.
+ *
+ */
+
+#include "qemu/osdep.h"
+#include "qapi/error.h"
+#include "trace.h"
+#include "qemu/iov.h"
+#include "qemu/thread.h"
+#include "qemu/error-report.h"
+#include "hw/virtio/virtio-access.h"
+#include "sysemu/block-backend.h"
+#include "hw/virtio/virtio-vssd.h"
+#include "virtio-vssd.h"
+#include "block/aio.h"
+#include "hw/virtio/virtio-bus.h"
+#include "qom/object_interfaces.h"
+
+struct VirtIOVssdDataPlane {
+    bool starting;
+    bool stopping;
+
+    VirtIOVssdConf *conf;
+    VirtIODevice *vdev;
+    QEMUBH *bh;                     /* bh for guest notification */
+    unsigned long *batch_notify_vqs;
+
+    /* Note that these EventNotifiers are assigned by value.  This is
+     * fine as long as you do not call event_notifier_cleanup on them
+     * (because you don't own the file descriptor or handle; you just
+     * use it).
+     */
+    IOThread *iothread;
+    AioContext *ctx;
+};
+
+/* Raise an interrupt to signal guest, if necessary */
+void virtio_vssd_data_plane_notify(VirtIOVssdDataPlane *s, VirtQueue *vq)
+{
+    set_bit(virtio_get_queue_index(vq), s->batch_notify_vqs);
+    qemu_bh_schedule(s->bh);
+}
+
+static void notify_guest_bh(void *opaque)
+{
+    VirtIOVssdDataPlane *s = opaque;
+    unsigned nvqs = s->conf->num_queues;
+    unsigned long bitmap[BITS_TO_LONGS(nvqs)];
+    unsigned j;
+
+    memcpy(bitmap, s->batch_notify_vqs, sizeof(bitmap));
+    memset(s->batch_notify_vqs, 0, sizeof(bitmap));
+
+    for (j = 0; j < nvqs; j += BITS_PER_LONG) {
+        unsigned long bits = bitmap[j];
+
+        while (bits != 0) {
+            unsigned i = j + ctzl(bits);
+            VirtQueue *vq = virtio_get_queue(s->vdev, i);
+
+            virtio_notify_irqfd(s->vdev, vq);
+
+            bits &= bits - 1; /* clear right-most bit */
+        }
+    }
+}
+
+/* Context: QEMU global mutex held */
+void virtio_vssd_data_plane_create(VirtIODevice *vdev, VirtIOVssdConf *conf,
+                                  VirtIOVssdDataPlane **dataplane,
+                                  Error **errp)
+{
+    VirtIOVssdDataPlane *s;
+    BusState *qbus = BUS(qdev_get_parent_bus(DEVICE(vdev)));
+    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);
+
+    *dataplane = NULL;
+
+    if (conf->iothread) {
+        if (!k->set_guest_notifiers || !k->ioeventfd_assign) {
+            error_setg(errp,
+                       "device is incompatible with iothread "
+                       "(transport does not support notifiers)");
+            return;
+        }
+        if (!virtio_device_ioeventfd_enabled(vdev)) {
+            error_setg(errp, "ioeventfd is required for iothread");
+            return;
+        }
+
+        /* If dataplane is (re-)enabled while the guest is running there could
+         * be block jobs that can conflict.
+         */
+        if (blk_op_is_blocked(conf->conf.blk, BLOCK_OP_TYPE_DATAPLANE, errp)) {
+            error_prepend(errp, "cannot start virtio-blk dataplane: ");
+            return;
+        }
+    }
+    /* Don't try if transport does not support notifiers. */
+    if (!virtio_device_ioeventfd_enabled(vdev)) {
+        return;
+    }
+
+    s = g_new0(VirtIOVssdDataPlane, 1);
+    s->vdev = vdev;
+    s->conf = conf;
+
+    if (conf->iothread) {
+        s->iothread = conf->iothread;
+        object_ref(OBJECT(s->iothread));
+        s->ctx = iothread_get_aio_context(s->iothread);
+    } else {
+        s->ctx = qemu_get_aio_context();
+    }
+    s->bh = aio_bh_new(s->ctx, notify_guest_bh, s);
+    s->batch_notify_vqs = bitmap_new(conf->num_queues);
+
+    *dataplane = s;
+}
+
+/* Context: QEMU global mutex held */
+void virtio_vssd_data_plane_destroy(VirtIOVssdDataPlane *s)
+{
+    VirtIOVssd *vblk;
+
+    if (!s) {
+        return;
+    }
+
+    vblk = VIRTIO_VSSD(s->vdev);
+    assert(!vblk->dataplane_started);
+    g_free(s->batch_notify_vqs);
+    qemu_bh_delete(s->bh);
+    if (s->iothread) {
+        object_unref(OBJECT(s->iothread));
+    }
+    g_free(s);
+}
+
+static bool virtio_vssd_data_plane_handle_output(VirtIODevice *vdev,
+                                                VirtQueue *vq)
+{
+    VirtIOVssd *s = (VirtIOVssd *)vdev;
+
+    assert(s->dataplane);
+    assert(s->dataplane_started);
+
+    return true;
+    //ToDoUnais
+    //return virtio_vssd_handle_vq(s, vq);
+}
+
+/* Context: QEMU global mutex held */
+int virtio_vssd_data_plane_start(VirtIODevice *vdev)
+{
+    VirtIOVssd *vblk = VIRTIO_VSSD(vdev);
+    VirtIOVssdDataPlane *s = vblk->dataplane;
+    BusState *qbus = BUS(qdev_get_parent_bus(DEVICE(vblk)));
+    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);
+    unsigned i;
+    unsigned nvqs = s->conf->num_queues;
+    int r;
+
+    if (vblk->dataplane_started || s->starting) {
+        return 0;
+    }
+
+    s->starting = true;
+
+    /* Set up guest notifier (irq) */
+    r = k->set_guest_notifiers(qbus->parent, nvqs, true);
+    if (r != 0) {
+        fprintf(stderr, "virtio-blk failed to set guest notifier (%d), "
+                "ensure -enable-kvm is set\n", r);
+        goto fail_guest_notifiers;
+    }
+
+    /* Set up virtqueue notify */
+    for (i = 0; i < nvqs; i++) {
+        r = virtio_bus_set_host_notifier(VIRTIO_BUS(qbus), i, true);
+        if (r != 0) {
+            fprintf(stderr, "virtio-blk failed to set host notifier (%d)\n", r);
+            while (i--) {
+                virtio_bus_set_host_notifier(VIRTIO_BUS(qbus), i, false);
+            }
+            goto fail_guest_notifiers;
+        }
+    }
+
+    s->starting = false;
+    vblk->dataplane_started = true;
+    trace_virtio_vssd_data_plane_start(s);
+
+    blk_set_aio_context(s->conf->conf.blk, s->ctx);
+
+    /* Kick right away to begin processing requests already in vring */
+    for (i = 0; i < nvqs; i++) {
+        VirtQueue *vq = virtio_get_queue(s->vdev, i);
+
+        event_notifier_set(virtio_queue_get_host_notifier(vq));
+    }
+
+    /* Get this show started by hooking up our callbacks */
+    aio_context_acquire(s->ctx);
+    for (i = 0; i < nvqs; i++) {
+        VirtQueue *vq = virtio_get_queue(s->vdev, i);
+
+        virtio_queue_aio_set_host_notifier_handler(vq, s->ctx,
+                virtio_vssd_data_plane_handle_output);
+    }
+    aio_context_release(s->ctx);
+    return 0;
+
+  fail_guest_notifiers:
+    vblk->dataplane_disabled = true;
+    s->starting = false;
+    vblk->dataplane_started = true;
+    return -ENOSYS;
+}
+
+/* Context: QEMU global mutex held */
+void virtio_vssd_data_plane_stop(VirtIODevice *vdev)
+{
+    VirtIOVssd *vblk = VIRTIO_VSSD(vdev);
+    VirtIOVssdDataPlane *s = vblk->dataplane;
+    BusState *qbus = qdev_get_parent_bus(DEVICE(vblk));
+    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);
+    unsigned i;
+    unsigned nvqs = s->conf->num_queues;
+
+    if (!vblk->dataplane_started || s->stopping) {
+        return;
+    }
+
+    /* Better luck next time. */
+    if (vblk->dataplane_disabled) {
+        vblk->dataplane_disabled = false;
+        vblk->dataplane_started = false;
+        return;
+    }
+    s->stopping = true;
+    trace_virtio_vssd_data_plane_stop(s);
+
+    aio_context_acquire(s->ctx);
+
+    /* Stop notifications for new requests from guest */
+    for (i = 0; i < nvqs; i++) {
+        VirtQueue *vq = virtio_get_queue(s->vdev, i);
+
+        virtio_queue_aio_set_host_notifier_handler(vq, s->ctx, NULL);
+    }
+
+    /* Drain and switch bs back to the QEMU main loop */
+    blk_set_aio_context(s->conf->conf.blk, qemu_get_aio_context());
+
+    aio_context_release(s->ctx);
+
+    for (i = 0; i < nvqs; i++) {
+        virtio_bus_set_host_notifier(VIRTIO_BUS(qbus), i, false);
+    }
+
+    /* Clean up guest notifier (irq) */
+    k->set_guest_notifiers(qbus->parent, nvqs, false);
+
+    vblk->dataplane_started = false;
+    s->stopping = false;
+}
diff -rupN /home/prafull/Desktop/qemu-2.9.0/hw/block/dataplane/virtio-vssd.h /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/block/dataplane/virtio-vssd.h
--- /home/prafull/Desktop/qemu-2.9.0/hw/block/dataplane/virtio-vssd.h	1970-01-01 05:30:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/block/dataplane/virtio-vssd.h	2018-05-28 13:06:18.000000000 +0530
@@ -0,0 +1,31 @@
+/*
+ * Dedicated thread for virtio-blk I/O processing
+ *
+ * Copyright 2012 IBM, Corp.
+ * Copyright 2012 Red Hat, Inc. and/or its affiliates
+ *
+ * Authors:
+ *   Stefan Hajnoczi <stefanha@redhat.com>
+ *
+ * This work is licensed under the terms of the GNU GPL, version 2 or later.
+ * See the COPYING file in the top-level directory.
+ *
+ */
+
+#ifndef HW_DATAPLANE_VIRTIO_VSSD_H
+#define HW_DATAPLANE_VIRTIO_VSSD_H
+
+#include "hw/virtio/virtio.h"
+
+typedef struct VirtIOVssdDataPlane VirtIOVssdDataPlane;
+
+void virtio_vssd_data_plane_create(VirtIODevice *vdev, VirtIOVssdConf *conf,
+                                  VirtIOVssdDataPlane **dataplane,
+                                  Error **errp);
+void virtio_vssd_data_plane_destroy(VirtIOVssdDataPlane *s);
+void virtio_vssd_data_plane_notify(VirtIOVssdDataPlane *s, VirtQueue *vq);
+
+int virtio_vssd_data_plane_start(VirtIODevice *vdev);
+void virtio_vssd_data_plane_stop(VirtIODevice *vdev);
+
+#endif /* HW_DATAPLANE_VIRTIO_VSSd_H */
diff -rupN /home/prafull/Desktop/qemu-2.9.0/hw/block/Makefile.objs /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/block/Makefile.objs
--- /home/prafull/Desktop/qemu-2.9.0/hw/block/Makefile.objs	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/block/Makefile.objs	2018-05-28 13:06:18.000000000 +0530
@@ -12,4 +12,9 @@ common-obj-$(CONFIG_NVME_PCI) += nvme.o
 obj-$(CONFIG_SH4) += tc58128.o
 
 obj-$(CONFIG_VIRTIO) += virtio-blk.o
+obj-$(CONFIG_VIRTIO) += virtio-vssd.o
 obj-$(CONFIG_VIRTIO) += dataplane/
+
+# Added by Bhavesh Singh. 2017.05.10. Begin add #
+# obj-y += virtio-vssd.o
+# Added by Bhavesh Singh. 2017.05.10. End add #
diff -rupN /home/prafull/Desktop/qemu-2.9.0/hw/block/virtio-blk.c /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/block/virtio-blk.c
--- /home/prafull/Desktop/qemu-2.9.0/hw/block/virtio-blk.c	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/block/virtio-blk.c	2018-05-28 13:06:18.000000000 +0530
@@ -29,6 +29,9 @@
 #include "hw/virtio/virtio-bus.h"
 #include "hw/virtio/virtio-access.h"
 
+
+#include "qemu/timer.h"
+
 static void virtio_blk_init_request(VirtIOBlock *s, VirtQueue *vq,
                                     VirtIOBlockReq *req)
 {
@@ -366,6 +369,18 @@ static inline void submit_requests(Block
     }
 
     if (is_write) {
+
+        /*printf("\n\tvirt-blk:submit_requests: write: sector=%ld [%ld]. call blk_aio_pwritev. req = %x \n", 
+            sector_num,
+            sector_num << BDRV_SECTOR_BITS,
+            mrb->reqs[start]);
+
+        printf("\virt-blk:QEMUIOVector. iov=%x, niov=%d, nalloc=%d, size=%d \n",
+            qiov->iov,
+            qiov->niov,
+            qiov->nalloc,
+            qiov->size);*/
+
         blk_aio_pwritev(blk, sector_num << BDRV_SECTOR_BITS, qiov, 0,
                         virtio_blk_rw_complete, mrb->reqs[start]);
     } else {
@@ -394,6 +409,9 @@ static int multireq_compare(const void *
 
 static void virtio_blk_submit_multireq(BlockBackend *blk, MultiReqBuffer *mrb)
 {
+
+   // printf("virtio: virtio_blk_submit_multireq called \n");
+
     int i = 0, start = 0, num_reqs = 0, niov = 0, nb_sectors = 0;
     uint32_t max_transfer;
     int64_t sector_num = 0;
@@ -489,6 +507,9 @@ static int virtio_blk_handle_request(Vir
     VirtIOBlock *s = req->dev;
     VirtIODevice *vdev = VIRTIO_DEVICE(s);
 
+    //clock_t start, end;
+    //double cpu_time_used; 
+
     if (req->elem.out_num < 1 || req->elem.in_num < 1) {
         virtio_error(vdev, "virtio-blk missing headers");
         return -1;
@@ -526,21 +547,30 @@ static int virtio_blk_handle_request(Vir
         req->sector_num = virtio_ldq_p(VIRTIO_DEVICE(req->dev),
                                        &req->out.sector);
 
-        if (is_write) {
+        if (is_write) 
+        {
+            //printf("virtio: Received Write request.................................\n");
+            //start = clock(); 
+
             qemu_iovec_init_external(&req->qiov, iov, out_num);
             trace_virtio_blk_handle_write(req, req->sector_num,
                                           req->qiov.size / BDRV_SECTOR_SIZE);
-        } else {
+
+        } 
+        else 
+        {   
+            //printf("virtio: Received Read request\n"); 
+            //start = clock();
+
             qemu_iovec_init_external(&req->qiov, in_iov, in_num);
             trace_virtio_blk_handle_read(req, req->sector_num,
                                          req->qiov.size / BDRV_SECTOR_SIZE);
         }
 
-        if (!virtio_blk_sect_range_ok(req->dev, req->sector_num,
-                                      req->qiov.size)) {
+        if (!virtio_blk_sect_range_ok(req->dev, req->sector_num, req->qiov.size)) 
+        {
             virtio_blk_req_complete(req, VIRTIO_BLK_S_IOERR);
-            block_acct_invalid(blk_get_stats(req->dev->blk),
-                               is_write ? BLOCK_ACCT_WRITE : BLOCK_ACCT_READ);
+            block_acct_invalid(blk_get_stats(req->dev->blk), is_write ? BLOCK_ACCT_WRITE : BLOCK_ACCT_READ);
             virtio_blk_free_request(req);
             return 0;
         }
@@ -551,12 +581,20 @@ static int virtio_blk_handle_request(Vir
 
         /* merge would exceed maximum number of requests or IO direction
          * changes */
-        if (mrb->num_reqs > 0 && (mrb->num_reqs == VIRTIO_BLK_MAX_MERGE_REQS ||
-                                  is_write != mrb->is_write ||
-                                  !req->dev->conf.request_merging)) {
+        //printf("virtio: num_reqs=%d, VIRTIO_BLK_MAX_MERGE_REQS=32, is_write=%d, mrb->is_write=%d, req->dev->conf.request_merging=%d \n",
+        //     mrb->num_reqs, is_write, mrb->is_write, req->dev->conf.request_merging);
+
+        if (mrb->num_reqs > 0 && (mrb->num_reqs == VIRTIO_BLK_MAX_MERGE_REQS || is_write != mrb->is_write || !req->dev->conf.request_merging)) 
+        {
+           // printf("virtio: Calling submit_multireq from handle_request \n");
             virtio_blk_submit_multireq(req->dev->blk, mrb);
         }
 
+        // end = clock();
+        // cpu_time_used = ((double) (end - start)) / CLOCKS_PER_SEC;
+        // printf("virtio: %s Completed......\n", is_write ? "Write" : "Read");  
+        // printf("virtio: %s took %f seconds to execute--------------->>>> \n", is_write ? "Write" : "Read", cpu_time_used);
+
         assert(mrb->num_reqs < VIRTIO_BLK_MAX_MERGE_REQS);
         mrb->reqs[mrb->num_reqs++] = req;
         mrb->is_write = is_write;
@@ -606,17 +644,22 @@ bool virtio_blk_handle_vq(VirtIOBlock *s
 
         while ((req = virtio_blk_get_request(s, vq))) {
             progress = true;
-            if (virtio_blk_handle_request(req, &mrb)) {
+            if (virtio_blk_handle_request(req, &mrb)) 
+            {
                 virtqueue_detach_element(req->vq, &req->elem, 0);
                 virtio_blk_free_request(req);
                 break;
             }
+         
         }
 
         virtio_queue_set_notification(vq, 1);
     } while (!virtio_queue_empty(vq));
 
-    if (mrb.num_reqs) {
+
+    if (mrb.num_reqs) 
+    {
+       // printf("virtio: Calling multi req from handle_vq \n");
         virtio_blk_submit_multireq(s->blk, &mrb);
     }
 
@@ -726,6 +769,7 @@ static void virtio_blk_reset(VirtIODevic
  */
 static void virtio_blk_update_config(VirtIODevice *vdev, uint8_t *config)
 {
+    //printf("virtio-blk : virtio_blk_update_config \n");
     VirtIOBlock *s = VIRTIO_BLK(vdev);
     BlockConf *conf = &s->conf.conf;
     struct virtio_blk_config blkcfg;
@@ -908,6 +952,8 @@ static const BlockDevOps virtio_block_op
 
 static void virtio_blk_device_realize(DeviceState *dev, Error **errp)
 {
+    printf("\n\n\nvirtio: 'virtio_blk_device_realize' function called \n");   
+
     VirtIODevice *vdev = VIRTIO_DEVICE(dev);
     VirtIOBlock *s = VIRTIO_BLK(dev);
     VirtIOBlkConf *conf = &s->conf;
@@ -935,6 +981,8 @@ static void virtio_blk_device_realize(De
         error_propagate(errp, err);
         return;
     }
+
+
     s->original_wce = blk_enable_write_cache(conf->conf.blk);
     blkconf_geometry(&conf->conf, NULL, 65535, 255, 255, &err);
     if (err) {
@@ -942,14 +990,19 @@ static void virtio_blk_device_realize(De
         return;
     }
     blkconf_blocksizes(&conf->conf);
+    printf("virtio-vssd: Logical block size=%d, Physicalblock size=%d \n",
+        conf->conf.logical_block_size, conf->conf.physical_block_size);
 
+    
     virtio_init(vdev, "virtio-blk", VIRTIO_ID_BLOCK,
+
                 sizeof(struct virtio_blk_config));
 
     s->blk = conf->conf.blk;
     s->rq = NULL;
     s->sector_mask = (s->conf.conf.logical_block_size / BDRV_SECTOR_SIZE) - 1;
 
+    printf("virtio: Adding %d queues to virtio \n", conf->num_queues);
     for (i = 0; i < conf->num_queues; i++) {
         virtio_add_queue(vdev, 128, virtio_blk_handle_output);
     }
@@ -1019,6 +1072,8 @@ static Property virtio_blk_properties[]
 
 static void virtio_blk_class_init(ObjectClass *klass, void *data)
 {
+    printf("virtio: 'virtio_blk_class_init' function called \n");   
+
     DeviceClass *dc = DEVICE_CLASS(klass);
     VirtioDeviceClass *vdc = VIRTIO_DEVICE_CLASS(klass);
 
@@ -1048,7 +1103,11 @@ static const TypeInfo virtio_blk_info =
 
 static void virtio_register_types(void)
 {
+    printf("Registration \n");   
+
     type_register_static(&virtio_blk_info);
+    printf("\tvirtio-blk device registered. \n");
+
 }
 
 type_init(virtio_register_types)
diff -rupN /home/prafull/Desktop/qemu-2.9.0/hw/block/virtio-vssd.c /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/block/virtio-vssd.c
--- /home/prafull/Desktop/qemu-2.9.0/hw/block/virtio-vssd.c	1970-01-01 05:30:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/block/virtio-vssd.c	2018-06-14 15:50:20.547965000 +0530
@@ -0,0 +1,2548 @@
+/*
+ * Virtio Block Device
+ *
+ * Copyright IBM, Corp. 2007
+ *
+ * Authors:
+ *  Anthony Liguori   <aliguori@us.ibm.com>
+ *
+ * This work is licensed under the terms of the GNU GPL, version 2.  See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+
+#include "qemu/osdep.h"
+#include "qapi/error.h"
+#include "qemu-common.h"
+#include "qemu/iov.h"
+#include "qemu/error-report.h"
+#include "trace.h"
+#include "hw/block/block.h"
+#include "sysemu/block-backend.h"
+#include "sysemu/blockdev.h"
+#include "hw/virtio/virtio-vssd.h"
+#include "dataplane/virtio-vssd.h"
+#include "block/scsi.h"
+#ifdef __linux__
+# include <scsi/sg.h>
+#endif
+#include "hw/virtio/virtio-bus.h"
+#include "hw/virtio/virtio-access.h"
+
+#include "sysemu/balloon.h"
+#include "hw/virtio/virtio-balloon.h"
+
+#include "qemu/timer.h"
+#include "../../../../../server.h"
+
+
+#define BENCHMARKING 0
+#define DM_CACHE_STATS 69
+#define MSG_TYPE_DM_CACHE_STATISTICS 96
+
+unsigned long int backend_resize_duration = 0;
+uint32_t resize_request_id = 0; 
+
+
+/* NEW FUNCTION START */
+
+uint32_t bitmap_offset;
+uint32_t bitmap_reading;
+
+static uint64_t l2p(VirtIOVssd *vssd, uint64_t logical)
+{
+	//return logical+SSD_BLOCK_SIZE;
+
+	unsigned long int pbn = vssd->block_list[logical/SSD_BLOCK_SIZE];
+	if(pbn < 6 * SSD_BLOCK_SIZE)
+	{
+		printf("BUG \n. lbn: %lu. pbn: %lu \n",logical/SSD_BLOCK_SIZE, pbn);
+	}
+
+	return vssd->block_list[logical/SSD_BLOCK_SIZE] + (logical % SSD_BLOCK_SIZE);
+}
+
+/*static unsigned long long current_time(void)
+  {
+  unsigned int lo, hi;
+  __asm__ __volatile__ ("rdtsc" : "=a"(lo), "=d"(hi));                        
+  return ( (unsigned long long)lo)|( ((unsigned long long)hi)<<32 );  
+  }*/
+
+
+static unsigned long long current_time(void)
+{
+	static unsigned long long min = 0;
+	struct timeval cur_time;
+	unsigned long long cur;
+
+	gettimeofday(&cur_time, NULL);
+	cur = cur_time.tv_sec*1000000 + cur_time.tv_usec;
+
+	// if(min == 0)
+	//   min = cur;
+
+	return cur-min;
+}
+
+static char* get_date(char *buf)
+{
+	time_t rawtime;
+	struct tm * timeinfo;
+
+	time ( &rawtime );
+	timeinfo = localtime ( &rawtime );
+	sprintf(buf, "%s", asctime(timeinfo));
+	buf[strlen(buf)-1] = '\0';
+	// strpcy(buf, asctime (timeinfo));
+
+	// free(timeinfo);
+	return buf;
+}
+
+static unsigned long long int min(unsigned long long int n1, unsigned long long int n2)
+{
+	if(n1 < n2)
+		return n1;
+	else
+		return n2;
+}
+
+static unsigned long long int gb_to_sectors(int size_in_gb)
+{
+	return (unsigned long long int) size_in_gb * 2097152;
+
+	//	1GB = 2097152 sectors
+}
+
+static unsigned long long int gb_to_vssd_blocks(int size_in_gb)
+{
+	return (unsigned long long int) (gb_to_sectors(size_in_gb) / SSD_BLOCK_SIZE);
+
+	//	1GB = 2097152 sectors
+}
+
+
+
+static void get_access_to_shared_memory(struct SharedMemory *message)
+{
+	int ret;
+	pthread_mutex_lock(&message->lock);
+	while(message->msg_type != MSG_TYPE_BUF_FREE)
+	{
+		ret = pthread_mutex_trylock(&message->lock);
+		if(ret == 0)
+		{
+			printf("\n\nACCESS: Releasing shared memory: It should not happen\n\n\n\n\n\n");
+			exit(1);
+		}
+
+		printf("ACCESS: \t\tWaiting for buffer \n");
+		pthread_cond_wait(&message->vm_can_enter, &message->lock);
+		printf("ACCESS: \t\tWaking up");
+	}
+}
+
+static void release_shared_memory(struct SharedMemory *message, int line)
+{
+	printf("%llu: RLS-SHM: Called by %d \n", current_time(), line);
+	message->msg_type = MSG_TYPE_BUF_FREE;
+	message->source = TARGET_NONE;
+	message->destination = TARGET_NONE;
+
+	pthread_cond_broadcast(&message->vm_can_enter);
+	pthread_cond_broadcast(&message->cm_can_enter);
+
+	pthread_mutex_unlock(&message->lock);
+
+	printf("%llu: RLS-SHM: Completed. %d \n", current_time(), line);
+}
+
+static void wait_for_reply(struct SharedMemory *message, struct VirtIOVssd *vssd, int msg_type)
+{
+	int ret;
+	while(!(message->destination == vssd->vm_id && 
+				message->source == CM_LISTENER_ID && 
+				message->msg_type == msg_type))
+	{
+		printf("%llu: WAIT_FOR_REPLY: Dest=%d. Src=%d. MsgType=%d \n", current_time(), 
+			message->destination, message->source, message->msg_type);
+		ret = pthread_mutex_trylock(&message->lock);
+		if(ret == 0)
+		{
+			printf("\n\n\nWAIT_FOR_REPLY: BUG. Try lock succeeded \n\n\n");
+			exit(1);
+		}
+
+		printf("%llu: WAIT_FOR_REPLY: Going to sleep. Will release the lock \n", current_time());
+		pthread_cond_wait(&message->vm_can_enter, &message->lock);
+		printf("%llu: WAIT_FOR_REPLY: I woke up. I have the lock\n", current_time());
+	}
+}
+
+static void lock_and_wait_for_reply(struct SharedMemory *message, struct VirtIOVssd *vssd, int msg_type)
+{
+	pthread_mutex_lock(&message->lock);
+	wait_for_reply(message, vssd, msg_type);
+}
+
+static void send_message(struct SharedMemory *message, struct VirtIOVssd *vssd, int msg_type)
+{
+	message->msg_type = msg_type;
+	message->source = vssd->vm_id;
+	message->destination = CM_LISTENER_ID; 
+
+	pthread_cond_broadcast(&message->cm_can_enter);
+	// printf("VSSD: \t\tInformed the manager \n");
+}
+
+
+
+static void virtio_vssd_handle_resize_callback(VirtIODevice *vdev, VirtQueue *vq)
+{
+	#if BENCHMARKING
+	 	unsigned long int start_time = current_time();
+	 #endif
+
+	struct iovec *iov;
+	struct SharedMemory *message;
+	uint32_t in_num;
+	uint64_t logical_block_num=0;
+	unsigned long int i;
+	VirtIOVssd *vssd;
+	VirtIOVssdResizeInfo resize_info;
+	VirtQueueElement *elem;
+	int flag;
+
+	printf("\nVSSD: virtio_vssd_handle_resize_callback   called\n");
+
+	vssd = (VirtIOVssd *)vdev;
+	message = (struct SharedMemory*)vssd->message;
+
+	while((elem = virtqueue_pop(vq, sizeof(VirtQueueElement))) != NULL) 
+	{
+		iov = elem->in_sg;
+		in_num = elem->in_num;
+		iov_to_buf(iov, in_num, 0, &resize_info, sizeof(resize_info));
+
+		if(resize_info.operation == SSD_BALLOON_INFLATION)
+		{
+
+			printf("VSSD: \tSSD_BALLOON_INFLATION \n");
+			printf("VSSD: \t\tRequested blocks: %d. Number of blocks given by front-end: %d. Command=%ld \n", 
+					resize_info.current_requested_blocks, resize_info.given_blocks, vssd->command);
+
+			int total_send_to_cm = 0;
+
+			do
+			{
+				// printf("VSSD: \t\tLoop \n");
+				get_access_to_shared_memory(message);
+				inflation_reply *reply = (inflation_reply *)message->msg_content;
+
+				int j=0;
+				for(i=total_send_to_cm; i<resize_info.given_blocks && j<max_transfer_inflation; i++, j++) 
+				{
+					logical_block_num = resize_info.block_list[i];
+					unsigned long int pbn = vssd->block_list[logical_block_num] / SSD_BLOCK_SIZE;
+					reply->block_list[j] = pbn;
+					vssd->block_list[logical_block_num] = -1;
+					// printf("%ld [%ld-%ld]\t", i, logical_block_num, pbn);
+				}
+
+				reply->vm_id = vssd->vm_id;
+				reply->count = j;
+				total_send_to_cm += j;
+				if(total_send_to_cm < resize_info.given_blocks)
+					reply->flag = FLAG_ASK_ME_AGAIN;
+				else
+					reply->flag = resize_info.flag;
+				
+				send_message(message, vssd, MSG_TYPE_INFLATION_REPLY);
+				pthread_mutex_unlock(&message->lock);
+			}
+			while(total_send_to_cm < resize_info.given_blocks);
+
+			// todo  Atomicatlly update the command variable
+			vssd->current_capacity -= resize_info.given_blocks*SSD_BLOCK_SIZE;
+
+			if(resize_info.flag == FLAG_DO_NOT_ASK_ME && vssd->command == -1*resize_info.current_requested_blocks)
+				vssd->command = 0;
+			else
+				vssd->command += resize_info.given_blocks;
+
+			printf("VSSD: \t\tBlocks transfered to CM: %d. Current-capacity: %lu. Command: %ld. Flag: %s \n",
+					total_send_to_cm, vssd->current_capacity/SSD_BLOCK_SIZE, vssd->command,
+					resize_info.flag == FLAG_DO_NOT_ASK_ME? "FLAG_DO_NOT_ASK_ME": "FLAG_ASK_ME_AGAIN");
+
+			iov_from_buf(iov, in_num, 0, &resize_info, sizeof(resize_info));
+			virtqueue_push(vq, elem, sizeof(resize_info));
+			virtio_notify(vdev, vq);
+
+			// printf("virtio_notify ........... after inflation ........................... \n");
+
+		}
+		else if(resize_info.operation == SSD_BALLOON_DEFLATION)
+		{
+
+			// int max_transfer  = (BUF_SIZE_IN_BYTES-(3*sizeof(int)))/sizeof(struct physical_logical_block);
+
+			printf("VSSD: \t\tSSD_BALLOON_DEFLATION \n");
+			printf("VSSD: \t\t\tFront-end-requested-size: %d. command: %ld \n", 
+					resize_info.current_requested_blocks,
+					vssd->command);
+
+			get_access_to_shared_memory(message);
+
+			DeflationReq *deflation_req = (DeflationReq *)message->msg_content;
+			deflation_req->vm_id = vssd->vm_id;
+			deflation_req->requested_size = min(resize_info.current_requested_blocks, SSD_BALLOON_UNIT);
+
+			printf("VSSD: \t\t\tDeflation request \n");
+
+			printf("VSSD: \t\t\t\tRequested size = %ld \n", deflation_req->requested_size);
+
+			send_message(message, vssd, MSG_TYPE_DEFLATION_REQ);
+			wait_for_reply(message, vssd, MSG_TYPE_DEFLATION_BLK_ALCN);
+
+			DeflationBlockAllocation *delfation_alcn = (DeflationBlockAllocation *)message->msg_content;
+			printf("VSSD: \t\t\tDeflation block allocation \n");
+			printf("VSSD: \t\t\t\tNumber of blocks received from CM: %d \n", delfation_alcn->count);
+
+			unsigned long j=0, lbn=0;
+			for(lbn=0, j=0; lbn<vssd->capacity/SSD_BLOCK_SIZE && j<delfation_alcn->count;  lbn++)
+			{
+				if(vssd->block_list[lbn] == -1)
+				{
+					unsigned long int pbn = delfation_alcn->block_list[j].physical_block;
+					vssd->block_list[lbn] = pbn * SSD_BLOCK_SIZE;
+					resize_info.block_list[j] = lbn;
+					delfation_alcn->block_list[j].logical_block = lbn;
+
+					j++;
+
+					// printf(" %ld[%ld-%ld]   ", j, lbn, pbn);
+				}
+			}
+			flag = delfation_alcn->flag;
+			resize_info.flag = delfation_alcn->flag;
+			resize_info.given_blocks = j;
+
+			// Send deflation block allocation reply
+			send_message(message, vssd, MSG_TYPE_DEFLATION_BLK_ALCN_REPLY);
+			pthread_mutex_unlock(&message->lock);
+
+			vssd->current_capacity += resize_info.given_blocks*SSD_BLOCK_SIZE;
+			if(flag == FLAG_DO_NOT_ASK_ME && vssd->command == resize_info.current_requested_blocks)
+				vssd->command = 0;
+			else
+				vssd->command -= resize_info.given_blocks;
+
+			printf("VSSD: \t\t\tFront-end-requested-size: %d. Given = %d. Command=%ld. Current-capacity: %lu \n", 
+					resize_info.current_requested_blocks,
+					resize_info.given_blocks,
+					vssd->command,
+					vssd->current_capacity/SSD_BLOCK_SIZE);
+
+			iov_from_buf(iov, in_num, 0, &resize_info, sizeof(resize_info));
+			virtqueue_push(vq, elem, sizeof(resize_info));
+			virtio_notify(vdev, vq);
+		}
+		else if(resize_info.operation == DM_CACHE_STATS){
+				printf("VSSD: \t\t\tGot statistics from dmcache..\n\n");
+				printf("No of hits=%d No of acceesses=%d\n",resize_info.block_list[0],resize_info.block_list[1]);
+				float hit_ratio = (float)resize_info.block_list[0]/resize_info.block_list[1];
+				printf("Hit ratio is %f\n",hit_ratio);
+				get_access_to_shared_memory(message);
+				inflation_reply *reply = (inflation_reply *)message->msg_content;
+				reply->vm_id = vssd->vm_id;
+				reply->hit_ratio =(float)resize_info.block_list[0]/resize_info.block_list[1];
+			//	reply->count = 6;
+			//	total_send_to_cm += j;	
+				send_message(message, vssd, MSG_TYPE_DM_CACHE_STATISTICS);
+				printf("VSSD: \t\t\tMessage sent from my side..\n\n");
+				pthread_mutex_unlock(&message->lock);
+
+		}
+		else
+		{
+			printf("VSSD-ERROR: \n");
+		}
+
+		#if BENCHMARKING
+		 	unsigned long int end_time = current_time();
+		 	backend_resize_duration += end_time - start_time;
+		 #endif
+
+		// if(vssd->command != 0)
+		// {
+		// 	virtio_notify_config(vdev);			
+		// 	printf("Command != 0. Notify front endfor %ld blocks \n", vssd->command);
+		// }
+	}
+
+}
+
+
+static void virtio_ssd_balloon_to_target(void *opaque, int64_t target)
+{
+	VirtIOVssd *vssd = VIRTIO_VSSD(opaque);
+	VirtIODevice *vdev = VIRTIO_DEVICE(vssd);
+
+	vssd->command += target; // We add to ensure that the direction is preserved.
+
+	printf("VSSD: virtio_ssd_balloon_to_target \n");
+	printf("VSSD: \tTarget size : %ld \n", target);
+
+	virtio_notify_config(vdev);
+}
+
+static struct SharedMemory* get_shared_memory(void)
+{
+	int ShmID;
+	key_t ShmKEY;
+	struct SharedMemory *ptr;
+
+	/*Allocating shared memory segment*/
+	printf("VSSD: Setup shared memory \n");
+	ShmKEY = ftok("/tmp", 's');     
+	ShmID = shmget(ShmKEY, sizeof(struct SharedMemory), 0666); 
+	if (ShmID < 0) {
+		printf("VSSD-ERROR: shmget error \n");
+		return NULL;
+	}
+	printf("VSSD: \tKey = [%x] \n", ShmID);  
+
+	ptr = (struct SharedMemory *) shmat(ShmID, NULL, 0);
+	if (!ptr)
+	{
+		printf("VSSD-ERROR: shmat failed. \n");
+		return NULL;
+	} 
+	printf("VSSD: \tAttached the shared memory\n");
+	printf("VSSD: \tVirtual Address of the shared memory is : %p \n", ptr);
+
+	return ptr;
+}
+
+
+static void resize_fn(VirtIOVssd *vssd)
+{
+	#if BENCHMARKING
+		static int resize_id = 0;
+		resize_id++;
+
+		if(resize_id != 1)
+		{
+			printf("LOG: Resize id:\t%d\t Time Taken:\t%lu\t\n", resize_id-1, backend_resize_duration);
+			backend_resize_duration = 0;
+		}
+	 	unsigned long int start_time = current_time();
+	#endif
+
+	struct SharedMemory *message;
+	struct resize_req *req;
+
+	message = (struct SharedMemory *)vssd->message;
+	req = (struct resize_req *)message->msg_content;
+
+	// todo Update the command atomically 
+	// atomic {
+	printf("VSSD: Resize \n");
+	printf("VSSD: \tResize size: %ld blocks. current-command: %ld \n", req->size, vssd->command);
+
+	vssd->command += req->size;
+	if(vssd->command == req->size)
+	{
+		#if BENCHMARKING
+	 		unsigned long int end_time = current_time();
+	 		backend_resize_duration += end_time - start_time;
+	 		resize_request_id++;
+		#endif
+
+		printf("VSSD: \tNotifying front end for %ld blocks \n", vssd->command);
+		virtio_notify_config(VIRTIO_DEVICE(vssd));
+		printf("VSSD: \tNotified front end \n");
+	}
+	else
+	{
+		printf("VSSD: No need to notify the front-end. One operation is also going \n");
+	}
+	release_shared_memory(message, __LINE__);
+
+	// atomic }
+}
+
+static void* backend_listner_thread(void *arg)
+{
+	VirtIOVssd *vssd = (VirtIOVssd *)arg;
+	struct SharedMemory *message = (struct SharedMemory *)vssd->message; 
+
+	int ret;
+
+	
+	while(1)
+	{	
+		//printf("%llu: LISTENER: Aquiring lock \n", current_time());
+		pthread_mutex_lock(&message->lock);
+		//printf("%llu: LISTENER: I got the lock \n", current_time());
+
+
+		while(!(message->destination == vssd->vm_id 
+					&& message->msg_type != MSG_TYPE_BUF_FREE 
+					&& message->msg_type == MSG_TYPE_RESIZE_REQ))
+		{
+			ret = pthread_mutex_trylock(&message->lock);
+			if(ret == 0)
+			{
+				printf("\n\nLISTNER: BUG: It should not happen\n\n\n\n\n\n");
+				exit(1);
+			}
+
+		//	printf("%llu: LISTENER: Message is not for me. I am going sleep. will release the lock \n", current_time());
+			pthread_cond_wait(&message->vm_can_enter, &message->lock);
+		//	printf("%llu: LISTENER: I woke up, I have the lock \n", current_time());
+			
+		//	printf("%llu: LISTNER: Dest=%d. Source=%d.  Type=%d \n", current_time(), 
+		//		message->destination, message->source, message->msg_type);
+		}
+
+		printf("%llu: LISTENER: Message received. Type: %d \n", current_time(), message->msg_type);
+
+		switch(message->msg_type)
+		{
+			case MSG_TYPE_RESIZE_REQ:
+				resize_fn(vssd);
+				break;
+		}
+	}
+
+	return NULL;
+
+}
+
+static void setup_log_file(struct VirtIOVssd *vssd)
+{
+	char file_name[100];
+	char temp_buf[100];
+
+	sprintf(file_name,"/home/unaisp/ssd_sdb1/provm/benchmark/log/%s_%s.log",vssd->vm_name, get_date(temp_buf));
+	printf("VSSD: Creating log file. name = [%s] \n", file_name);
+	FILE *filep = fopen(file_name, "a");
+	if(!filep)
+	{
+		printf("VSSD-ERROR: Unable to open file %s \n", file_name);
+		exit(1);
+	}
+	vssd->log_file = filep;
+
+	sprintf(file_name,"/home/unaisp/ssd_sdb1/provm/benchmark/log/%s_%s_summary.log",vssd->vm_name, get_date(temp_buf));
+	printf("VSSD: Creating log summmary file. name = [%s] \n", file_name);
+	vssd->summary_file = fopen(file_name, "a");
+	if(!vssd->summary_file)
+	{
+		printf("VSSD-ERROR: Unable to open file %s \n", file_name);
+		exit(1);
+	}
+
+
+
+}
+static void write_to_log(struct VirtIOVssdReq *req)
+{
+
+	if(!VSSD_DEBUG_MODE)
+		return;
+
+	pthread_mutex_lock(&req->dev->lock);
+
+	struct statistics *s = &req->stats;
+
+	static unsigned long long base2 = 0;
+	if(base2 == 0)
+		base2 = s->request_pull_time;
+
+	unsigned long long base = s->request_pull_time;
+	static unsigned long long count = 0;
+
+	fprintf(req->dev->log_file, 
+			"%llu\t" "%lu\t%lu\t%lu\t%lu\t%lu\t%lu\t" "%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\n",
+			++count,
+			s->qiov_size,
+			s->qiov_iov_count,
+
+			//  After merging multiple requests
+			s->qiov_merge_count,
+			s->qiov_mereg_size,
+			s->qiov_merg_iov_count,
+
+			//  After dividing merged requests
+			s->sub_qiov_count,
+
+			s->request_pull_time,              //  First event
+			s->adding_to_mrb_time - base,
+			s->callback_time - base,                  //  Time at which call back for last sub-qiov received
+			s->request_push_time - base,              //  Final event
+
+			s->qiov_merging_start_time - base,
+			s->qiov_division_start_time - base,       //  Merging end time
+			s->first_sub_qiov_send_time - base,
+			s->last_sub_qiov_send_time - base,        //  Division end time
+			s->first_sub_qiov_callback_time - base,
+			s->last_sub_qiov_callback_time - base    //  Request call back time
+				);
+
+	static unsigned long long exp_count=0; 
+	static unsigned long long last_request_pull_time = 0;
+	static unsigned long long last_request_push_time = 0;
+	static unsigned long long requests_count = 0;
+	//	Number of requests per GB = requests_count/40
+	static unsigned long long sum_qiov_size = 0;
+	//	Qiov size per GB = sum_qiov_size / 40
+	//	Average qiov size = sum_qiov_size / requests count
+	static unsigned long long qiov_vectors_count = 0;
+	//	Number of qiov vectors per GB = qiov_vectors_ount/40
+	//	Average number of qiov-vectors per qiov = qiov_vectors_count / requests_count
+	static unsigned long long merging_count = 0;
+	static unsigned long long merged_qiov_count = 0;
+	//	Average merege size = merged_qiov_count / merging_count
+	static unsigned long long after_merging_qiov_count = 0;		// merging_count + (requests_count-merged_qiov_count)
+	static unsigned long long boundary_crossing_qiov_count = 0;		//After merging
+	static unsigned long long sub_qiov_count = 0;
+	//	Average partition size = sub_qiov_count / boundary_crossing_qiov_count
+	static unsigned long long pull_send_time_sum = 0;			//	Divide by request count to get average
+	static unsigned long long send_callback_time_sum = 0;		//	Divide by sub_qiov_count to get average
+	static unsigned long long callback_push_time_sum = 0;		//	Divide by request count to get average
+	static unsigned long long pull_push_time_sum = 0;
+	static unsigned long long merging_time_sum = 0;
+	static unsigned long long merge_send_time = 0;				//	after_merging_qiov_count
+
+	static unsigned long long first_request_time = 0 ;
+
+	// printf("Count = %llu. Delay = %llu \n", requests_count, (s->request_pull_time - last_request_pull_time)/1000000);
+
+	unsigned long long delay = (s->request_pull_time > last_request_pull_time ) ? (s->request_pull_time - last_request_pull_time)/1000000 : 0;
+	// 	printf("Pull-time=%llu. last-rqst-pull-time=%llu. delay = %llu. %llu \n "
+	// ,		s->request_pull_time-base2, last_request_pull_time-base2, (s->request_pull_time - last_request_pull_time)/1000000, delay);
+
+	if(delay > 10)
+	{
+		double duration = (last_request_push_time-first_request_time) > 0 ? (double)((last_request_push_time-first_request_time))/1000000 : 1;
+
+		last_request_pull_time = s->request_pull_time;
+
+		if(requests_count == 0)
+			requests_count = 1;
+
+		// printf("K. [%llu]  [%llu]. %f \n", after_merging_qiov_count, boundary_crossing_qiov_count, duration );
+
+		// fprintf(req->dev->summary_file,
+		printf(
+				"%llu\t%4.2f\t%f\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\n\n",
+				exp_count,
+				(double)(sum_qiov_size/(1024*1024))/duration,
+				duration,
+				requests_count,
+				requests_count/40,
+				(unsigned long long)sum_qiov_size/(1024*1024),
+				sum_qiov_size/(1024*1024*40),
+				sum_qiov_size/requests_count,
+				qiov_vectors_count,
+				qiov_vectors_count/40,
+				qiov_vectors_count/requests_count,
+				merging_count,
+				merged_qiov_count,
+				(merged_qiov_count == 0) ? 0 : merging_count/merged_qiov_count,
+				(merged_qiov_count == 0) ? 0 : requests_count - merged_qiov_count,
+				after_merging_qiov_count,
+				boundary_crossing_qiov_count,
+				sub_qiov_count,
+				(boundary_crossing_qiov_count == 0) ? 0 : sub_qiov_count/boundary_crossing_qiov_count,
+
+				(after_merging_qiov_count == 0) ? 0 : pull_send_time_sum/after_merging_qiov_count,		//	avg pull-send time
+				(after_merging_qiov_count == 0) ? 0 : send_callback_time_sum/after_merging_qiov_count,	//	avg send-callback time
+				callback_push_time_sum/requests_count,				//	avg callback-pull push time 	CHECK
+				pull_push_time_sum/requests_count,					//	avg pull-push time
+
+				merging_time_sum,			//	merging time per request
+				merge_send_time			//	merge-send time
+					);
+
+		/*fprintf(req->dev->summary_file,
+		  "%llu\t%4.2f\t%f\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\t%llu\n\n",
+		  exp_count,
+		  (double)(sum_qiov_size/(1024*1024))/duration,
+		  duration,
+		  requests_count,
+		  requests_count/40,
+		  (unsigned long long)sum_qiov_size/(1024*1024),
+		  sum_qiov_size/(1024*1024*40),
+		  sum_qiov_size/requests_count,
+		  qiov_vectors_count,
+		  qiov_vectors_count/40,
+		  qiov_vectors_count/requests_count,
+		  merging_count,
+		  merged_qiov_count,
+		  merging_count/merged_qiov_count,
+		  requests_count - merged_qiov_count,
+		  after_merging_qiov_count,
+		  boundary_crossing_qiov_count,
+		  sub_qiov_count,
+		  sub_qiov_count/boundary_crossing_qiov_count,
+
+		  pull_send_time_sum/after_merging_qiov_count,
+		  send_callback_time_sum/after_merging_qiov_count,
+		  callback_push_time_sum/after_merging_qiov_count,
+		  pull_push_time_sum/requests_count,
+		  merging_time_sum/after_merging_qiov_count,
+		  merge_send_time/after_merging_qiov_count,
+
+		  send_callback_time_sum/after_merging_qiov_count,
+		  merge_send_time/after_merging_qiov_count
+		  );*/
+
+
+
+		first_request_time = s->request_pull_time;
+
+		exp_count++; 
+		requests_count = 0;
+		//	Number of requests per GB = requests_count/40
+		sum_qiov_size = 0;
+		//	Qiov size per GB = sum_qiov_size / 40
+		//	Average qiov size = sum_qiov_size / requests count
+		qiov_vectors_count = 0;
+		//	Number of qiov vectors per GB = qiov_vectors_ount/40
+		//	Average number of qiov-vectors per qiov = qiov_vectors_count / requests_count
+		merging_count = 0;
+		merged_qiov_count = 0;
+		//	Average merege size = merged_qiov_count / merging_count
+		after_merging_qiov_count = 0;		// merging_count + (requests_count-merged_qiov_count)
+		boundary_crossing_qiov_count = 0;		//After merging
+		sub_qiov_count = 0;
+		//	Average partition size = sub_qiov_count / boundary_crossing_qiov_count
+		pull_send_time_sum = 0;			//	Divide by request count to get average
+		send_callback_time_sum = 0;		//	Divide by sub_qiov_count to get average
+		callback_push_time_sum = 0;		//	Divide by request count to get average
+		pull_push_time_sum = 0;
+		merging_time_sum = 0;
+		merge_send_time = 0;				//	after_merging_qiov_count
+	}
+
+	if(s->request_pull_time > last_request_pull_time) 	
+		last_request_pull_time = s->request_pull_time;
+	if(s->request_pull_time < first_request_time)
+		first_request_time = s->request_pull_time;
+
+	if(s->request_push_time > last_request_push_time)
+		last_request_push_time = s->request_push_time;
+
+	requests_count++;
+	sum_qiov_size += s->qiov_size;
+	qiov_vectors_count += s->qiov_iov_count;
+
+	merging_count += (s->qiov_merge_count>1) ? 1 : 0;
+	merged_qiov_count += (s->qiov_merge_count>1) ? s->qiov_merge_count : 0;
+
+	// after_merging_qiov_count = merging_count + (requests_count-merged_qiov_count);
+	after_merging_qiov_count += s->qiov_merge_count;
+
+	boundary_crossing_qiov_count += (s->sub_qiov_count>1) ? 1 : 0;
+	sub_qiov_count += (s->sub_qiov_count>1) ? s->sub_qiov_count : 0;
+
+	pull_send_time_sum += (s->first_sub_qiov_send_time > 0) ? s->first_sub_qiov_send_time - s->request_pull_time : 0;
+	send_callback_time_sum += (s->first_sub_qiov_send_time > 0) ? s->callback_time - s->first_sub_qiov_send_time : 0;
+	callback_push_time_sum += s->request_push_time - s->callback_time ;
+	pull_push_time_sum += s->request_push_time - s->request_pull_time;
+
+	merging_time_sum += s->qiov_division_start_time - s->qiov_merging_start_time;
+	merge_send_time += s->last_sub_qiov_send_time - s->qiov_division_start_time; 
+
+	pthread_mutex_unlock(&req->dev->lock);
+
+}
+
+static int virtio_setup_vssd(struct VirtIOVssd *vssd)
+{
+	// Registering ballong handler
+	int flag;
+	printf("VSSD: Setup resize  and manager communcation \n");
+	VirtIODevice *vdev = VIRTIO_DEVICE(vssd);
+	vssd->ctrl_vq = virtio_add_queue(vdev, 1024, virtio_vssd_handle_resize_callback);
+
+	qemu_add_ssd_balloon_handler(virtio_ssd_balloon_to_target, vssd);
+
+	uint64_t capacity_in_blocks = vssd->capacity/SSD_BLOCK_SIZE;
+	vssd->block_list = (uint64_t *)malloc(sizeof(uint64_t) * capacity_in_blocks);
+
+	for(unsigned long int i=0; i < capacity_in_blocks; i++)
+		vssd->block_list[i] = -1;
+
+	vssd->vm_id = vssd_vm_id;				//	Read from global variables
+	strcpy(vssd->vm_name, vssd_vm_name);		//	Read from global variables
+
+//	setup_log_file(vssd);
+
+	// Communicate with manager.
+	struct SharedMemory * message = get_shared_memory();
+	if(!message)
+		return -1;
+	printf("VSSD: \tShared memory ready !!!\n");
+	vssd->message = (void *)message;
+
+	printf("VSSD: \tRegistration \n");
+	get_access_to_shared_memory(message);
+	printf("VSSD:\t\tGot the access to SharedMemory \n");
+
+	printf("VSSD: \t\tSize: %d GB. Allocate: %d GB. Persist: %d GB \n", vssd_size, vssd_current_allocate, vssd_current_persist);
+	printf("VSSD: \t\tSize: %llu sectors. Allocate: %llu sectors. Persist: %llu sectors \n", 
+			gb_to_sectors(vssd_size), gb_to_sectors(vssd_current_allocate), gb_to_sectors(vssd_current_persist));
+
+
+	struct vm_registration_req *req = (struct vm_registration_req *)message->msg_content;
+	req->vm_id = vssd->vm_id;
+	req->capacity = vssd->capacity / SSD_BLOCK_SIZE;		
+	req->current_allocate = gb_to_vssd_blocks(vssd_current_allocate);		
+	req->current_persist =  gb_to_vssd_blocks(vssd_current_persist);
+	req->persist_full = vssd_persist_full;
+	req->pid = getpid();
+	printf("VSSD: \t\tSize: %lu blocks. Allocate: %lu blocks. Persist: %lu blocks \n", 
+			req->capacity, req->current_allocate, req->current_persist);
+
+	strcpy(req->vm_name, vssd->vm_name);
+	printf("VSSD: \t\tRegistration request ready \n");
+
+	send_message(message, vssd, MSG_TYPE_VM_REG);
+	wait_for_reply(message, vssd, MSG_TYPE_VM_REG_REPLY);
+	printf("VSSD: \tRegistration reply received \n");
+
+	struct vm_registration_reply *reply = (struct vm_registration_reply *)message->msg_content;
+	if(reply->error)
+	{
+		printf("VSSD-ERROR: Registration failed \n");
+		printf("VSSD-ERROR: Exiting the virtual machine \n");
+
+		release_shared_memory(message, __LINE__);
+		exit(1);
+	}
+
+	printf("VSSD: \t\tOld-blocks: %lu. Old-persistent-blocks: %lu. New-reserved-blocks: %ld. New-reserved-persistent-blocks: %ld. \n",
+			reply->old_blocks, reply->old_persistent_blocks, reply->reserved_blocks, reply->reserved_blocks_persistent);
+	bool first_time_registration = reply->first_time_registration;
+
+
+	release_shared_memory(message, __LINE__);
+
+	if(first_time_registration)
+	{
+		int round = 0;
+		unsigned long long lbn=0;
+		do 
+		{
+			lock_and_wait_for_reply(message, vssd, MSG_TYPE_PHYSICAL_BLOCK_ALCN);
+			struct phsical_block_allocation *alcn = (struct phsical_block_allocation *)&message->msg_content;
+
+			for(unsigned long int j=0; j<alcn->count; j++, lbn++)		//	lbn < vssd capacity todo
+			{
+				vssd->block_list[lbn] = alcn->block_list[j].physical_block * SSD_BLOCK_SIZE;
+				alcn->block_list[j].logical_block = lbn;
+
+					// printf("lbn: %llu. pbn: %lu \n", lbn, vssd->block_list[lbn] / SSD_BLOCK_SIZE);
+			}
+			vssd->current_capacity += alcn->count * SSD_BLOCK_SIZE;
+			flag = alcn->flag;
+
+			printf("VSSD: \t\tRound: %d. Type: %s. Received: %d. Current size: %lu \n", 
+					++round, 
+					(alcn->allocation_type == 2) ? "NEW-PERSISTENT": "NEW-NON-PERSISTENT",
+					alcn->count, 
+					vssd->current_capacity/SSD_BLOCK_SIZE);
+
+			//	Sending logical block numbers of given physical blocks
+			// phsical_block_allocation_reply *alcn_reply = (phsical_block_allocation_reply *)alloc;
+
+			send_message(message, vssd, MSG_TYPE_PHYSICAL_BLOCK_ALCN_REPLY);
+			pthread_mutex_unlock(&message->lock);
+		}
+		while(flag == FLAG_ASK_ME_AGAIN);
+	}
+	else
+	{
+		unsigned long long lbn=0;
+
+		// Receiving old persistent blocks
+		int round = 0;
+
+		do
+		{
+			lock_and_wait_for_reply(message, vssd, MSG_TYPE_PHYSICAL_BLOCK_ALCN);
+			struct phsical_block_allocation *alcn = (struct phsical_block_allocation *)&message->msg_content;
+
+			if(alcn->allocation_type == 1)
+			{
+				// Old blocks
+				for(unsigned long int j=0; j<alcn->count; j++)		
+				{
+					vssd->block_list[alcn->block_list[j].logical_block] = alcn->block_list[j].physical_block * SSD_BLOCK_SIZE;
+				}
+
+				vssd->current_capacity += alcn->count * SSD_BLOCK_SIZE;
+				flag = alcn->flag;
+
+				printf("VSSD: \t\tRound: %d. Type: OLD. Received: %d. Current size: %lu \n", 
+						++round, alcn->count, vssd->current_capacity/SSD_BLOCK_SIZE);
+
+				release_shared_memory(message, __LINE__);
+			}
+			else if(alcn->allocation_type == 2 || alcn->allocation_type == 3)
+			{
+				//	New Persistent block or New non-persistent block
+
+				lock_and_wait_for_reply(message, vssd, MSG_TYPE_PHYSICAL_BLOCK_ALCN);
+				struct phsical_block_allocation *alcn = (struct phsical_block_allocation *)&message->msg_content;
+
+				for(unsigned long int j=0; j<alcn->count && lbn<vssd->capacity/SSD_BLOCK_SIZE; lbn++)		//	lbn < vssd capacity todo
+				{
+					if(vssd->block_list[lbn] == -1)
+					{
+						vssd->block_list[lbn] = alcn->block_list[j].physical_block * SSD_BLOCK_SIZE;
+						alcn->block_list[j].logical_block = lbn;
+
+						j++;
+					}
+				}
+				vssd->current_capacity += alcn->count * SSD_BLOCK_SIZE;
+				flag = alcn->flag;
+
+				printf("VSSD: \t\tRound: %d. Type: %s. Received: %d. Current size: %lu \n", 
+						++round, 
+						(alcn->allocation_type == 2) ? "NEW-PERSISTENT": "NEW-NON-PERSISTENT",
+						alcn->count, 
+						vssd->current_capacity/SSD_BLOCK_SIZE);
+
+				//	Sending logical block numbers of given physical blocks
+				// phsical_block_allocation_reply *alcn_reply = (phsical_block_allocation_reply *)alloc;
+
+				send_message(message, vssd, MSG_TYPE_PHYSICAL_BLOCK_ALCN_REPLY);
+				pthread_mutex_unlock(&message->lock);
+			}
+		}
+		while(flag == FLAG_ASK_ME_AGAIN);
+
+		//	Receiving new persistent/non-persistent blocks
+
+	}
+
+
+
+
+
+
+	/*for(unsigned long int i=0;  i < reply->given;  i++)
+	  {
+	// if(i<20)
+	// 	printf("%ld[%ld]  ",i, reply->block_list[i]);
+	vssd->block_list[i] = reply->block_list[i] * SSD_BLOCK_SIZE;
+	}
+	vssd->current_capacity = reply->given * SSD_BLOCK_SIZE;
+
+	int flag = reply->flag;
+	int round = 1;
+	while(flag == FLAG_ASK_ME_AGAIN && vssd->current_capacity < vssd->capacity)
+	{
+	printf("VSSD: \tNeed more blocks. round:%d \n", ++round);
+	struct allocation_req *alloc_req = (struct allocation_req *)message->msg_content;
+	alloc_req->vm_id = vssd->vm_id;
+	alloc_req->requested_size = (vssd->capacity-vssd->current_capacity) / SSD_BLOCK_SIZE;
+	printf("VSSD: \t\tRequest size = %ld blocks \n", alloc_req->requested_size);
+
+	send_message(message, vssd, MSG_TYPE_ALLOCATION_REQ);
+	wait_for_reply(message, vssd, MSG_TYPE_ALLOCATION_REPLY);
+
+	allocation_reply *alloc_reply = (allocation_reply *)message->msg_content;
+	printf("VSSD: \tAllocation reply received \n");
+	printf("VSSD: \t\tRequest:%ld given:%ld \n", alloc_reply->requested_size, alloc_reply->given);
+
+	unsigned last_filled = vssd->current_capacity/SSD_BLOCK_SIZE;
+	for(unsigned long int i=0;  i < alloc_reply->given;  i++)
+	{
+	// if(i<20)
+	// 	printf("%ld[%ld] ", i+last_filled, alloc_reply->block_list[i]);
+	vssd->block_list[i+last_filled] = alloc_reply->block_list[i] * SSD_BLOCK_SIZE;
+	}
+	vssd->current_capacity += alloc_reply->given * SSD_BLOCK_SIZE;
+
+	flag = alloc_reply->flag;		
+	}
+
+	release_shared_memory(message);*/
+
+	if(pthread_create(&vssd->listener_thread_ID, NULL, backend_listner_thread, vssd))
+	{
+		fprintf(stderr, "Error creating thread\n");
+		exit(1);
+	}
+
+	printf("VSSD: \tRegistration completed \n");
+
+	/*DEBUG START*/
+	pthread_mutex_init(&vssd->lock, NULL);
+	printf("VSSD: \tLock initialized \n");
+	/*DEBUG END*/
+	return 1;
+}
+
+/*NEW FUNCTIONS END*/
+
+
+
+static void virtio_vssd_init_request(VirtIOVssd *s, VirtQueue *vq,
+		VirtIOVssdReq *req)
+{
+	req->dev = s;
+	req->vq = vq;
+	req->qiov.size = 0;
+	req->in_len = 0;
+	req->next = NULL;
+	req->mr_next = NULL;
+}
+
+static void virtio_vssd_free_request(VirtIOVssdReq *req)
+{
+	if (req) {
+		g_free(req);
+	}
+}
+
+static void virtio_vssd_req_complete(VirtIOVssdReq *req, unsigned char status)
+{
+	VirtIOVssd *s = req->dev;
+	VirtIODevice *vdev = VIRTIO_DEVICE(s);
+
+	trace_virtio_blk_req_complete(req, status);
+
+	stb_p(&req->in->status, status);
+	virtqueue_push(req->vq, &req->elem, req->in_len);
+/*
+	if(VSSD_DEBUG_MODE)
+	{
+		req->stats.request_push_time = current_time();
+		write_to_log(req);
+
+	}
+*/
+	if (s->dataplane_started && !s->dataplane_disabled) {
+		virtio_vssd_data_plane_notify(s->dataplane, req->vq);
+	} else {
+		virtio_notify(vdev, req->vq);
+	}
+}
+
+static int virtio_vssd_handle_rw_error(VirtIOVssdReq *req, int error,
+		bool is_read)
+{
+	BlockErrorAction action = blk_get_error_action(req->dev->blk,
+			is_read, error);
+	VirtIOVssd *s = req->dev;
+
+	if (action == BLOCK_ERROR_ACTION_STOP) {
+		/* Break the link as the next request is going to be parsed from the
+		 * ring again. Otherwise we may end up doing a double completion! */
+		req->mr_next = NULL;
+		req->next = s->rq;
+		s->rq = req;
+	} else if (action == BLOCK_ERROR_ACTION_REPORT) {
+		virtio_vssd_req_complete(req, VIRTIO_VSSD_S_IOERR);
+		block_acct_failed(blk_get_stats(s->blk), &req->acct);
+		virtio_vssd_free_request(req);
+	}
+
+	blk_error_action(s->blk, action, is_read, error);
+	return action != BLOCK_ERROR_ACTION_IGNORE;
+}
+
+struct new_struct
+{
+	VirtIOVssdReq *req;
+	QEMUIOVector *qiov;		//delete it man
+};
+
+static void virtio_vssd_rw_complete(void *opaque, int ret)
+{
+	struct new_struct *temp = opaque;
+	VirtIOVssdReq *next = temp->req;
+	VirtIOVssd *s = next->dev;
+
+	/*Unais*/
+
+	int completed = 0;
+	int sub_qiov_count = 0;
+	pthread_spin_lock(&next->lock);
+
+	/*Unais DEBUG start*/
+	if(VSSD_DEBUG_MODE)
+	{
+		next->stats.last_sub_qiov_callback_time = current_time();
+		if(next->stats.first_sub_qiov_callback_time == 0)
+			next->stats.first_sub_qiov_callback_time = next->stats.last_sub_qiov_callback_time;
+
+		next->stats.sub_qiov_count = next->sub_qiov_count;
+	}
+	/*Unais DEBUG end*/
+
+
+	next->sub_qiov_finished++;
+	if(next->sub_qiov_finished == next->sub_qiov_count)
+		completed = 1;
+	sub_qiov_count = next->sub_qiov_count;
+
+	// printf("VSSD: virtio_vssd_rw_complete \n");
+	// printf("VSSD: \tqiov_count=%d finished=%d. \n", next->sub_qiov_count, next->sub_qiov_finished);
+
+	pthread_spin_unlock(&next->lock);
+
+	if(sub_qiov_count > 1)
+	{
+		qemu_iovec_destroy(temp->qiov);
+		g_free(temp->qiov);
+	}
+	g_free(temp);
+
+	if(completed == 0)
+		return;
+	/*End*/
+
+	aio_context_acquire(blk_get_aio_context(s->conf.conf.blk));
+	while (next) 
+	{
+
+		/*Unais DEBUG start*/
+		if(VSSD_DEBUG_MODE)
+		{
+			next->stats.callback_time = current_time();
+		}
+		/*Unais DEBUG end*/
+
+		VirtIOVssdReq *req = next;
+		next = req->mr_next;
+		trace_virtio_blk_rw_complete(req, ret);
+
+		// printf("VSSD: \treq->qiov.nalloc = %d \n", req->qiov.nalloc);
+
+		if (req->qiov.nalloc != -1) {
+
+			/* If nalloc is != 1 req->qiov is a local copy of the original
+			 * external iovec. It was allocated in submit_merged_requests
+			 * to be able to merge requests. */
+			// printf("VSSD: \tDestrying requests \n");
+			qemu_iovec_destroy(&req->qiov);
+		}
+
+		// printf("VSSD: \tret = %d \n", ret);
+		if (ret) {
+			int p = virtio_ldl_p(VIRTIO_DEVICE(req->dev), &req->out.type);
+			bool is_read = !(p & VIRTIO_VSSD_T_OUT);
+			/* Note that memory may be dirtied on read failure.  If the
+			 * virtio request is not completed here, as is the case for
+			 * BLOCK_ERROR_ACTION_STOP, the memory may not be copied
+			 * correctly during live migration.  While this is ugly,
+			 * it is acceptable because the device is free to write to
+			 * the memory until the request is completed (which will
+			 * happen on the other side of the migration).
+			 */
+			if (virtio_vssd_handle_rw_error(req, -ret, is_read)) {
+				continue;
+			}
+		}
+
+		virtio_vssd_req_complete(req, VIRTIO_VSSD_S_OK);
+		block_acct_done(blk_get_stats(req->dev->blk), &req->acct);
+		virtio_vssd_free_request(req);
+	}
+	aio_context_release(blk_get_aio_context(s->conf.conf.blk));
+}
+
+static void virtio_vssd_flush_complete(void *opaque, int ret)
+{
+	VirtIOVssdReq *req = opaque;
+	VirtIOVssd *s = req->dev;
+
+	aio_context_acquire(blk_get_aio_context(s->conf.conf.blk));
+	if (ret) {
+		if (virtio_vssd_handle_rw_error(req, -ret, 0)) {
+			goto out;
+		}
+	}
+
+	virtio_vssd_req_complete(req, VIRTIO_VSSD_S_OK);
+	block_acct_done(blk_get_stats(req->dev->blk), &req->acct);
+	virtio_vssd_free_request(req);
+
+out:
+	aio_context_release(blk_get_aio_context(s->conf.conf.blk));
+}
+
+#ifdef __linux__
+
+typedef struct {
+	VirtIOVssdReq *req;
+	struct sg_io_hdr hdr;
+} VirtIOBlockIoctlReq;
+
+static void virtio_vssd_ioctl_complete(void *opaque, int status)
+{
+	VirtIOBlockIoctlReq *ioctl_req = opaque;
+	VirtIOVssdReq *req = ioctl_req->req;
+	VirtIOVssd *s = req->dev;
+	VirtIODevice *vdev = VIRTIO_DEVICE(s);
+	struct virtio_scsi_inhdr *scsi;
+	struct sg_io_hdr *hdr;
+
+	scsi = (void *)req->elem.in_sg[req->elem.in_num - 2].iov_base;
+
+	if (status) {
+		status = VIRTIO_VSSD_S_UNSUPP;
+		virtio_stl_p(vdev, &scsi->errors, 255);
+		goto out;
+	}
+
+	hdr = &ioctl_req->hdr;
+	/*
+	 * From SCSI-Generic-HOWTO: "Some lower level drivers (e.g. ide-scsi)
+	 * clear the masked_status field [hence status gets cleared too, see
+	 * block/scsi_ioctl.c] even when a CHECK_CONDITION or COMMAND_TERMINATED
+	 * status has occurred.  However they do set DRIVER_SENSE in driver_status
+	 * field. Also a (sb_len_wr > 0) indicates there is a sense buffer.
+	 */
+	if (hdr->status == 0 && hdr->sb_len_wr > 0) {
+		hdr->status = CHECK_CONDITION;
+	}
+
+	virtio_stl_p(vdev, &scsi->errors,
+			hdr->status | (hdr->msg_status << 8) |
+			(hdr->host_status << 16) | (hdr->driver_status << 24));
+	virtio_stl_p(vdev, &scsi->residual, hdr->resid);
+	virtio_stl_p(vdev, &scsi->sense_len, hdr->sb_len_wr);
+	virtio_stl_p(vdev, &scsi->data_len, hdr->dxfer_len);
+
+out:
+	aio_context_acquire(blk_get_aio_context(s->conf.conf.blk));
+	virtio_vssd_req_complete(req, status);
+	virtio_vssd_free_request(req);
+	aio_context_release(blk_get_aio_context(s->conf.conf.blk));
+	g_free(ioctl_req);
+}
+
+#endif
+
+static VirtIOVssdReq *virtio_vssd_get_request(VirtIOVssd *s, VirtQueue *vq)
+{
+	VirtIOVssdReq *req = virtqueue_pop(vq, sizeof(VirtIOVssdReq));
+
+	if (req) {
+		virtio_vssd_init_request(s, vq, req);
+	}
+	return req;
+}
+
+static int virtio_vssd_handle_scsi_req(VirtIOVssdReq *req)
+{
+	int status = VIRTIO_VSSD_S_OK;
+	struct virtio_scsi_inhdr *scsi = NULL;
+	VirtIODevice *vdev = VIRTIO_DEVICE(req->dev);
+	VirtQueueElement *elem = &req->elem;
+	VirtIOVssd *blk = req->dev;
+
+#ifdef __linux__
+	int i;
+	VirtIOBlockIoctlReq *ioctl_req;
+	BlockAIOCB *acb;
+#endif
+
+	/*
+	 * We require at least one output segment each for the virtio_vssd_outhdr
+	 * and the SCSI command block.
+	 *
+	 * We also at least require the virtio_vssd_inhdr, the virtio_scsi_inhdr
+	 * and the sense buffer pointer in the input segments.
+	 */
+	if (elem->out_num < 2 || elem->in_num < 3) {
+		status = VIRTIO_VSSD_S_IOERR;
+		goto fail;
+	}
+
+	/*
+	 * The scsi inhdr is placed in the second-to-last input segment, just
+	 * before the regular inhdr.
+	 */
+	scsi = (void *)elem->in_sg[elem->in_num - 2].iov_base;
+
+	if (!blk->conf.scsi) {
+		status = VIRTIO_VSSD_S_UNSUPP;
+		goto fail;
+	}
+
+	/*
+	 * No support for bidirection commands yet.
+	 */
+	if (elem->out_num > 2 && elem->in_num > 3) {
+		status = VIRTIO_VSSD_S_UNSUPP;
+		goto fail;
+	}
+
+#ifdef __linux__
+	ioctl_req = g_new0(VirtIOBlockIoctlReq, 1);
+	ioctl_req->req = req;
+	ioctl_req->hdr.interface_id = 'S';
+	ioctl_req->hdr.cmd_len = elem->out_sg[1].iov_len;
+	ioctl_req->hdr.cmdp = elem->out_sg[1].iov_base;
+	ioctl_req->hdr.dxfer_len = 0;
+
+	if (elem->out_num > 2) {
+		/*
+		 * If there are more than the minimally required 2 output segments
+		 * there is write payload starting from the third iovec.
+		 */
+		ioctl_req->hdr.dxfer_direction = SG_DXFER_TO_DEV;
+		ioctl_req->hdr.iovec_count = elem->out_num - 2;
+
+		for (i = 0; i < ioctl_req->hdr.iovec_count; i++) {
+			ioctl_req->hdr.dxfer_len += elem->out_sg[i + 2].iov_len;
+		}
+
+		ioctl_req->hdr.dxferp = elem->out_sg + 2;
+
+	} else if (elem->in_num > 3) {
+		/*
+		 * If we have more than 3 input segments the guest wants to actually
+		 * read data.
+		 */
+		ioctl_req->hdr.dxfer_direction = SG_DXFER_FROM_DEV;
+		ioctl_req->hdr.iovec_count = elem->in_num - 3;
+		for (i = 0; i < ioctl_req->hdr.iovec_count; i++) {
+			ioctl_req->hdr.dxfer_len += elem->in_sg[i].iov_len;
+		}
+
+		ioctl_req->hdr.dxferp = elem->in_sg;
+	} else {
+		/*
+		 * Some SCSI commands don't actually transfer any data.
+		 */
+		ioctl_req->hdr.dxfer_direction = SG_DXFER_NONE;
+	}
+
+	ioctl_req->hdr.sbp = elem->in_sg[elem->in_num - 3].iov_base;
+	ioctl_req->hdr.mx_sb_len = elem->in_sg[elem->in_num - 3].iov_len;
+
+	acb = blk_aio_ioctl(blk->blk, SG_IO, &ioctl_req->hdr,
+			virtio_vssd_ioctl_complete, ioctl_req);
+	if (!acb) {
+		g_free(ioctl_req);
+		status = VIRTIO_VSSD_S_UNSUPP;
+		goto fail;
+	}
+	return -EINPROGRESS;
+#else
+	abort();
+#endif
+
+fail:
+	/* Just put anything nonzero so that the ioctl fails in the guest.  */
+	if (scsi) {
+		virtio_stl_p(vdev, &scsi->errors, 255);
+	}
+	return status;
+}
+
+static void virtio_vssd_handle_scsi(VirtIOVssdReq *req)
+{
+	int status;
+
+	status = virtio_vssd_handle_scsi_req(req);
+	if (status != -EINPROGRESS) {
+		virtio_vssd_req_complete(req, status);
+		virtio_vssd_free_request(req);
+	}
+}
+
+
+static inline void submit_requests(BlockBackend *blk, MultiVssdReqBuffer *mrb,
+		int start, int num_reqs, int niov)
+{
+	/*Unais DEBUG start*/
+	if(VSSD_DEBUG_MODE)
+	{
+		mrb->reqs[start]->stats.qiov_merging_start_time = current_time();
+	}
+	/*Unais DEBUG end*/
+
+	QEMUIOVector *qiov = &mrb->reqs[start]->qiov;
+	int64_t sector_num = mrb->reqs[start]->sector_num;
+	bool is_write = mrb->is_write;
+
+	/*Unais DEBUG start*/
+	if(VSSD_DEBUG_MODE)
+	{
+		mrb->reqs[start]->stats.qiov_size = qiov->size;
+		mrb->reqs[start]->stats.qiov_iov_count = qiov->niov;
+
+		// mrb->reqs[start]->stats.qiov_merge_count++;
+		// 		mrb->reqs[start]->stats.qiov_mereg_size = qiov->size
+		// 		mrb->reqs[start]->stats.qiov_merg_iov_count = qiov.niov;
+	}
+	/*Unais DEBUG end*/
+
+	// printf("VSSD: submit_requests \n");
+	// printf("VSSD: \tNumber of request = %d \n", num_reqs);
+	if (num_reqs > 1) {
+		int i;
+		struct iovec *tmp_iov = qiov->iov;
+		int tmp_niov = qiov->niov;
+
+		/* mrb->reqs[start]->qiov was initialized from external so we can't
+		 * modify it here. We need to initialize it locally and then add the
+		 * external iovecs. */
+		qemu_iovec_init(qiov, niov);
+
+		for (i = 0; i < tmp_niov; i++) {
+			qemu_iovec_add(qiov, tmp_iov[i].iov_base, tmp_iov[i].iov_len);
+			// printf("VSSD: \tiov_base = %p, length = %ld \n", tmp_iov[i].iov_base, tmp_iov[i].iov_len);
+		}
+
+		for (i = start + 1; i < start + num_reqs; i++) {
+			qemu_iovec_concat(qiov, &mrb->reqs[i]->qiov, 0,
+					mrb->reqs[i]->qiov.size);
+			mrb->reqs[i - 1]->mr_next = mrb->reqs[i];
+
+			/*Unais DEBUG start*/
+			if(VSSD_DEBUG_MODE)
+			{
+				mrb->reqs[i]->stats.qiov_size = mrb->reqs[i]->qiov.size;
+				mrb->reqs[i]->stats.qiov_iov_count = mrb->reqs[i]->qiov.niov;
+
+				// mrb->reqs[start]->stats.qiov_merge_count++;
+				// mrb->reqs[start]->stats.qiov_mereg_size += mrb->reqs[i]->qiov.size;
+				// mrb->reqs[start]->stats.qiov_merg_iov_count += mrb->reqs[i]->qiov.niov;
+			}
+			/*Unais DEBUG end*/
+		}
+
+		trace_virtio_blk_submit_multireq(mrb, start, num_reqs,
+				sector_num << BDRV_SECTOR_BITS,
+				qiov->size, is_write);
+		block_acct_merge_done(blk_get_stats(blk),
+				is_write ? BLOCK_ACCT_WRITE : BLOCK_ACCT_READ,
+				num_reqs - 1);
+	}
+
+
+	/*Unais DEBUG start*/
+	if(VSSD_DEBUG_MODE)
+	{
+		mrb->reqs[start]->stats.qiov_merge_count = num_reqs;
+		mrb->reqs[start]->stats.qiov_mereg_size = qiov->size;
+		mrb->reqs[start]->stats.qiov_merg_iov_count = qiov->niov;
+	}
+	/*Unais DEBUG end*/
+
+
+
+	/*Unais Start */
+
+	/*blk_aio_pwritev(blk, (sector_num) << BDRV_SECTOR_BITS, qiov, 0,
+	  virtio_vssd_rw_complete, mrb->reqs[start]);*/
+
+	/*printf("\nVSSD: \t%s: sector=%ld [%ld]. req = %p \n", 
+	  is_write?"Write. ":"Read. ",
+	  sector_num,
+	  sector_num << BDRV_SECTOR_BITS,
+	  mrb->reqs[start]);
+
+	  printf("VSSD: \tQEMUIOVector. iov=%p, niov=%d, nalloc=%d, size=%ldB[%lds]  \n",
+	  qiov->iov,
+	  qiov->niov,
+	  qiov->nalloc,
+	  qiov->size,
+	  qiov->size/512);
+
+	  printf("VSSD: \tList of iovs \n");
+	  for(int i=0; i<qiov->niov; i++)
+	  printf("VSSD: \t\tiov[%d]. base=%p, len=%ldB[%lds] \n", i, qiov->iov[i].iov_base, qiov->iov[i].iov_len, qiov->iov[i].iov_len/512);
+
+	  printf("VSSD: \tParition the qiov \n");*/
+
+
+	/*Unais DEBUG start*/
+	if(VSSD_DEBUG_MODE)
+	{
+		mrb->reqs[start]->stats.qiov_division_start_time = current_time();
+	}
+	/*Unais DEBUG end*/
+
+	mrb->reqs[start]->sub_qiov_count = 1000;
+	mrb->reqs[start]->sub_qiov_finished = 0;
+	int rqst_count=0;
+	int64_t total_processed = 0;
+	int64_t total_send_to_device = 0;
+	int64_t start_sector, end;
+	QEMUIOVector *qiov_temp = NULL;
+
+	//Initialize the lock
+	pthread_spin_init(&mrb->reqs[start]->lock, 0);
+
+	int multiple_qiov_required = 0;
+	if((sector_num%SSD_BLOCK_SIZE) + qiov->size/512 > SSD_BLOCK_SIZE )
+		multiple_qiov_required = 1;
+
+	if(!multiple_qiov_required)
+	{
+		struct new_struct *tmp_parameter = (struct new_struct *)malloc(sizeof(struct new_struct));
+		tmp_parameter->req = mrb->reqs[start];
+		tmp_parameter->qiov = qiov;
+
+		mrb->reqs[start]->sub_qiov_count = 1;		// Do not delete this qiov.
+		mrb->reqs[start]->sub_qiov_finished = 0;
+
+		qiov_temp = qiov;
+
+		/*Unais DEBUG start*/
+		if(VSSD_DEBUG_MODE)
+		{
+			mrb->reqs[start]->stats.last_sub_qiov_send_time = current_time();
+			if(mrb->reqs[start]->stats.first_sub_qiov_send_time == 0)
+				mrb->reqs[start]->stats.first_sub_qiov_send_time = mrb->reqs[start]->stats.last_sub_qiov_send_time;
+		}
+		/*Unais DEBUG end*/
+
+
+		if (is_write) 
+			blk_aio_pwritev(blk, l2p(mrb->reqs[start]->dev, sector_num) << BDRV_SECTOR_BITS, qiov_temp, 0,
+					virtio_vssd_rw_complete, tmp_parameter);
+		else
+			blk_aio_preadv(blk, l2p(mrb->reqs[start]->dev, sector_num) << BDRV_SECTOR_BITS, qiov_temp, 0,
+					virtio_vssd_rw_complete, tmp_parameter);
+
+		return;
+	}
+
+	for(int i=0; i<qiov->niov; i++)
+	{
+		//Partitioning by blocks of 8 sectors
+		int64_t current_iov_sectors = qiov->iov[i].iov_len/512;
+		int64_t current_iov_processed = 0;
+		int64_t current_iov_remaining = current_iov_sectors;
+
+
+		while(current_iov_remaining > 0)
+		{
+			start_sector = sector_num + total_processed;
+
+			if(start_sector % SSD_BLOCK_SIZE == 0)
+			{
+				// Starting is block aligned
+
+				if(current_iov_remaining >= SSD_BLOCK_SIZE)
+					end = start_sector + SSD_BLOCK_SIZE;
+				else
+					end = start_sector + current_iov_remaining;
+			}
+			else
+			{
+				//	Starting is not block aligned.
+				int offset_in_block = start_sector % SSD_BLOCK_SIZE;
+				if(offset_in_block + current_iov_remaining >= SSD_BLOCK_SIZE)
+					end = start_sector + SSD_BLOCK_SIZE - offset_in_block;
+				else
+					end = start_sector + current_iov_remaining;
+			}
+
+			if(!qiov_temp)
+			{
+				// printf("VSSD: \t\tCreating new iovector = %d.  set nalloc=%d \n", rqst_count, qiov->niov);
+
+				qiov_temp = (QEMUIOVector *)malloc(sizeof(QEMUIOVector));
+				qemu_iovec_init(qiov_temp, qiov->niov);
+
+				rqst_count++;
+			}
+
+			// printf("VSSD: \t\t\tiov = %d. start = %ld, end = %ld. \n",
+			// 		i, start_sector, end);
+
+			qemu_iovec_add(qiov_temp, qiov->iov[i].iov_base + current_iov_processed*512, (end-start_sector)*512);
+
+			if(i == qiov->niov-1 && current_iov_remaining - (end-start_sector) == 0)
+			{
+				//printf("VSSD: \t\t   Last sub parition.  \n");
+				pthread_spin_lock(&mrb->reqs[start]->lock);
+				mrb->reqs[start]->sub_qiov_count = rqst_count;
+				pthread_spin_unlock(&mrb->reqs[start]->lock);
+			}
+
+			if(end % SSD_BLOCK_SIZE  == 0)
+			{
+
+				// printf("VSSD: \t\t\tReached block boundary. last iov start=%ld. end=%ld.  \n ",
+				// 		start_sector, end);
+				int64_t temp_qiov_size = qiov_temp->size;
+
+				struct new_struct *tmp_parameter=(struct new_struct *)malloc(sizeof(struct new_struct));
+				tmp_parameter->req = mrb->reqs[start];
+				tmp_parameter->qiov = qiov_temp;
+
+				/*printf("VSSD: \t\t\tiovs in this QIOV. This QIOV size=%ldB[%lds] \n", qiov_temp->size,  qiov_temp->size/512);
+				  for(int t=0; t<qiov_temp->niov; t++)
+				  {
+				  printf("VSSD: \t\t\t    iov[%d]. base=%p, len=%ldB[%lds] \n", 
+				  t, qiov_temp->iov[t].iov_base, qiov_temp->iov[t].iov_len, qiov_temp->iov[t].iov_len/512);
+				  }*/
+
+				/*Unais DEBUG start*/
+				if(VSSD_DEBUG_MODE)
+				{
+					mrb->reqs[start]->stats.last_sub_qiov_send_time = current_time();
+					if(mrb->reqs[start]->stats.first_sub_qiov_send_time == 0)
+						mrb->reqs[start]->stats.first_sub_qiov_send_time = mrb->reqs[start]->stats.last_sub_qiov_send_time;
+				}
+				/*Unais DEBUG end*/
+
+
+				if (is_write) 
+					blk_aio_pwritev(blk, l2p(mrb->reqs[start]->dev, sector_num + total_send_to_device) << BDRV_SECTOR_BITS, qiov_temp, 0,
+							virtio_vssd_rw_complete, tmp_parameter);
+				else
+					blk_aio_preadv(blk, l2p(mrb->reqs[start]->dev, sector_num + total_send_to_device) << BDRV_SECTOR_BITS, qiov_temp, 0,
+							virtio_vssd_rw_complete, tmp_parameter);
+
+				total_send_to_device += temp_qiov_size;
+				qiov_temp = NULL;
+
+			}
+
+			total_processed += end-start_sector;
+			current_iov_processed += end-start_sector;
+			current_iov_remaining -= end-start_sector;			
+		}	 		
+
+	}
+
+	if(qiov_temp)
+	{
+		int64_t temp_qiov_size = qiov_temp->size;
+
+		/*printf("VSSD: \t\t\tReached the end last iov start=%ld. end=%ld. \n ",
+		  start_sector, end);
+
+		  printf("VSSD: \t\t\tiovs in this QIOV. This QIOV size=%ldB[%lds] \n", qiov_temp->size,  qiov_temp->size/512);
+		  for(int t=0; t<qiov_temp->niov; t++)
+		  {
+		  printf("VSSD: \t\t\t    iov[%d]. base=%p, len=%ldB[%lds] \n", 
+		  t, qiov_temp->iov[t].iov_base, qiov_temp->iov[t].iov_len, qiov_temp->iov[t].iov_len/512);
+		  }*/
+
+
+		struct new_struct *tmp_parameter=(struct new_struct *)malloc(sizeof(struct new_struct));
+		tmp_parameter->req = mrb->reqs[start];
+		tmp_parameter->qiov = qiov_temp;
+
+		/*printf("VSSD : \t\t\tSending reqeust. logical=%ld. physical=%ld. \n", 
+		  sector_num + total_send_to_device, 
+		  l2p(mrb->reqs[start]->dev, sector_num + total_send_to_device));*/
+
+		/*Unais DEBUG start*/
+		if(VSSD_DEBUG_MODE)
+		{
+			mrb->reqs[start]->stats.last_sub_qiov_send_time = current_time();
+			if(mrb->reqs[start]->stats.first_sub_qiov_send_time == 0)
+				mrb->reqs[start]->stats.first_sub_qiov_send_time = mrb->reqs[start]->stats.last_sub_qiov_send_time;
+		}
+		/*Unais DEBUG end*/
+
+		if (is_write) 
+			blk_aio_pwritev(blk, l2p(mrb->reqs[start]->dev, sector_num + total_send_to_device) << BDRV_SECTOR_BITS, 
+					qiov_temp, 0, virtio_vssd_rw_complete, tmp_parameter);
+		else
+			blk_aio_preadv(blk, l2p(mrb->reqs[start]->dev, sector_num + total_send_to_device) << BDRV_SECTOR_BITS, qiov_temp, 0,
+					virtio_vssd_rw_complete, tmp_parameter);
+
+		total_send_to_device += temp_qiov_size;
+		qiov_temp = NULL;
+	}
+
+	/*Unais End*/
+}
+
+
+
+static int multireq_compare(const void *a, const void *b)
+{
+	const VirtIOVssdReq *req1 = *(VirtIOVssdReq **)a,
+	      *req2 = *(VirtIOVssdReq **)b;
+
+	/*
+	 * Note that we can't simply subtract sector_num1 from sector_num2
+	 * here as that could overflow the return value.
+	 */
+	if (req1->sector_num > req2->sector_num) {
+		return 1;
+	} else if (req1->sector_num < req2->sector_num) {
+		return -1;
+	} else {
+		return 0;
+	}
+}
+
+static void virtio_vssd_submit_multireq(BlockBackend *blk, MultiVssdReqBuffer *mrb)
+{
+
+	// printf("virtio: virtio_vssd_submit_multireq called \n");
+
+	int i = 0, start = 0, num_reqs = 0, niov = 0, nb_sectors = 0;
+	uint32_t max_transfer;
+	int64_t sector_num = 0;
+
+	//("VSSD: virtio_vssd_submit_multireq \n");
+
+	if (mrb->num_reqs == 1) {
+		//("VSSD: \tOnly  1 request. Directly calls submit_requests \n");
+		submit_requests(blk, mrb, 0, 1, -1);
+		mrb->num_reqs = 0;
+		return;
+	}
+
+	max_transfer = blk_get_max_transfer(mrb->reqs[0]->dev->blk);
+	//printf("VSSD: \tmax_transfer = %d \n", max_transfer);
+
+	qsort(mrb->reqs, mrb->num_reqs, sizeof(*mrb->reqs),
+			&multireq_compare);
+
+	//printf("VSSD: \tmrb->num_reqs = %d \n", mrb->num_reqs);
+	for (i = 0; i < mrb->num_reqs; i++) {
+		VirtIOVssdReq *req = mrb->reqs[i];
+		if (num_reqs > 0) {
+			/*
+			 * NOTE: We cannot merge the requests in below situations:
+			 * 1. requests are not sequential
+			 * 2. merge would exceed maximum number of IOVs
+			 * 3. merge would exceed maximum transfer length of backend device
+			 */
+			if (sector_num + nb_sectors != req->sector_num ||
+					niov > blk_get_max_iov(blk) - req->qiov.niov ||
+					req->qiov.size > max_transfer ||
+					nb_sectors > (max_transfer - req->qiov.size) / BDRV_SECTOR_SIZE) 
+			{
+				submit_requests(blk, mrb, start, num_reqs, niov);
+				num_reqs = 0;
+			}
+		}
+
+		if (num_reqs == 0) 
+		{
+			sector_num = req->sector_num;
+			nb_sectors = niov = 0;
+			start = i;
+		}
+
+		nb_sectors += req->qiov.size / BDRV_SECTOR_SIZE;
+		niov += req->qiov.niov;
+		num_reqs++;
+	}
+
+	submit_requests(blk, mrb, start, num_reqs, niov);
+	mrb->num_reqs = 0;
+}
+
+static void virtio_vssd_handle_flush(VirtIOVssdReq *req, MultiVssdReqBuffer *mrb)
+{
+	block_acct_start(blk_get_stats(req->dev->blk), &req->acct, 0,
+			BLOCK_ACCT_FLUSH);
+
+	/*
+	 * Make sure all outstanding writes are posted to the backing device.
+	 */
+	if (mrb->is_write && mrb->num_reqs > 0) {
+		virtio_vssd_submit_multireq(req->dev->blk, mrb);
+	}
+	blk_aio_flush(req->dev->blk, virtio_vssd_flush_complete, req);
+}
+
+/*static bool virtio_vssd_sect_range_ok(VirtIOVssd *dev,
+  uint64_t sector, size_t size)
+  {
+  uint64_t nb_sectors = size >> BDRV_SECTOR_BITS;
+  uint64_t total_sectors;
+
+  if (nb_sectors > BDRV_REQUEST_MAX_SECTORS) {
+  return false;
+  }
+  if (sector & dev->sector_mask) {
+  return false;
+  }
+  if (size % dev->conf.conf.logical_block_size) {
+  return false;
+  }
+  blk_get_geometry(dev->blk, &total_sectors);
+  if (sector > total_sectors || nb_sectors > total_sectors - sector) {
+  return false;
+  }
+  return true;
+  }*/
+
+static int virtio_vssd_handle_request(VirtIOVssdReq *req, MultiVssdReqBuffer *mrb)
+{
+	uint32_t type;
+	struct iovec *in_iov = req->elem.in_sg;
+	struct iovec *iov = req->elem.out_sg;
+	unsigned in_num = req->elem.in_num;
+	unsigned out_num = req->elem.out_num;
+	VirtIOVssd *s = req->dev;
+	VirtIODevice *vdev = VIRTIO_DEVICE(s);
+
+	//clock_t start, end;
+	//double cpu_time_used; 
+
+	// printf("\nVSSD: Handle request \n");
+
+	if (req->elem.out_num < 1 || req->elem.in_num < 1) {
+		virtio_error(vdev, "virtio-blk missing headers");
+		return -1;
+	}
+
+	if (unlikely(iov_to_buf(iov, out_num, 0, &req->out,
+					sizeof(req->out)) != sizeof(req->out))) {
+		virtio_error(vdev, "virtio-blk request outhdr too short");
+		return -1;
+	}
+
+	iov_discard_front(&iov, &out_num, sizeof(req->out));
+
+	if (in_iov[in_num - 1].iov_len < sizeof(struct virtio_vssd_inhdr)) {
+		virtio_error(vdev, "virtio-blk request inhdr too short");
+		return -1;
+	}
+
+	/* We always touch the last byte, so just see how big in_iov is.  */
+	req->in_len = iov_size(in_iov, in_num);
+	req->in = (void *)in_iov[in_num - 1].iov_base
+		+ in_iov[in_num - 1].iov_len
+		- sizeof(struct virtio_vssd_inhdr);
+	iov_discard_back(in_iov, &in_num, sizeof(struct virtio_vssd_inhdr));
+
+	type = virtio_ldl_p(VIRTIO_DEVICE(req->dev), &req->out.type);
+
+	/* VIRTIO_VSSd_T_OUT defines the command direction. VIRTIO_VSSd_T_BARRIER
+	 * is an optional flag. Although a guest should not send this flag if
+	 * not negotiated we ignored it in the past. So keep ignoring it. */
+	switch (type & ~(VIRTIO_VSSD_T_OUT | VIRTIO_VSSD_T_BARRIER)) {
+		case VIRTIO_VSSD_T_IN:
+			{
+				bool is_write = type & VIRTIO_VSSD_T_OUT;
+				req->sector_num = virtio_ldq_p(VIRTIO_DEVICE(req->dev), &req->out.sector);
+
+				/*printf("VSSD: \tout.sector:%ld, logical-sector:%ld, physical-sector:%ld \n",
+				  req->out.sector,
+				  req->sector_num,
+				  l2p(s, req->sector_num));*/
+
+				if (is_write) 
+				{
+					//printf("VSSD: \tWrite request \n");
+					//start = clock(); 
+
+					qemu_iovec_init_external(&req->qiov, iov, out_num);
+					trace_virtio_blk_handle_write(req, req->sector_num,
+							req->qiov.size / BDRV_SECTOR_SIZE);
+
+				} 
+				else 
+				{   
+					//printf("VSSD: \tRead request \n");
+					//start = clock();
+
+					qemu_iovec_init_external(&req->qiov, in_iov, in_num);
+					trace_virtio_blk_handle_read(req, req->sector_num,
+							req->qiov.size / BDRV_SECTOR_SIZE);
+				}
+
+				//	ToDoUnais  Verify range of physical......
+				/*if (!virtio_vssd_sect_range_ok(req->dev, req->sector_num, req->qiov.size)) 
+				  {
+				  virtio_vssd_req_complete(req, VIRTIO_VSSD_S_IOERR);
+				  block_acct_invalid(blk_get_stats(req->dev->blk), is_write ? BLOCK_ACCT_WRITE : BLOCK_ACCT_READ);
+				  virtio_vssd_free_request(req);
+				  return 0;
+				  }
+				 */
+				block_acct_start(blk_get_stats(req->dev->blk),
+						&req->acct, req->qiov.size,
+						is_write ? BLOCK_ACCT_WRITE : BLOCK_ACCT_READ);
+
+				/* merge would exceed maximum number of requests or IO direction
+				 * changes */
+				//printf("virtio: num_reqs=%d, VIRTIO_VSSd_MAX_MERGE_REQS=32, is_write=%d, mrb->is_write=%d, req->dev->conf.request_merging=%d \n",
+				//     mrb->num_reqs, is_write, mrb->is_write, req->dev->conf.request_merging);
+
+				if (mrb->num_reqs > 0 && (	mrb->num_reqs == VIRTIO_VSSD_MAX_MERGE_REQS || 
+							is_write != mrb->is_write || 
+							!req->dev->conf.request_merging)) 
+				{
+					// printf("virtio: Calling submit_multireq from handle_request \n");
+					virtio_vssd_submit_multireq(req->dev->blk, mrb);
+				}
+
+				// end = clock();
+				// cpu_time_used = ((double) (end - start)) / CLOCKS_PER_SEC;
+				// printf("virtio: %s Completed......\n", is_write ? "Write" : "Read");  
+				// printf("virtio: %s took %f seconds to execute--------------->>>> \n", is_write ? "Write" : "Read", cpu_time_used);
+
+				assert(mrb->num_reqs < VIRTIO_VSSD_MAX_MERGE_REQS);
+				mrb->reqs[mrb->num_reqs++] = req;
+				mrb->is_write = is_write;
+
+
+				/*Unais DEBUG start*/
+				if(VSSD_DEBUG_MODE)
+				{
+					req->stats.adding_to_mrb_time = current_time();
+				}
+				/*Unais DEBUG end*/
+
+				break;
+			}
+		case VIRTIO_VSSD_T_FLUSH:
+			virtio_vssd_handle_flush(req, mrb);
+			break;
+		case VIRTIO_VSSD_T_SCSI_CMD:
+			virtio_vssd_handle_scsi(req);
+			break;
+		case VIRTIO_VSSD_T_GET_ID:
+			{
+				VirtIOVssd *s = req->dev;
+
+				/*
+				 * NB: per existing s/n string convention the string is
+				 * terminated by '\0' only when shorter than buffer.
+				 */
+				const char *serial = s->conf.serial ? s->conf.serial : "";
+				size_t size = MIN(strlen(serial) + 1,
+						MIN(iov_size(in_iov, in_num),
+							VIRTIO_VSSD_ID_BYTES));
+				iov_from_buf(in_iov, in_num, 0, serial, size);
+				virtio_vssd_req_complete(req, VIRTIO_VSSD_S_OK);
+				virtio_vssd_free_request(req);
+				break;
+			}
+		default:
+			virtio_vssd_req_complete(req, VIRTIO_VSSD_S_UNSUPP);
+			virtio_vssd_free_request(req);
+	}
+	return 0;
+}
+
+bool virtio_vssd_handle_vq(VirtIOVssd *s, VirtQueue *vq)
+{
+	VirtIOVssdReq *req;
+	MultiVssdReqBuffer mrb = {};
+	bool progress = false;
+
+	/*Unais Start */
+	clock_t temp_time = current_time();
+	/*Unais End*/
+
+	aio_context_acquire(blk_get_aio_context(s->blk));
+	blk_io_plug(s->blk);
+
+
+
+	do 
+	{
+		virtio_queue_set_notification(vq, 0);
+
+		while ((req = virtio_vssd_get_request(s, vq))) 
+		{
+
+			/*Unais start*/
+			if(VSSD_DEBUG_MODE)
+			{
+				req->stats.request_pull_time = temp_time;
+				req->stats.adding_to_mrb_time = 0;
+				req->stats.callback_time = 0;                  //  Time at which call back for last sub-qiov received
+				req->stats.request_push_time = 0;              //  Final event
+
+				req->stats.qiov_merging_start_time = 0;
+				req->stats.qiov_division_start_time = 0;       //  Merging end time
+				req->stats.first_sub_qiov_send_time = 0;
+				req->stats.last_sub_qiov_send_time = 0;        //  Division end time
+				req->stats.first_sub_qiov_callback_time = 0;
+				req->stats.last_sub_qiov_callback_time = 0;
+			}
+			/*Unais end*/
+
+			progress = true;
+			if (virtio_vssd_handle_request(req, &mrb)) 
+			{
+				virtqueue_detach_element(req->vq, &req->elem, 0);
+				virtio_vssd_free_request(req);
+				break;
+			}
+		}
+
+		virtio_queue_set_notification(vq, 1);
+	} while (!virtio_queue_empty(vq));
+
+
+	if (mrb.num_reqs) 
+	{
+		// printf("virtio: Calling multi req from handle_vq \n");
+		virtio_vssd_submit_multireq(s->blk, &mrb);
+	}
+
+	blk_io_unplug(s->blk);
+	aio_context_release(blk_get_aio_context(s->blk));
+	return progress;
+}
+
+static void virtio_vssd_handle_output_do(VirtIOVssd *s, VirtQueue *vq)
+{
+	virtio_vssd_handle_vq(s, vq);
+}
+
+static void virtio_vssd_handle_output(VirtIODevice *vdev, VirtQueue *vq)
+{
+	VirtIOVssd *s = (VirtIOVssd *)vdev;
+
+	if (s->dataplane) {
+		/* Some guests kick before setting VIRTIO_CONFIG_S_DRIVER_OK so start
+		 * dataplane here instead of waiting for .set_status().
+		 */
+		virtio_device_start_ioeventfd(vdev);
+		if (!s->dataplane_disabled) {
+			return;
+		}
+	}
+	virtio_vssd_handle_output_do(s, vq);
+}
+
+static void virtio_vssd_dma_restart_bh(void *opaque)
+{
+	VirtIOVssd *s = opaque;
+	VirtIOVssdReq *req = s->rq;
+	MultiVssdReqBuffer mrb = {};
+
+	qemu_bh_delete(s->bh);
+	s->bh = NULL;
+
+	s->rq = NULL;
+
+	aio_context_acquire(blk_get_aio_context(s->conf.conf.blk));
+	while (req) {
+		VirtIOVssdReq *next = req->next;
+		if (virtio_vssd_handle_request(req, &mrb)) {
+			/* Device is now broken and won't do any processing until it gets
+			 * reset. Already queued requests will be lost: let's purge them.
+			 */
+			while (req) {
+				next = req->next;
+				virtqueue_detach_element(req->vq, &req->elem, 0);
+				virtio_vssd_free_request(req);
+				req = next;
+			}
+			break;
+		}
+		req = next;
+	}
+
+	if (mrb.num_reqs) {
+		virtio_vssd_submit_multireq(s->blk, &mrb);
+	}
+	aio_context_release(blk_get_aio_context(s->conf.conf.blk));
+}
+
+static void virtio_vssd_dma_restart_cb(void *opaque, int running,
+		RunState state)
+{
+	VirtIOVssd *s = opaque;
+
+	if (!running) {
+		return;
+	}
+
+	if (!s->bh) {
+		s->bh = aio_bh_new(blk_get_aio_context(s->conf.conf.blk),
+				virtio_vssd_dma_restart_bh, s);
+		qemu_bh_schedule(s->bh);
+	}
+}
+
+static void virtio_vssd_reset(VirtIODevice *vdev)
+{
+	VirtIOVssd *s = VIRTIO_VSSD(vdev);
+	AioContext *ctx;
+	VirtIOVssdReq *req;
+
+	ctx = blk_get_aio_context(s->blk);
+	aio_context_acquire(ctx);
+	blk_drain(s->blk);
+
+	/* We drop queued requests after blk_drain() because blk_drain() itself can
+	 * produce them. */
+	while (s->rq) {
+		req = s->rq;
+		s->rq = req->next;
+		virtqueue_detach_element(req->vq, &req->elem, 0);
+		virtio_vssd_free_request(req);
+	}
+
+	aio_context_release(ctx);
+
+	assert(!s->dataplane_started);
+	blk_set_enable_write_cache(s->blk, s->original_wce);
+}
+
+static void int_to_binary(unsigned int ptr, char *buf)
+{
+
+	int i;
+
+	for (i = 0; i < 32; i++) 
+	{
+		if( (ptr>>(32-i-1)) & 1) 
+			buf[i] = '1';
+		else
+			buf[i] = '0';
+	}
+	buf[i] = '\0';
+}
+
+
+/* coalesce internal state, copy to pci i/o region 0
+ */
+static void virtio_vssd_update_config(VirtIODevice *vdev, uint8_t *config)
+{
+	VirtIOVssd *s = VIRTIO_VSSD(vdev);
+	BlockConf *conf = &s->conf.conf;
+	struct virtio_vssd_config blkcfg;
+	uint64_t capacity;
+	uint64_t lbn;
+	int blk_size = conf->logical_block_size;
+
+	char buf[50];
+
+	// printf("VSSD: Virtio_vssd_update_config \n ");
+
+	blk_get_geometry(s->blk, &capacity);
+	memset(&blkcfg, 0, sizeof(blkcfg));
+
+	/*UNias start*/
+
+	virtio_stq_p(vdev, &blkcfg.capacity, s->capacity);
+	virtio_stq_p(vdev, &blkcfg.current_capacity, s->current_capacity);
+
+	//int sign = s->command < 0 ? -1 : 1;
+	//blkcfg.command = sign*s->command > SSD_BALLOON_UNIT ? sign*SSD_BALLOON_UNIT : s->command;
+	blkcfg.command = s->command;
+	//s->command -= blkcfg.command;
+
+	// printf("VSSD: virtio_vssd_update_config called.  \n");
+
+	if(bitmap_reading)
+	{
+		uint64_t starting = bitmap_offset * 32;
+		uint64_t ending = starting + 32;
+		if(ending > s->capacity * SSD_BLOCK_SIZE)
+			ending = s->capacity * SSD_BLOCK_SIZE;
+
+		blkcfg.bitmap_value = 0XFFFFFFFF;
+		for(lbn = starting; lbn < ending; lbn++ ) 
+		{
+			if(s->block_list[lbn] != -1)
+				blkcfg.bitmap_value &= ~(1 << (lbn % 32));
+			// if(lbn > 10000 && lbn < 11000)
+			// printf(" [lbn:pbn %lu:%ld] \n", lbn, s->block_list[lbn]);
+		}
+
+		int_to_binary(blkcfg.bitmap_value, buf);
+
+		// printf("VSSD: Reading bitmap. %lu-%lu: [%s] \n",
+		// starting, ending-1, buf);
+
+		bitmap_offset ++;
+	}
+
+
+	/*End*/
+
+	virtio_stl_p(vdev, &blkcfg.seg_max, 128 - 2);
+	virtio_stw_p(vdev, &blkcfg.geometry.cylinders, conf->cyls);
+	virtio_stl_p(vdev, &blkcfg.blk_size, blk_size);
+	virtio_stw_p(vdev, &blkcfg.min_io_size, conf->min_io_size / blk_size);
+	virtio_stw_p(vdev, &blkcfg.opt_io_size, conf->opt_io_size / blk_size);
+
+	blkcfg.current_request_id = resize_request_id;
+
+	blkcfg.geometry.heads = conf->heads;
+
+	// printf("VSSD: \tCapacity = %ld \n", blkcfg.capacity);
+	/*
+	 * We must ensure that the block device capacity is a multiple of
+	 * the logical block size. If that is not the case, let's use
+	 * sector_mask to adopt the geometry to have a correct picture.
+	 * For those devices where the capacity is ok for the given geometry
+	 * we don't touch the sector value of the geometry, since some devices
+	 * (like s390 dasd) need a specific value. Here the capacity is already
+	 * cyls*heads*secs*blk_size and the sector value is not block size
+	 * divided by 512 - instead it is the amount of blk_size blocks
+	 * per track (cylinder).
+	 */
+	if (blk_getlength(s->blk) /  conf->heads / conf->secs % blk_size) {
+		blkcfg.geometry.sectors = conf->secs & ~s->sector_mask;
+	} else {
+		blkcfg.geometry.sectors = conf->secs;
+	}
+	blkcfg.size_max = 0;
+	blkcfg.physical_block_exp = get_physical_block_exp(conf);
+	blkcfg.alignment_offset = 0;
+	blkcfg.wce = blk_enable_write_cache(s->blk);
+	virtio_stw_p(vdev, &blkcfg.num_queues, s->conf.num_queues);
+	memcpy(config, &blkcfg, sizeof(struct virtio_vssd_config));
+}
+
+static void virtio_vssd_set_config(VirtIODevice *vdev, const uint8_t *config)
+{
+
+	VirtIOVssd *s = VIRTIO_VSSD(vdev);
+	struct virtio_vssd_config blkcfg;
+
+	memcpy(&blkcfg, config, sizeof(blkcfg));
+	bitmap_reading = blkcfg.bitmap_reading;
+	bitmap_offset = blkcfg.bitmap_offset;
+
+	// printf("VSSD: virtio_vssd_set_config. offset: %u. count: %u \n",
+	// 	blkcfg.bitmap_offset, blkcfg.bitmap_count);
+
+	aio_context_acquire(blk_get_aio_context(s->blk));
+	blk_set_enable_write_cache(s->blk, blkcfg.wce != 0);
+	aio_context_release(blk_get_aio_context(s->blk));
+}
+
+static uint64_t virtio_vssd_get_features(VirtIODevice *vdev, uint64_t features,
+		Error **errp)
+{
+	VirtIOVssd *s = VIRTIO_VSSD(vdev);
+
+	virtio_add_feature(&features, VIRTIO_VSSD_F_SEG_MAX);
+	virtio_add_feature(&features, VIRTIO_VSSD_F_GEOMETRY);
+	virtio_add_feature(&features, VIRTIO_VSSD_F_TOPOLOGY);
+	virtio_add_feature(&features, VIRTIO_VSSD_F_BLK_SIZE);
+	if (virtio_has_feature(features, VIRTIO_F_VERSION_1)) {
+		if (s->conf.scsi) {
+			error_setg(errp, "Please set scsi=off for virtio-blk devices in order to use virtio 1.0");
+			return 0;
+		}
+	} else {
+		virtio_clear_feature(&features, VIRTIO_F_ANY_LAYOUT);
+		virtio_add_feature(&features, VIRTIO_VSSD_F_SCSI);
+	}
+
+	if (s->conf.config_wce) {
+		virtio_add_feature(&features, VIRTIO_VSSD_F_CONFIG_WCE);
+	}
+	if (blk_enable_write_cache(s->blk)) {
+		virtio_add_feature(&features, VIRTIO_VSSD_F_WCE);
+	}
+	if (blk_is_read_only(s->blk)) {
+		virtio_add_feature(&features, VIRTIO_VSSD_F_RO);
+	}
+	if (s->conf.num_queues > 1) {
+		virtio_add_feature(&features, VIRTIO_VSSD_F_MQ);
+	}
+
+	return features;
+}
+
+static void virtio_vssd_set_status(VirtIODevice *vdev, uint8_t status)
+{
+	VirtIOVssd *s = VIRTIO_VSSD(vdev);
+
+	if (!(status & (VIRTIO_CONFIG_S_DRIVER | VIRTIO_CONFIG_S_DRIVER_OK))) {
+		assert(!s->dataplane_started);
+	}
+
+	if (!(status & VIRTIO_CONFIG_S_DRIVER_OK)) {
+		return;
+	}
+
+	/* A guest that supports VIRTIO_VSSd_F_CONFIG_WCE must be able to send
+	 * cache flushes.  Thus, the "auto writethrough" behavior is never
+	 * necessary for guests that support the VIRTIO_VSSd_F_CONFIG_WCE feature.
+	 * Leaving it enabled would break the following sequence:
+	 *
+	 *     Guest started with "-drive cache=writethrough"
+	 *     Guest sets status to 0
+	 *     Guest sets DRIVER bit in status field
+	 *     Guest reads host features (WCE=0, CONFIG_WCE=1)
+	 *     Guest writes guest features (WCE=0, CONFIG_WCE=1)
+	 *     Guest writes 1 to the WCE configuration field (writeback mode)
+	 *     Guest sets DRIVER_OK bit in status field
+	 *
+	 * s->blk would erroneously be placed in writethrough mode.
+	 */
+	if (!virtio_vdev_has_feature(vdev, VIRTIO_VSSD_F_CONFIG_WCE)) {
+		aio_context_acquire(blk_get_aio_context(s->blk));
+		blk_set_enable_write_cache(s->blk,
+				virtio_vdev_has_feature(vdev,
+					VIRTIO_VSSD_F_WCE));
+		aio_context_release(blk_get_aio_context(s->blk));
+	}
+}
+
+static void virtio_vssd_save_device(VirtIODevice *vdev, QEMUFile *f)
+{
+	VirtIOVssd *s = VIRTIO_VSSD(vdev);
+	VirtIOVssdReq *req = s->rq;
+
+	while (req) {
+		qemu_put_sbyte(f, 1);
+
+		if (s->conf.num_queues > 1) {
+			qemu_put_be32(f, virtio_get_queue_index(req->vq));
+		}
+
+		qemu_put_virtqueue_element(f, &req->elem);
+		req = req->next;
+	}
+	qemu_put_sbyte(f, 0);
+}
+
+static int virtio_vssd_load_device(VirtIODevice *vdev, QEMUFile *f,
+		int version_id)
+{
+	VirtIOVssd *s = VIRTIO_VSSD(vdev);
+
+	while (qemu_get_sbyte(f)) {
+		unsigned nvqs = s->conf.num_queues;
+		unsigned vq_idx = 0;
+		VirtIOVssdReq *req;
+
+		if (nvqs > 1) {
+			vq_idx = qemu_get_be32(f);
+
+			if (vq_idx >= nvqs) {
+				error_report("Invalid virtqueue index in request list: %#x",
+						vq_idx);
+				return -EINVAL;
+			}
+		}
+
+		req = qemu_get_virtqueue_element(vdev, f, sizeof(VirtIOVssdReq));
+		virtio_vssd_init_request(s, virtio_get_queue(vdev, vq_idx), req);
+		req->next = s->rq;
+		s->rq = req;
+	}
+
+	return 0;
+}
+
+static void virtio_vssd_resize(void *opaque)
+{
+	VirtIODevice *vdev = VIRTIO_DEVICE(opaque);
+
+	virtio_notify_config(vdev);
+}
+
+static const BlockDevOps virtio_block_ops = {
+	.resize_cb = virtio_vssd_resize,
+};
+
+static void virtio_vssd_device_realize(DeviceState *dev, Error **errp)
+{
+	printf("\n\n\nVSSD: virtio_vssd_device_realize function called \n");   
+
+	// extern int test_vm_id;
+	// extern char test_vm_name;
+	// printf("id = %d, name = %s \n", test_vm_id, test_vm_name);
+
+	//pritf("vm-id  = %d \n", test_vm_id);
+	//exit(1);
+
+	VirtIODevice *vdev = VIRTIO_DEVICE(dev);
+	VirtIOVssd *s = VIRTIO_VSSD(dev);
+	VirtIOVssdConf *conf = &s->conf;
+	Error *err = NULL;
+	int ret;
+	unsigned i;
+
+	if (!conf->conf.blk) {
+		error_setg(errp, "drive property not set");
+		return;
+	}
+	if (!blk_is_inserted(conf->conf.blk)) {
+		error_setg(errp, "Device needs media, but drive is empty");
+		return;
+	}
+	if (!conf->num_queues) {
+		error_setg(errp, "num-queues property must be larger than 0");
+		return;
+	}
+
+	blkconf_serial(&conf->conf, &conf->serial);
+	blkconf_apply_backend_options(&conf->conf,
+			blk_is_read_only(conf->conf.blk), true,
+			&err);
+	if (err) {
+		error_propagate(errp, err);
+		return;
+	}
+
+
+	s->original_wce = blk_enable_write_cache(conf->conf.blk);
+	blkconf_geometry(&conf->conf, NULL, 65535, 255, 255, &err);
+	if (err) {
+		error_propagate(errp, err);
+		return;
+	}
+	blkconf_blocksizes(&conf->conf);
+	printf("VSSD: \tLogical block size=%d, Physicalblock size=%d \n",
+			conf->conf.logical_block_size, conf->conf.physical_block_size);
+
+
+	virtio_init(vdev, "virtio-vssd", VIRTIO_ID_VSSD,
+			sizeof(struct virtio_vssd_config));
+
+
+	/*ToDoUnais  Read vssd size as command line argumenet to qemu*/  
+	// int capacity_in_gb = 10;
+	s->capacity = gb_to_sectors (vssd_size); 
+	// s->current_capacity = s->capacity;
+	s->current_capacity = 0;
+
+	printf("VSSD: \tvssd capacity = %ld \n", s->capacity);
+
+	s->blk = conf->conf.blk;
+	s->rq = NULL;
+	s->sector_mask = (s->conf.conf.logical_block_size / BDRV_SECTOR_SIZE) - 1;
+
+	printf("VSSD: \tAdding %d queues to virtio \n", conf->num_queues);
+	for (i = 0; i < conf->num_queues; i++) {
+		virtio_add_queue(vdev, 128, virtio_vssd_handle_output);
+	}
+
+	/*UNAIS START*/
+	ret = virtio_setup_vssd(s);
+	if(ret < 0)
+		exit(1);
+	/*END*/
+
+
+	virtio_vssd_data_plane_create(vdev, conf, &s->dataplane, &err);
+	if (err != NULL) {
+		error_propagate(errp, err);
+		virtio_cleanup(vdev);
+		return;
+	}
+
+
+	printf("VSSD: \tData plane created \n");
+
+	s->change = qemu_add_vm_change_state_handler(virtio_vssd_dma_restart_cb, s);
+	blk_set_dev_ops(s->blk, &virtio_block_ops, s);
+	blk_set_guest_block_size(s->blk, s->conf.conf.logical_block_size);
+
+	blk_iostatus_enable(s->blk);
+
+
+	printf("VSSD: \tRealize completed \n");
+}
+
+static void virtio_vssd_device_unrealize(DeviceState *dev, Error **errp)
+{
+	VirtIODevice *vdev = VIRTIO_DEVICE(dev);
+	VirtIOVssd *s = VIRTIO_VSSD(dev);
+
+	virtio_vssd_data_plane_destroy(s->dataplane);
+	s->dataplane = NULL;
+	qemu_del_vm_change_state_handler(s->change);
+	blockdev_mark_auto_del(s->blk);
+	virtio_cleanup(vdev);
+}
+
+static void virtio_vssd_instance_init(Object *obj)
+{
+	VirtIOVssd *s = VIRTIO_VSSD(obj);
+
+	object_property_add_link(obj, "iothread", TYPE_IOTHREAD,
+			(Object **)&s->conf.iothread,
+			qdev_prop_allow_set_link_before_realize,
+			OBJ_PROP_LINK_UNREF_ON_RELEASE, NULL);
+	device_add_bootindex_property(obj, &s->conf.conf.bootindex,
+			"bootindex", "/disk@0,0",
+			DEVICE(obj), NULL);
+}
+
+static const VMStateDescription vmstate_virtio_vssd = {
+	.name = "virtio-vssd",
+	.minimum_version_id = 2,
+	.version_id = 2,
+	.fields = (VMStateField[]) {
+		VMSTATE_VIRTIO_DEVICE,
+		VMSTATE_END_OF_LIST()
+	},
+};
+
+static Property virtio_vssd_properties[] = {
+	DEFINE_BLOCK_PROPERTIES(VirtIOVssd, conf.conf),
+	DEFINE_BLOCK_ERROR_PROPERTIES(VirtIOVssd, conf.conf),
+	DEFINE_BLOCK_CHS_PROPERTIES(VirtIOVssd, conf.conf),
+	DEFINE_PROP_STRING("serial", VirtIOVssd, conf.serial),
+	DEFINE_PROP_BIT("config-wce", VirtIOVssd, conf.config_wce, 0, true),
+#ifdef __linux__
+	DEFINE_PROP_BIT("scsi", VirtIOVssd, conf.scsi, 0, false),
+#endif
+	DEFINE_PROP_BIT("request-merging", VirtIOVssd, conf.request_merging, 0,
+			true),
+	DEFINE_PROP_UINT16("num-queues", VirtIOVssd, conf.num_queues, 1),
+	DEFINE_PROP_END_OF_LIST(),
+};
+
+static void virtio_vssd_class_init(ObjectClass *klass, void *data)
+{
+	printf("virtio: 'virtio_vssd_class_init' function called \n");   
+
+	DeviceClass *dc = DEVICE_CLASS(klass);
+	VirtioDeviceClass *vdc = VIRTIO_DEVICE_CLASS(klass);
+
+	dc->props = virtio_vssd_properties;
+	dc->vmsd = &vmstate_virtio_vssd;
+	set_bit(DEVICE_CATEGORY_STORAGE, dc->categories);
+	vdc->realize = virtio_vssd_device_realize;
+	vdc->unrealize = virtio_vssd_device_unrealize;
+	vdc->get_config = virtio_vssd_update_config;
+	vdc->set_config = virtio_vssd_set_config;
+	vdc->get_features = virtio_vssd_get_features;
+	vdc->set_status = virtio_vssd_set_status;
+	vdc->reset = virtio_vssd_reset;
+	vdc->save = virtio_vssd_save_device;
+	vdc->load = virtio_vssd_load_device;
+	vdc->start_ioeventfd = virtio_vssd_data_plane_start;
+	vdc->stop_ioeventfd = virtio_vssd_data_plane_stop;
+}
+
+static const TypeInfo virtio_vssd_info = {
+	.name = TYPE_VIRTIO_VSSD,
+	.parent = TYPE_VIRTIO_DEVICE,
+	.instance_size = sizeof(VirtIOVssd),
+	.instance_init = virtio_vssd_instance_init,
+	.class_init = virtio_vssd_class_init,
+};
+
+
+
+static void virtio_register_types(void)
+{
+	printf("Registration \n");   
+	type_register_static(&virtio_vssd_info);
+	printf("\tvirtio-vssd device registered. \n");
+}
+
+type_init(virtio_register_types)
diff -rupN /home/prafull/Desktop/qemu-2.9.0/hw/virtio/virtio.c /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/virtio/virtio.c
--- /home/prafull/Desktop/qemu-2.9.0/hw/virtio/virtio.c	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/virtio/virtio.c	2018-05-28 13:06:18.000000000 +0530
@@ -411,6 +411,11 @@ static void virtqueue_unmap_sg(VirtQueue
 void virtqueue_detach_element(VirtQueue *vq, const VirtQueueElement *elem,
                               unsigned int len)
 {
+    /* Added by Deba. 2017.05.29. Begin Add. */
+    if(vq->inuse <= 0)
+         printf("Inuse is bad\n");
+    /* Added by Deba. 2017.05.29. End Add. */
+
     vq->inuse--;
     virtqueue_unmap_sg(vq, elem, len);
 }
diff -rupN /home/prafull/Desktop/qemu-2.9.0/hw/virtio/virtio.c.orig /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/virtio/virtio.c.orig
--- /home/prafull/Desktop/qemu-2.9.0/hw/virtio/virtio.c.orig	1970-01-01 05:30:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/virtio/virtio.c.orig	2018-05-28 13:06:18.000000000 +0530
@@ -0,0 +1,2685 @@
+/*
+ * Virtio Support
+ *
+ * Copyright IBM, Corp. 2007
+ *
+ * Authors:
+ *  Anthony Liguori   <aliguori@us.ibm.com>
+ *
+ * This work is licensed under the terms of the GNU GPL, version 2.  See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include "qemu/osdep.h"
+#include "qapi/error.h"
+#include "qemu-common.h"
+#include "cpu.h"
+#include "trace.h"
+#include "exec/address-spaces.h"
+#include "qemu/error-report.h"
+#include "hw/virtio/virtio.h"
+#include "qemu/atomic.h"
+#include "hw/virtio/virtio-bus.h"
+#include "migration/migration.h"
+#include "hw/virtio/virtio-access.h"
+#include "sysemu/dma.h"
+
+/*
+ * The alignment to use between consumer and producer parts of vring.
+ * x86 pagesize again. This is the default, used by transports like PCI
+ * which don't provide a means for the guest to tell the host the alignment.
+ */
+#define VIRTIO_PCI_VRING_ALIGN         4096
+
+typedef struct VRingDesc
+{
+    uint64_t addr;
+    uint32_t len;
+    uint16_t flags;
+    uint16_t next;
+} VRingDesc;
+
+typedef struct VRingAvail
+{
+    uint16_t flags;
+    uint16_t idx;
+    uint16_t ring[0];
+} VRingAvail;
+
+typedef struct VRingUsedElem
+{
+    uint32_t id;
+    uint32_t len;
+} VRingUsedElem;
+
+typedef struct VRingUsed
+{
+    uint16_t flags;
+    uint16_t idx;
+    VRingUsedElem ring[0];
+} VRingUsed;
+
+typedef struct VRingMemoryRegionCaches {
+    struct rcu_head rcu;
+    MemoryRegionCache desc;
+    MemoryRegionCache avail;
+    MemoryRegionCache used;
+} VRingMemoryRegionCaches;
+
+typedef struct VRing
+{
+    unsigned int num;
+    unsigned int num_default;
+    unsigned int align;
+    hwaddr desc;
+    hwaddr avail;
+    hwaddr used;
+    VRingMemoryRegionCaches *caches;
+} VRing;
+
+struct VirtQueue
+{
+    VRing vring;
+
+    /* Next head to pop */
+    uint16_t last_avail_idx;
+
+    /* Last avail_idx read from VQ. */
+    uint16_t shadow_avail_idx;
+
+    uint16_t used_idx;
+
+    /* Last used index value we have signalled on */
+    uint16_t signalled_used;
+
+    /* Last used index value we have signalled on */
+    bool signalled_used_valid;
+
+    /* Notification enabled? */
+    bool notification;
+
+    uint16_t queue_index;
+
+    unsigned int inuse;
+
+    uint16_t vector;
+    VirtIOHandleOutput handle_output;
+    VirtIOHandleAIOOutput handle_aio_output;
+    VirtIODevice *vdev;
+    EventNotifier guest_notifier;
+    EventNotifier host_notifier;
+    QLIST_ENTRY(VirtQueue) node;
+};
+
+static void virtio_free_region_cache(VRingMemoryRegionCaches *caches)
+{
+    if (!caches) {
+        return;
+    }
+
+    address_space_cache_destroy(&caches->desc);
+    address_space_cache_destroy(&caches->avail);
+    address_space_cache_destroy(&caches->used);
+    g_free(caches);
+}
+
+static void virtio_init_region_cache(VirtIODevice *vdev, int n)
+{
+    VirtQueue *vq = &vdev->vq[n];
+    VRingMemoryRegionCaches *old = vq->vring.caches;
+    VRingMemoryRegionCaches *new;
+    hwaddr addr, size;
+    int event_size;
+    int64_t len;
+
+    event_size = virtio_vdev_has_feature(vq->vdev, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;
+
+    addr = vq->vring.desc;
+    if (!addr) {
+        return;
+    }
+    new = g_new0(VRingMemoryRegionCaches, 1);
+    size = virtio_queue_get_desc_size(vdev, n);
+    len = address_space_cache_init(&new->desc, vdev->dma_as,
+                                   addr, size, false);
+    if (len < size) {
+        virtio_error(vdev, "Cannot map desc");
+        goto err_desc;
+    }
+
+    size = virtio_queue_get_used_size(vdev, n) + event_size;
+    len = address_space_cache_init(&new->used, vdev->dma_as,
+                                   vq->vring.used, size, true);
+    if (len < size) {
+        virtio_error(vdev, "Cannot map used");
+        goto err_used;
+    }
+
+    size = virtio_queue_get_avail_size(vdev, n) + event_size;
+    len = address_space_cache_init(&new->avail, vdev->dma_as,
+                                   vq->vring.avail, size, false);
+    if (len < size) {
+        virtio_error(vdev, "Cannot map avail");
+        goto err_avail;
+    }
+
+    atomic_rcu_set(&vq->vring.caches, new);
+    if (old) {
+        call_rcu(old, virtio_free_region_cache, rcu);
+    }
+    return;
+
+err_avail:
+    address_space_cache_destroy(&new->used);
+err_used:
+    address_space_cache_destroy(&new->desc);
+err_desc:
+    g_free(new);
+}
+
+/* virt queue functions */
+void virtio_queue_update_rings(VirtIODevice *vdev, int n)
+{
+    VRing *vring = &vdev->vq[n].vring;
+
+    if (!vring->desc) {
+        /* not yet setup -> nothing to do */
+        return;
+    }
+    vring->avail = vring->desc + vring->num * sizeof(VRingDesc);
+    vring->used = vring_align(vring->avail +
+                              offsetof(VRingAvail, ring[vring->num]),
+                              vring->align);
+    virtio_init_region_cache(vdev, n);
+}
+
+/* Called within rcu_read_lock().  */
+static void vring_desc_read(VirtIODevice *vdev, VRingDesc *desc,
+                            MemoryRegionCache *cache, int i)
+{
+    address_space_read_cached(cache, i * sizeof(VRingDesc),
+                              desc, sizeof(VRingDesc));
+    virtio_tswap64s(vdev, &desc->addr);
+    virtio_tswap32s(vdev, &desc->len);
+    virtio_tswap16s(vdev, &desc->flags);
+    virtio_tswap16s(vdev, &desc->next);
+}
+
+static VRingMemoryRegionCaches *vring_get_region_caches(struct VirtQueue *vq)
+{
+    VRingMemoryRegionCaches *caches = atomic_rcu_read(&vq->vring.caches);
+    assert(caches != NULL);
+    return caches;
+}
+/* Called within rcu_read_lock().  */
+static inline uint16_t vring_avail_flags(VirtQueue *vq)
+{
+    VRingMemoryRegionCaches *caches = vring_get_region_caches(vq);
+    hwaddr pa = offsetof(VRingAvail, flags);
+    return virtio_lduw_phys_cached(vq->vdev, &caches->avail, pa);
+}
+
+/* Called within rcu_read_lock().  */
+static inline uint16_t vring_avail_idx(VirtQueue *vq)
+{
+    VRingMemoryRegionCaches *caches = vring_get_region_caches(vq);
+    hwaddr pa = offsetof(VRingAvail, idx);
+    vq->shadow_avail_idx = virtio_lduw_phys_cached(vq->vdev, &caches->avail, pa);
+    return vq->shadow_avail_idx;
+}
+
+/* Called within rcu_read_lock().  */
+static inline uint16_t vring_avail_ring(VirtQueue *vq, int i)
+{
+    VRingMemoryRegionCaches *caches = vring_get_region_caches(vq);
+    hwaddr pa = offsetof(VRingAvail, ring[i]);
+    return virtio_lduw_phys_cached(vq->vdev, &caches->avail, pa);
+}
+
+/* Called within rcu_read_lock().  */
+static inline uint16_t vring_get_used_event(VirtQueue *vq)
+{
+    return vring_avail_ring(vq, vq->vring.num);
+}
+
+/* Called within rcu_read_lock().  */
+static inline void vring_used_write(VirtQueue *vq, VRingUsedElem *uelem,
+                                    int i)
+{
+    VRingMemoryRegionCaches *caches = vring_get_region_caches(vq);
+    hwaddr pa = offsetof(VRingUsed, ring[i]);
+    virtio_tswap32s(vq->vdev, &uelem->id);
+    virtio_tswap32s(vq->vdev, &uelem->len);
+    address_space_write_cached(&caches->used, pa, uelem, sizeof(VRingUsedElem));
+    address_space_cache_invalidate(&caches->used, pa, sizeof(VRingUsedElem));
+}
+
+/* Called within rcu_read_lock().  */
+static uint16_t vring_used_idx(VirtQueue *vq)
+{
+    VRingMemoryRegionCaches *caches = vring_get_region_caches(vq);
+    hwaddr pa = offsetof(VRingUsed, idx);
+    return virtio_lduw_phys_cached(vq->vdev, &caches->used, pa);
+}
+
+/* Called within rcu_read_lock().  */
+static inline void vring_used_idx_set(VirtQueue *vq, uint16_t val)
+{
+    VRingMemoryRegionCaches *caches = vring_get_region_caches(vq);
+    hwaddr pa = offsetof(VRingUsed, idx);
+    virtio_stw_phys_cached(vq->vdev, &caches->used, pa, val);
+    address_space_cache_invalidate(&caches->used, pa, sizeof(val));
+    vq->used_idx = val;
+}
+
+/* Called within rcu_read_lock().  */
+static inline void vring_used_flags_set_bit(VirtQueue *vq, int mask)
+{
+    VRingMemoryRegionCaches *caches = vring_get_region_caches(vq);
+    VirtIODevice *vdev = vq->vdev;
+    hwaddr pa = offsetof(VRingUsed, flags);
+    uint16_t flags = virtio_lduw_phys_cached(vq->vdev, &caches->used, pa);
+
+    virtio_stw_phys_cached(vdev, &caches->used, pa, flags | mask);
+    address_space_cache_invalidate(&caches->used, pa, sizeof(flags));
+}
+
+/* Called within rcu_read_lock().  */
+static inline void vring_used_flags_unset_bit(VirtQueue *vq, int mask)
+{
+    VRingMemoryRegionCaches *caches = vring_get_region_caches(vq);
+    VirtIODevice *vdev = vq->vdev;
+    hwaddr pa = offsetof(VRingUsed, flags);
+    uint16_t flags = virtio_lduw_phys_cached(vq->vdev, &caches->used, pa);
+
+    virtio_stw_phys_cached(vdev, &caches->used, pa, flags & ~mask);
+    address_space_cache_invalidate(&caches->used, pa, sizeof(flags));
+}
+
+/* Called within rcu_read_lock().  */
+static inline void vring_set_avail_event(VirtQueue *vq, uint16_t val)
+{
+    VRingMemoryRegionCaches *caches;
+    hwaddr pa;
+    if (!vq->notification) {
+        return;
+    }
+
+    caches = vring_get_region_caches(vq);
+    pa = offsetof(VRingUsed, ring[vq->vring.num]);
+    virtio_stw_phys_cached(vq->vdev, &caches->used, pa, val);
+    address_space_cache_invalidate(&caches->used, pa, sizeof(val));
+}
+
+void virtio_queue_set_notification(VirtQueue *vq, int enable)
+{
+    vq->notification = enable;
+
+    if (!vq->vring.desc) {
+        return;
+    }
+
+    rcu_read_lock();
+    if (virtio_vdev_has_feature(vq->vdev, VIRTIO_RING_F_EVENT_IDX)) {
+        vring_set_avail_event(vq, vring_avail_idx(vq));
+    } else if (enable) {
+        vring_used_flags_unset_bit(vq, VRING_USED_F_NO_NOTIFY);
+    } else {
+        vring_used_flags_set_bit(vq, VRING_USED_F_NO_NOTIFY);
+    }
+    if (enable) {
+        /* Expose avail event/used flags before caller checks the avail idx. */
+        smp_mb();
+    }
+    rcu_read_unlock();
+}
+
+int virtio_queue_ready(VirtQueue *vq)
+{
+    return vq->vring.avail != 0;
+}
+
+/* Fetch avail_idx from VQ memory only when we really need to know if
+ * guest has added some buffers.
+ * Called within rcu_read_lock().  */
+static int virtio_queue_empty_rcu(VirtQueue *vq)
+{
+    if (unlikely(!vq->vring.avail)) {
+        return 1;
+    }
+
+    if (vq->shadow_avail_idx != vq->last_avail_idx) {
+        return 0;
+    }
+
+    return vring_avail_idx(vq) == vq->last_avail_idx;
+}
+
+int virtio_queue_empty(VirtQueue *vq)
+{
+    bool empty;
+
+    if (unlikely(!vq->vring.avail)) {
+        return 1;
+    }
+
+    if (vq->shadow_avail_idx != vq->last_avail_idx) {
+        return 0;
+    }
+
+    rcu_read_lock();
+    empty = vring_avail_idx(vq) == vq->last_avail_idx;
+    rcu_read_unlock();
+    return empty;
+}
+
+static void virtqueue_unmap_sg(VirtQueue *vq, const VirtQueueElement *elem,
+                               unsigned int len)
+{
+    AddressSpace *dma_as = vq->vdev->dma_as;
+    unsigned int offset;
+    int i;
+
+    offset = 0;
+    for (i = 0; i < elem->in_num; i++) {
+        size_t size = MIN(len - offset, elem->in_sg[i].iov_len);
+
+        dma_memory_unmap(dma_as, elem->in_sg[i].iov_base,
+                         elem->in_sg[i].iov_len,
+                         DMA_DIRECTION_FROM_DEVICE, size);
+
+        offset += size;
+    }
+
+    for (i = 0; i < elem->out_num; i++)
+        dma_memory_unmap(dma_as, elem->out_sg[i].iov_base,
+                         elem->out_sg[i].iov_len,
+                         DMA_DIRECTION_TO_DEVICE,
+                         elem->out_sg[i].iov_len);
+}
+
+/* virtqueue_detach_element:
+ * @vq: The #VirtQueue
+ * @elem: The #VirtQueueElement
+ * @len: number of bytes written
+ *
+ * Detach the element from the virtqueue.  This function is suitable for device
+ * reset or other situations where a #VirtQueueElement is simply freed and will
+ * not be pushed or discarded.
+ */
+void virtqueue_detach_element(VirtQueue *vq, const VirtQueueElement *elem,
+                              unsigned int len)
+{
+    vq->inuse--;
+    virtqueue_unmap_sg(vq, elem, len);
+}
+
+/* virtqueue_unpop:
+ * @vq: The #VirtQueue
+ * @elem: The #VirtQueueElement
+ * @len: number of bytes written
+ *
+ * Pretend the most recent element wasn't popped from the virtqueue.  The next
+ * call to virtqueue_pop() will refetch the element.
+ */
+void virtqueue_unpop(VirtQueue *vq, const VirtQueueElement *elem,
+                     unsigned int len)
+{
+    vq->last_avail_idx--;
+    virtqueue_detach_element(vq, elem, len);
+}
+
+/* virtqueue_rewind:
+ * @vq: The #VirtQueue
+ * @num: Number of elements to push back
+ *
+ * Pretend that elements weren't popped from the virtqueue.  The next
+ * virtqueue_pop() will refetch the oldest element.
+ *
+ * Use virtqueue_unpop() instead if you have a VirtQueueElement.
+ *
+ * Returns: true on success, false if @num is greater than the number of in use
+ * elements.
+ */
+bool virtqueue_rewind(VirtQueue *vq, unsigned int num)
+{
+    if (num > vq->inuse) {
+        return false;
+    }
+    vq->last_avail_idx -= num;
+    vq->inuse -= num;
+    return true;
+}
+
+/* Called within rcu_read_lock().  */
+void virtqueue_fill(VirtQueue *vq, const VirtQueueElement *elem,
+                    unsigned int len, unsigned int idx)
+{
+    VRingUsedElem uelem;
+
+    trace_virtqueue_fill(vq, elem, len, idx);
+
+    virtqueue_unmap_sg(vq, elem, len);
+
+    if (unlikely(vq->vdev->broken)) {
+        return;
+    }
+
+    if (unlikely(!vq->vring.used)) {
+        return;
+    }
+
+    idx = (idx + vq->used_idx) % vq->vring.num;
+
+    uelem.id = elem->index;
+    uelem.len = len;
+    vring_used_write(vq, &uelem, idx);
+}
+
+/* Called within rcu_read_lock().  */
+void virtqueue_flush(VirtQueue *vq, unsigned int count)
+{
+    uint16_t old, new;
+
+    if (unlikely(vq->vdev->broken)) {
+        vq->inuse -= count;
+        return;
+    }
+
+    if (unlikely(!vq->vring.used)) {
+        return;
+    }
+
+    /* Make sure buffer is written before we update index. */
+    smp_wmb();
+    trace_virtqueue_flush(vq, count);
+    old = vq->used_idx;
+    new = old + count;
+    vring_used_idx_set(vq, new);
+    vq->inuse -= count;
+    if (unlikely((int16_t)(new - vq->signalled_used) < (uint16_t)(new - old)))
+        vq->signalled_used_valid = false;
+}
+
+void virtqueue_push(VirtQueue *vq, const VirtQueueElement *elem,
+                    unsigned int len)
+{
+    rcu_read_lock();
+    virtqueue_fill(vq, elem, len, 0);
+    virtqueue_flush(vq, 1);
+    rcu_read_unlock();
+}
+
+/* Called within rcu_read_lock().  */
+static int virtqueue_num_heads(VirtQueue *vq, unsigned int idx)
+{
+    uint16_t num_heads = vring_avail_idx(vq) - idx;
+
+    /* Check it isn't doing very strange things with descriptor numbers. */
+    if (num_heads > vq->vring.num) {
+        virtio_error(vq->vdev, "Guest moved used index from %u to %u",
+                     idx, vq->shadow_avail_idx);
+        return -EINVAL;
+    }
+    /* On success, callers read a descriptor at vq->last_avail_idx.
+     * Make sure descriptor read does not bypass avail index read. */
+    if (num_heads) {
+        smp_rmb();
+    }
+
+    return num_heads;
+}
+
+/* Called within rcu_read_lock().  */
+static bool virtqueue_get_head(VirtQueue *vq, unsigned int idx,
+                               unsigned int *head)
+{
+    /* Grab the next descriptor number they're advertising, and increment
+     * the index we've seen. */
+    *head = vring_avail_ring(vq, idx % vq->vring.num);
+
+    /* If their number is silly, that's a fatal mistake. */
+    if (*head >= vq->vring.num) {
+        virtio_error(vq->vdev, "Guest says index %u is available", *head);
+        return false;
+    }
+
+    return true;
+}
+
+enum {
+    VIRTQUEUE_READ_DESC_ERROR = -1,
+    VIRTQUEUE_READ_DESC_DONE = 0,   /* end of chain */
+    VIRTQUEUE_READ_DESC_MORE = 1,   /* more buffers in chain */
+};
+
+static int virtqueue_read_next_desc(VirtIODevice *vdev, VRingDesc *desc,
+                                    MemoryRegionCache *desc_cache, unsigned int max,
+                                    unsigned int *next)
+{
+    /* If this descriptor says it doesn't chain, we're done. */
+    if (!(desc->flags & VRING_DESC_F_NEXT)) {
+        return VIRTQUEUE_READ_DESC_DONE;
+    }
+
+    /* Check they're not leading us off end of descriptors. */
+    *next = desc->next;
+    /* Make sure compiler knows to grab that: we don't want it changing! */
+    smp_wmb();
+
+    if (*next >= max) {
+        virtio_error(vdev, "Desc next is %u", *next);
+        return VIRTQUEUE_READ_DESC_ERROR;
+    }
+
+    vring_desc_read(vdev, desc, desc_cache, *next);
+    return VIRTQUEUE_READ_DESC_MORE;
+}
+
+void virtqueue_get_avail_bytes(VirtQueue *vq, unsigned int *in_bytes,
+                               unsigned int *out_bytes,
+                               unsigned max_in_bytes, unsigned max_out_bytes)
+{
+    VirtIODevice *vdev = vq->vdev;
+    unsigned int max, idx;
+    unsigned int total_bufs, in_total, out_total;
+    VRingMemoryRegionCaches *caches;
+    MemoryRegionCache indirect_desc_cache = MEMORY_REGION_CACHE_INVALID;
+    int64_t len = 0;
+    int rc;
+
+    if (unlikely(!vq->vring.desc)) {
+        if (in_bytes) {
+            *in_bytes = 0;
+        }
+        if (out_bytes) {
+            *out_bytes = 0;
+        }
+        return;
+    }
+
+    rcu_read_lock();
+    idx = vq->last_avail_idx;
+    total_bufs = in_total = out_total = 0;
+
+    max = vq->vring.num;
+    caches = vring_get_region_caches(vq);
+    if (caches->desc.len < max * sizeof(VRingDesc)) {
+        virtio_error(vdev, "Cannot map descriptor ring");
+        goto err;
+    }
+
+    while ((rc = virtqueue_num_heads(vq, idx)) > 0) {
+        MemoryRegionCache *desc_cache = &caches->desc;
+        unsigned int num_bufs;
+        VRingDesc desc;
+        unsigned int i;
+
+        num_bufs = total_bufs;
+
+        if (!virtqueue_get_head(vq, idx++, &i)) {
+            goto err;
+        }
+
+        vring_desc_read(vdev, &desc, desc_cache, i);
+
+        if (desc.flags & VRING_DESC_F_INDIRECT) {
+            if (desc.len % sizeof(VRingDesc)) {
+                virtio_error(vdev, "Invalid size for indirect buffer table");
+                goto err;
+            }
+
+            /* If we've got too many, that implies a descriptor loop. */
+            if (num_bufs >= max) {
+                virtio_error(vdev, "Looped descriptor");
+                goto err;
+            }
+
+            /* loop over the indirect descriptor table */
+            len = address_space_cache_init(&indirect_desc_cache,
+                                           vdev->dma_as,
+                                           desc.addr, desc.len, false);
+            desc_cache = &indirect_desc_cache;
+            if (len < desc.len) {
+                virtio_error(vdev, "Cannot map indirect buffer");
+                goto err;
+            }
+
+            max = desc.len / sizeof(VRingDesc);
+            num_bufs = i = 0;
+            vring_desc_read(vdev, &desc, desc_cache, i);
+        }
+
+        do {
+            /* If we've got too many, that implies a descriptor loop. */
+            if (++num_bufs > max) {
+                virtio_error(vdev, "Looped descriptor");
+                goto err;
+            }
+
+            if (desc.flags & VRING_DESC_F_WRITE) {
+                in_total += desc.len;
+            } else {
+                out_total += desc.len;
+            }
+            if (in_total >= max_in_bytes && out_total >= max_out_bytes) {
+                goto done;
+            }
+
+            rc = virtqueue_read_next_desc(vdev, &desc, desc_cache, max, &i);
+        } while (rc == VIRTQUEUE_READ_DESC_MORE);
+
+        if (rc == VIRTQUEUE_READ_DESC_ERROR) {
+            goto err;
+        }
+
+        if (desc_cache == &indirect_desc_cache) {
+            address_space_cache_destroy(&indirect_desc_cache);
+            total_bufs++;
+        } else {
+            total_bufs = num_bufs;
+        }
+    }
+
+    if (rc < 0) {
+        goto err;
+    }
+
+done:
+    address_space_cache_destroy(&indirect_desc_cache);
+    if (in_bytes) {
+        *in_bytes = in_total;
+    }
+    if (out_bytes) {
+        *out_bytes = out_total;
+    }
+    rcu_read_unlock();
+    return;
+
+err:
+    in_total = out_total = 0;
+    goto done;
+}
+
+int virtqueue_avail_bytes(VirtQueue *vq, unsigned int in_bytes,
+                          unsigned int out_bytes)
+{
+    unsigned int in_total, out_total;
+
+    virtqueue_get_avail_bytes(vq, &in_total, &out_total, in_bytes, out_bytes);
+    return in_bytes <= in_total && out_bytes <= out_total;
+}
+
+static bool virtqueue_map_desc(VirtIODevice *vdev, unsigned int *p_num_sg,
+                               hwaddr *addr, struct iovec *iov,
+                               unsigned int max_num_sg, bool is_write,
+                               hwaddr pa, size_t sz)
+{
+    bool ok = false;
+    unsigned num_sg = *p_num_sg;
+    assert(num_sg <= max_num_sg);
+
+    if (!sz) {
+        virtio_error(vdev, "virtio: zero sized buffers are not allowed");
+        goto out;
+    }
+
+    while (sz) {
+        hwaddr len = sz;
+
+        if (num_sg == max_num_sg) {
+            virtio_error(vdev, "virtio: too many write descriptors in "
+                               "indirect table");
+            goto out;
+        }
+
+        iov[num_sg].iov_base = dma_memory_map(vdev->dma_as, pa, &len,
+                                              is_write ?
+                                              DMA_DIRECTION_FROM_DEVICE :
+                                              DMA_DIRECTION_TO_DEVICE);
+        if (!iov[num_sg].iov_base) {
+            virtio_error(vdev, "virtio: bogus descriptor or out of resources");
+            goto out;
+        }
+
+        iov[num_sg].iov_len = len;
+        addr[num_sg] = pa;
+
+        sz -= len;
+        pa += len;
+        num_sg++;
+    }
+    ok = true;
+
+out:
+    *p_num_sg = num_sg;
+    return ok;
+}
+
+/* Only used by error code paths before we have a VirtQueueElement (therefore
+ * virtqueue_unmap_sg() can't be used).  Assumes buffers weren't written to
+ * yet.
+ */
+static void virtqueue_undo_map_desc(unsigned int out_num, unsigned int in_num,
+                                    struct iovec *iov)
+{
+    unsigned int i;
+
+    for (i = 0; i < out_num + in_num; i++) {
+        int is_write = i >= out_num;
+
+        cpu_physical_memory_unmap(iov->iov_base, iov->iov_len, is_write, 0);
+        iov++;
+    }
+}
+
+static void virtqueue_map_iovec(VirtIODevice *vdev, struct iovec *sg,
+                                hwaddr *addr, unsigned int *num_sg,
+                                int is_write)
+{
+    unsigned int i;
+    hwaddr len;
+
+    for (i = 0; i < *num_sg; i++) {
+        len = sg[i].iov_len;
+        sg[i].iov_base = dma_memory_map(vdev->dma_as,
+                                        addr[i], &len, is_write ?
+                                        DMA_DIRECTION_FROM_DEVICE :
+                                        DMA_DIRECTION_TO_DEVICE);
+        if (!sg[i].iov_base) {
+            error_report("virtio: error trying to map MMIO memory");
+            exit(1);
+        }
+        if (len != sg[i].iov_len) {
+            error_report("virtio: unexpected memory split");
+            exit(1);
+        }
+    }
+}
+
+void virtqueue_map(VirtIODevice *vdev, VirtQueueElement *elem)
+{
+    virtqueue_map_iovec(vdev, elem->in_sg, elem->in_addr, &elem->in_num, 1);
+    virtqueue_map_iovec(vdev, elem->out_sg, elem->out_addr, &elem->out_num, 0);
+}
+
+static void *virtqueue_alloc_element(size_t sz, unsigned out_num, unsigned in_num)
+{
+    VirtQueueElement *elem;
+    size_t in_addr_ofs = QEMU_ALIGN_UP(sz, __alignof__(elem->in_addr[0]));
+    size_t out_addr_ofs = in_addr_ofs + in_num * sizeof(elem->in_addr[0]);
+    size_t out_addr_end = out_addr_ofs + out_num * sizeof(elem->out_addr[0]);
+    size_t in_sg_ofs = QEMU_ALIGN_UP(out_addr_end, __alignof__(elem->in_sg[0]));
+    size_t out_sg_ofs = in_sg_ofs + in_num * sizeof(elem->in_sg[0]);
+    size_t out_sg_end = out_sg_ofs + out_num * sizeof(elem->out_sg[0]);
+
+    assert(sz >= sizeof(VirtQueueElement));
+    elem = g_malloc(out_sg_end);
+    elem->out_num = out_num;
+    elem->in_num = in_num;
+    elem->in_addr = (void *)elem + in_addr_ofs;
+    elem->out_addr = (void *)elem + out_addr_ofs;
+    elem->in_sg = (void *)elem + in_sg_ofs;
+    elem->out_sg = (void *)elem + out_sg_ofs;
+    return elem;
+}
+
+void *virtqueue_pop(VirtQueue *vq, size_t sz)
+{
+    unsigned int i, head, max;
+    VRingMemoryRegionCaches *caches;
+    MemoryRegionCache indirect_desc_cache = MEMORY_REGION_CACHE_INVALID;
+    MemoryRegionCache *desc_cache;
+    int64_t len;
+    VirtIODevice *vdev = vq->vdev;
+    VirtQueueElement *elem = NULL;
+    unsigned out_num, in_num;
+    hwaddr addr[VIRTQUEUE_MAX_SIZE];
+    struct iovec iov[VIRTQUEUE_MAX_SIZE];
+    VRingDesc desc;
+    int rc;
+
+    if (unlikely(vdev->broken)) {
+        return NULL;
+    }
+    rcu_read_lock();
+    if (virtio_queue_empty_rcu(vq)) {
+        goto done;
+    }
+    /* Needed after virtio_queue_empty(), see comment in
+     * virtqueue_num_heads(). */
+    smp_rmb();
+
+    /* When we start there are none of either input nor output. */
+    out_num = in_num = 0;
+
+    max = vq->vring.num;
+
+    if (vq->inuse >= vq->vring.num) {
+        virtio_error(vdev, "Virtqueue size exceeded");
+        goto done;
+    }
+
+    if (!virtqueue_get_head(vq, vq->last_avail_idx++, &head)) {
+        goto done;
+    }
+
+    if (virtio_vdev_has_feature(vdev, VIRTIO_RING_F_EVENT_IDX)) {
+        vring_set_avail_event(vq, vq->last_avail_idx);
+    }
+
+    i = head;
+
+    caches = vring_get_region_caches(vq);
+    if (caches->desc.len < max * sizeof(VRingDesc)) {
+        virtio_error(vdev, "Cannot map descriptor ring");
+        goto done;
+    }
+
+    desc_cache = &caches->desc;
+    vring_desc_read(vdev, &desc, desc_cache, i);
+    if (desc.flags & VRING_DESC_F_INDIRECT) {
+        if (desc.len % sizeof(VRingDesc)) {
+            virtio_error(vdev, "Invalid size for indirect buffer table");
+            goto done;
+        }
+
+        /* loop over the indirect descriptor table */
+        len = address_space_cache_init(&indirect_desc_cache, vdev->dma_as,
+                                       desc.addr, desc.len, false);
+        desc_cache = &indirect_desc_cache;
+        if (len < desc.len) {
+            virtio_error(vdev, "Cannot map indirect buffer");
+            goto done;
+        }
+
+        max = desc.len / sizeof(VRingDesc);
+        i = 0;
+        vring_desc_read(vdev, &desc, desc_cache, i);
+    }
+
+    /* Collect all the descriptors */
+    do {
+        bool map_ok;
+
+        if (desc.flags & VRING_DESC_F_WRITE) {
+            map_ok = virtqueue_map_desc(vdev, &in_num, addr + out_num,
+                                        iov + out_num,
+                                        VIRTQUEUE_MAX_SIZE - out_num, true,
+                                        desc.addr, desc.len);
+        } else {
+            if (in_num) {
+                virtio_error(vdev, "Incorrect order for descriptors");
+                goto err_undo_map;
+            }
+            map_ok = virtqueue_map_desc(vdev, &out_num, addr, iov,
+                                        VIRTQUEUE_MAX_SIZE, false,
+                                        desc.addr, desc.len);
+        }
+        if (!map_ok) {
+            goto err_undo_map;
+        }
+
+        /* If we've got too many, that implies a descriptor loop. */
+        if ((in_num + out_num) > max) {
+            virtio_error(vdev, "Looped descriptor");
+            goto err_undo_map;
+        }
+
+        rc = virtqueue_read_next_desc(vdev, &desc, desc_cache, max, &i);
+    } while (rc == VIRTQUEUE_READ_DESC_MORE);
+
+    if (rc == VIRTQUEUE_READ_DESC_ERROR) {
+        goto err_undo_map;
+    }
+
+    /* Now copy what we have collected and mapped */
+    elem = virtqueue_alloc_element(sz, out_num, in_num);
+    elem->index = head;
+    for (i = 0; i < out_num; i++) {
+        elem->out_addr[i] = addr[i];
+        elem->out_sg[i] = iov[i];
+    }
+    for (i = 0; i < in_num; i++) {
+        elem->in_addr[i] = addr[out_num + i];
+        elem->in_sg[i] = iov[out_num + i];
+    }
+
+    vq->inuse++;
+
+    trace_virtqueue_pop(vq, elem, elem->in_num, elem->out_num);
+done:
+    address_space_cache_destroy(&indirect_desc_cache);
+    rcu_read_unlock();
+
+    return elem;
+
+err_undo_map:
+    virtqueue_undo_map_desc(out_num, in_num, iov);
+    goto done;
+}
+
+/* virtqueue_drop_all:
+ * @vq: The #VirtQueue
+ * Drops all queued buffers and indicates them to the guest
+ * as if they are done. Useful when buffers can not be
+ * processed but must be returned to the guest.
+ */
+unsigned int virtqueue_drop_all(VirtQueue *vq)
+{
+    unsigned int dropped = 0;
+    VirtQueueElement elem = {};
+    VirtIODevice *vdev = vq->vdev;
+    bool fEventIdx = virtio_vdev_has_feature(vdev, VIRTIO_RING_F_EVENT_IDX);
+
+    if (unlikely(vdev->broken)) {
+        return 0;
+    }
+
+    while (!virtio_queue_empty(vq) && vq->inuse < vq->vring.num) {
+        /* works similar to virtqueue_pop but does not map buffers
+        * and does not allocate any memory */
+        smp_rmb();
+        if (!virtqueue_get_head(vq, vq->last_avail_idx, &elem.index)) {
+            break;
+        }
+        vq->inuse++;
+        vq->last_avail_idx++;
+        if (fEventIdx) {
+            vring_set_avail_event(vq, vq->last_avail_idx);
+        }
+        /* immediately push the element, nothing to unmap
+         * as both in_num and out_num are set to 0 */
+        virtqueue_push(vq, &elem, 0);
+        dropped++;
+    }
+
+    return dropped;
+}
+
+/* Reading and writing a structure directly to QEMUFile is *awful*, but
+ * it is what QEMU has always done by mistake.  We can change it sooner
+ * or later by bumping the version number of the affected vm states.
+ * In the meanwhile, since the in-memory layout of VirtQueueElement
+ * has changed, we need to marshal to and from the layout that was
+ * used before the change.
+ */
+typedef struct VirtQueueElementOld {
+    unsigned int index;
+    unsigned int out_num;
+    unsigned int in_num;
+    hwaddr in_addr[VIRTQUEUE_MAX_SIZE];
+    hwaddr out_addr[VIRTQUEUE_MAX_SIZE];
+    struct iovec in_sg[VIRTQUEUE_MAX_SIZE];
+    struct iovec out_sg[VIRTQUEUE_MAX_SIZE];
+} VirtQueueElementOld;
+
+void *qemu_get_virtqueue_element(VirtIODevice *vdev, QEMUFile *f, size_t sz)
+{
+    VirtQueueElement *elem;
+    VirtQueueElementOld data;
+    int i;
+
+    qemu_get_buffer(f, (uint8_t *)&data, sizeof(VirtQueueElementOld));
+
+    /* TODO: teach all callers that this can fail, and return failure instead
+     * of asserting here.
+     * When we do, we might be able to re-enable NDEBUG below.
+     */
+#ifdef NDEBUG
+#error building with NDEBUG is not supported
+#endif
+    assert(ARRAY_SIZE(data.in_addr) >= data.in_num);
+    assert(ARRAY_SIZE(data.out_addr) >= data.out_num);
+
+    elem = virtqueue_alloc_element(sz, data.out_num, data.in_num);
+    elem->index = data.index;
+
+    for (i = 0; i < elem->in_num; i++) {
+        elem->in_addr[i] = data.in_addr[i];
+    }
+
+    for (i = 0; i < elem->out_num; i++) {
+        elem->out_addr[i] = data.out_addr[i];
+    }
+
+    for (i = 0; i < elem->in_num; i++) {
+        /* Base is overwritten by virtqueue_map.  */
+        elem->in_sg[i].iov_base = 0;
+        elem->in_sg[i].iov_len = data.in_sg[i].iov_len;
+    }
+
+    for (i = 0; i < elem->out_num; i++) {
+        /* Base is overwritten by virtqueue_map.  */
+        elem->out_sg[i].iov_base = 0;
+        elem->out_sg[i].iov_len = data.out_sg[i].iov_len;
+    }
+
+    virtqueue_map(vdev, elem);
+    return elem;
+}
+
+void qemu_put_virtqueue_element(QEMUFile *f, VirtQueueElement *elem)
+{
+    VirtQueueElementOld data;
+    int i;
+
+    memset(&data, 0, sizeof(data));
+    data.index = elem->index;
+    data.in_num = elem->in_num;
+    data.out_num = elem->out_num;
+
+    for (i = 0; i < elem->in_num; i++) {
+        data.in_addr[i] = elem->in_addr[i];
+    }
+
+    for (i = 0; i < elem->out_num; i++) {
+        data.out_addr[i] = elem->out_addr[i];
+    }
+
+    for (i = 0; i < elem->in_num; i++) {
+        /* Base is overwritten by virtqueue_map when loading.  Do not
+         * save it, as it would leak the QEMU address space layout.  */
+        data.in_sg[i].iov_len = elem->in_sg[i].iov_len;
+    }
+
+    for (i = 0; i < elem->out_num; i++) {
+        /* Do not save iov_base as above.  */
+        data.out_sg[i].iov_len = elem->out_sg[i].iov_len;
+    }
+    qemu_put_buffer(f, (uint8_t *)&data, sizeof(VirtQueueElementOld));
+}
+
+/* virtio device */
+static void virtio_notify_vector(VirtIODevice *vdev, uint16_t vector)
+{
+    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));
+    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);
+
+    if (unlikely(vdev->broken)) {
+        return;
+    }
+
+    if (k->notify) {
+        k->notify(qbus->parent, vector);
+    }
+}
+
+void virtio_update_irq(VirtIODevice *vdev)
+{
+    virtio_notify_vector(vdev, VIRTIO_NO_VECTOR);
+}
+
+static int virtio_validate_features(VirtIODevice *vdev)
+{
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+
+    if (virtio_host_has_feature(vdev, VIRTIO_F_IOMMU_PLATFORM) &&
+        !virtio_vdev_has_feature(vdev, VIRTIO_F_IOMMU_PLATFORM)) {
+        return -EFAULT;
+    }
+
+    if (k->validate_features) {
+        return k->validate_features(vdev);
+    } else {
+        return 0;
+    }
+}
+
+int virtio_set_status(VirtIODevice *vdev, uint8_t val)
+{
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    trace_virtio_set_status(vdev, val);
+
+    if (virtio_vdev_has_feature(vdev, VIRTIO_F_VERSION_1)) {
+        if (!(vdev->status & VIRTIO_CONFIG_S_FEATURES_OK) &&
+            val & VIRTIO_CONFIG_S_FEATURES_OK) {
+            int ret = virtio_validate_features(vdev);
+
+            if (ret) {
+                return ret;
+            }
+        }
+    }
+    if (k->set_status) {
+        k->set_status(vdev, val);
+    }
+    vdev->status = val;
+    return 0;
+}
+
+bool target_words_bigendian(void);
+static enum virtio_device_endian virtio_default_endian(void)
+{
+    if (target_words_bigendian()) {
+        return VIRTIO_DEVICE_ENDIAN_BIG;
+    } else {
+        return VIRTIO_DEVICE_ENDIAN_LITTLE;
+    }
+}
+
+static enum virtio_device_endian virtio_current_cpu_endian(void)
+{
+    CPUClass *cc = CPU_GET_CLASS(current_cpu);
+
+    if (cc->virtio_is_big_endian(current_cpu)) {
+        return VIRTIO_DEVICE_ENDIAN_BIG;
+    } else {
+        return VIRTIO_DEVICE_ENDIAN_LITTLE;
+    }
+}
+
+static void virtio_virtqueue_reset_region_cache(struct VirtQueue *vq)
+{
+    VRingMemoryRegionCaches *caches;
+
+    caches = atomic_read(&vq->vring.caches);
+    atomic_rcu_set(&vq->vring.caches, NULL);
+    if (caches) {
+        call_rcu(caches, virtio_free_region_cache, rcu);
+    }
+}
+
+void virtio_reset(void *opaque)
+{
+    VirtIODevice *vdev = opaque;
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    int i;
+
+    virtio_set_status(vdev, 0);
+    if (current_cpu) {
+        /* Guest initiated reset */
+        vdev->device_endian = virtio_current_cpu_endian();
+    } else {
+        /* System reset */
+        vdev->device_endian = virtio_default_endian();
+    }
+
+    if (k->reset) {
+        k->reset(vdev);
+    }
+
+    vdev->broken = false;
+    vdev->guest_features = 0;
+    vdev->queue_sel = 0;
+    vdev->status = 0;
+    atomic_set(&vdev->isr, 0);
+    vdev->config_vector = VIRTIO_NO_VECTOR;
+    virtio_notify_vector(vdev, vdev->config_vector);
+
+    for(i = 0; i < VIRTIO_QUEUE_MAX; i++) {
+        vdev->vq[i].vring.desc = 0;
+        vdev->vq[i].vring.avail = 0;
+        vdev->vq[i].vring.used = 0;
+        vdev->vq[i].last_avail_idx = 0;
+        vdev->vq[i].shadow_avail_idx = 0;
+        vdev->vq[i].used_idx = 0;
+        virtio_queue_set_vector(vdev, i, VIRTIO_NO_VECTOR);
+        vdev->vq[i].signalled_used = 0;
+        vdev->vq[i].signalled_used_valid = false;
+        vdev->vq[i].notification = true;
+        vdev->vq[i].vring.num = vdev->vq[i].vring.num_default;
+        vdev->vq[i].inuse = 0;
+        virtio_virtqueue_reset_region_cache(&vdev->vq[i]);
+    }
+}
+
+uint32_t virtio_config_readb(VirtIODevice *vdev, uint32_t addr)
+{
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    uint8_t val;
+
+    if (addr + sizeof(val) > vdev->config_len) {
+        return (uint32_t)-1;
+    }
+
+    k->get_config(vdev, vdev->config);
+
+    val = ldub_p(vdev->config + addr);
+    return val;
+}
+
+uint32_t virtio_config_readw(VirtIODevice *vdev, uint32_t addr)
+{
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    uint16_t val;
+
+    if (addr + sizeof(val) > vdev->config_len) {
+        return (uint32_t)-1;
+    }
+
+    k->get_config(vdev, vdev->config);
+
+    val = lduw_p(vdev->config + addr);
+    return val;
+}
+
+uint32_t virtio_config_readl(VirtIODevice *vdev, uint32_t addr)
+{
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    uint32_t val;
+
+    if (addr + sizeof(val) > vdev->config_len) {
+        return (uint32_t)-1;
+    }
+
+    k->get_config(vdev, vdev->config);
+
+    val = ldl_p(vdev->config + addr);
+    return val;
+}
+
+void virtio_config_writeb(VirtIODevice *vdev, uint32_t addr, uint32_t data)
+{
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    uint8_t val = data;
+
+    if (addr + sizeof(val) > vdev->config_len) {
+        return;
+    }
+
+    stb_p(vdev->config + addr, val);
+
+    if (k->set_config) {
+        k->set_config(vdev, vdev->config);
+    }
+}
+
+void virtio_config_writew(VirtIODevice *vdev, uint32_t addr, uint32_t data)
+{
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    uint16_t val = data;
+
+    if (addr + sizeof(val) > vdev->config_len) {
+        return;
+    }
+
+    stw_p(vdev->config + addr, val);
+
+    if (k->set_config) {
+        k->set_config(vdev, vdev->config);
+    }
+}
+
+void virtio_config_writel(VirtIODevice *vdev, uint32_t addr, uint32_t data)
+{
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    uint32_t val = data;
+
+    if (addr + sizeof(val) > vdev->config_len) {
+        return;
+    }
+
+    stl_p(vdev->config + addr, val);
+
+    if (k->set_config) {
+        k->set_config(vdev, vdev->config);
+    }
+}
+
+uint32_t virtio_config_modern_readb(VirtIODevice *vdev, uint32_t addr)
+{
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    uint8_t val;
+
+    if (addr + sizeof(val) > vdev->config_len) {
+        return (uint32_t)-1;
+    }
+
+    k->get_config(vdev, vdev->config);
+
+    val = ldub_p(vdev->config + addr);
+    return val;
+}
+
+uint32_t virtio_config_modern_readw(VirtIODevice *vdev, uint32_t addr)
+{
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    uint16_t val;
+
+    if (addr + sizeof(val) > vdev->config_len) {
+        return (uint32_t)-1;
+    }
+
+    k->get_config(vdev, vdev->config);
+
+    val = lduw_le_p(vdev->config + addr);
+    return val;
+}
+
+uint32_t virtio_config_modern_readl(VirtIODevice *vdev, uint32_t addr)
+{
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    uint32_t val;
+
+    if (addr + sizeof(val) > vdev->config_len) {
+        return (uint32_t)-1;
+    }
+
+    k->get_config(vdev, vdev->config);
+
+    val = ldl_le_p(vdev->config + addr);
+    return val;
+}
+
+void virtio_config_modern_writeb(VirtIODevice *vdev,
+                                 uint32_t addr, uint32_t data)
+{
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    uint8_t val = data;
+
+    if (addr + sizeof(val) > vdev->config_len) {
+        return;
+    }
+
+    stb_p(vdev->config + addr, val);
+
+    if (k->set_config) {
+        k->set_config(vdev, vdev->config);
+    }
+}
+
+void virtio_config_modern_writew(VirtIODevice *vdev,
+                                 uint32_t addr, uint32_t data)
+{
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    uint16_t val = data;
+
+    if (addr + sizeof(val) > vdev->config_len) {
+        return;
+    }
+
+    stw_le_p(vdev->config + addr, val);
+
+    if (k->set_config) {
+        k->set_config(vdev, vdev->config);
+    }
+}
+
+void virtio_config_modern_writel(VirtIODevice *vdev,
+                                 uint32_t addr, uint32_t data)
+{
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    uint32_t val = data;
+
+    if (addr + sizeof(val) > vdev->config_len) {
+        return;
+    }
+
+    stl_le_p(vdev->config + addr, val);
+
+    if (k->set_config) {
+        k->set_config(vdev, vdev->config);
+    }
+}
+
+void virtio_queue_set_addr(VirtIODevice *vdev, int n, hwaddr addr)
+{
+    vdev->vq[n].vring.desc = addr;
+    virtio_queue_update_rings(vdev, n);
+}
+
+hwaddr virtio_queue_get_addr(VirtIODevice *vdev, int n)
+{
+    return vdev->vq[n].vring.desc;
+}
+
+void virtio_queue_set_rings(VirtIODevice *vdev, int n, hwaddr desc,
+                            hwaddr avail, hwaddr used)
+{
+    vdev->vq[n].vring.desc = desc;
+    vdev->vq[n].vring.avail = avail;
+    vdev->vq[n].vring.used = used;
+    virtio_init_region_cache(vdev, n);
+}
+
+void virtio_queue_set_num(VirtIODevice *vdev, int n, int num)
+{
+    /* Don't allow guest to flip queue between existent and
+     * nonexistent states, or to set it to an invalid size.
+     */
+    if (!!num != !!vdev->vq[n].vring.num ||
+        num > VIRTQUEUE_MAX_SIZE ||
+        num < 0) {
+        return;
+    }
+    vdev->vq[n].vring.num = num;
+}
+
+VirtQueue *virtio_vector_first_queue(VirtIODevice *vdev, uint16_t vector)
+{
+    return QLIST_FIRST(&vdev->vector_queues[vector]);
+}
+
+VirtQueue *virtio_vector_next_queue(VirtQueue *vq)
+{
+    return QLIST_NEXT(vq, node);
+}
+
+int virtio_queue_get_num(VirtIODevice *vdev, int n)
+{
+    return vdev->vq[n].vring.num;
+}
+
+int virtio_queue_get_max_num(VirtIODevice *vdev, int n)
+{
+    return vdev->vq[n].vring.num_default;
+}
+
+int virtio_get_num_queues(VirtIODevice *vdev)
+{
+    int i;
+
+    for (i = 0; i < VIRTIO_QUEUE_MAX; i++) {
+        if (!virtio_queue_get_num(vdev, i)) {
+            break;
+        }
+    }
+
+    return i;
+}
+
+void virtio_queue_set_align(VirtIODevice *vdev, int n, int align)
+{
+    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));
+    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);
+
+    /* virtio-1 compliant devices cannot change the alignment */
+    if (virtio_vdev_has_feature(vdev, VIRTIO_F_VERSION_1)) {
+        error_report("tried to modify queue alignment for virtio-1 device");
+        return;
+    }
+    /* Check that the transport told us it was going to do this
+     * (so a buggy transport will immediately assert rather than
+     * silently failing to migrate this state)
+     */
+    assert(k->has_variable_vring_alignment);
+
+    vdev->vq[n].vring.align = align;
+    virtio_queue_update_rings(vdev, n);
+}
+
+static bool virtio_queue_notify_aio_vq(VirtQueue *vq)
+{
+    if (vq->vring.desc && vq->handle_aio_output) {
+        VirtIODevice *vdev = vq->vdev;
+
+        trace_virtio_queue_notify(vdev, vq - vdev->vq, vq);
+        return vq->handle_aio_output(vdev, vq);
+    }
+
+    return false;
+}
+
+static void virtio_queue_notify_vq(VirtQueue *vq)
+{
+    if (vq->vring.desc && vq->handle_output) {
+        VirtIODevice *vdev = vq->vdev;
+
+        if (unlikely(vdev->broken)) {
+            return;
+        }
+
+        trace_virtio_queue_notify(vdev, vq - vdev->vq, vq);
+        vq->handle_output(vdev, vq);
+    }
+}
+
+void virtio_queue_notify(VirtIODevice *vdev, int n)
+{
+    VirtQueue *vq = &vdev->vq[n];
+
+    if (unlikely(!vq->vring.desc || vdev->broken)) {
+        return;
+    }
+
+    trace_virtio_queue_notify(vdev, vq - vdev->vq, vq);
+    if (vq->handle_aio_output) {
+        event_notifier_set(&vq->host_notifier);
+    } else if (vq->handle_output) {
+        vq->handle_output(vdev, vq);
+    }
+}
+
+uint16_t virtio_queue_vector(VirtIODevice *vdev, int n)
+{
+    return n < VIRTIO_QUEUE_MAX ? vdev->vq[n].vector :
+        VIRTIO_NO_VECTOR;
+}
+
+void virtio_queue_set_vector(VirtIODevice *vdev, int n, uint16_t vector)
+{
+    VirtQueue *vq = &vdev->vq[n];
+
+    if (n < VIRTIO_QUEUE_MAX) {
+        if (vdev->vector_queues &&
+            vdev->vq[n].vector != VIRTIO_NO_VECTOR) {
+            QLIST_REMOVE(vq, node);
+        }
+        vdev->vq[n].vector = vector;
+        if (vdev->vector_queues &&
+            vector != VIRTIO_NO_VECTOR) {
+            QLIST_INSERT_HEAD(&vdev->vector_queues[vector], vq, node);
+        }
+    }
+}
+
+VirtQueue *virtio_add_queue(VirtIODevice *vdev, int queue_size,
+                            VirtIOHandleOutput handle_output)
+{
+    int i;
+
+    for (i = 0; i < VIRTIO_QUEUE_MAX; i++) {
+        if (vdev->vq[i].vring.num == 0)
+            break;
+    }
+
+    if (i == VIRTIO_QUEUE_MAX || queue_size > VIRTQUEUE_MAX_SIZE)
+        abort();
+
+    vdev->vq[i].vring.num = queue_size;
+    vdev->vq[i].vring.num_default = queue_size;
+    vdev->vq[i].vring.align = VIRTIO_PCI_VRING_ALIGN;
+    vdev->vq[i].handle_output = handle_output;
+    vdev->vq[i].handle_aio_output = NULL;
+
+    return &vdev->vq[i];
+}
+
+void virtio_del_queue(VirtIODevice *vdev, int n)
+{
+    if (n < 0 || n >= VIRTIO_QUEUE_MAX) {
+        abort();
+    }
+
+    vdev->vq[n].vring.num = 0;
+    vdev->vq[n].vring.num_default = 0;
+}
+
+static void virtio_set_isr(VirtIODevice *vdev, int value)
+{
+    uint8_t old = atomic_read(&vdev->isr);
+
+    /* Do not write ISR if it does not change, so that its cacheline remains
+     * shared in the common case where the guest does not read it.
+     */
+    if ((old & value) != value) {
+        atomic_or(&vdev->isr, value);
+    }
+}
+
+/* Called within rcu_read_lock().  */
+static bool virtio_should_notify(VirtIODevice *vdev, VirtQueue *vq)
+{
+    uint16_t old, new;
+    bool v;
+    /* We need to expose used array entries before checking used event. */
+    smp_mb();
+    /* Always notify when queue is empty (when feature acknowledge) */
+    if (virtio_vdev_has_feature(vdev, VIRTIO_F_NOTIFY_ON_EMPTY) &&
+        !vq->inuse && virtio_queue_empty(vq)) {
+        return true;
+    }
+
+    if (!virtio_vdev_has_feature(vdev, VIRTIO_RING_F_EVENT_IDX)) {
+        return !(vring_avail_flags(vq) & VRING_AVAIL_F_NO_INTERRUPT);
+    }
+
+    v = vq->signalled_used_valid;
+    vq->signalled_used_valid = true;
+    old = vq->signalled_used;
+    new = vq->signalled_used = vq->used_idx;
+    return !v || vring_need_event(vring_get_used_event(vq), new, old);
+}
+
+void virtio_notify_irqfd(VirtIODevice *vdev, VirtQueue *vq)
+{
+    bool should_notify;
+    rcu_read_lock();
+    should_notify = virtio_should_notify(vdev, vq);
+    rcu_read_unlock();
+
+    if (!should_notify) {
+        return;
+    }
+
+    trace_virtio_notify_irqfd(vdev, vq);
+
+    /*
+     * virtio spec 1.0 says ISR bit 0 should be ignored with MSI, but
+     * windows drivers included in virtio-win 1.8.0 (circa 2015) are
+     * incorrectly polling this bit during crashdump and hibernation
+     * in MSI mode, causing a hang if this bit is never updated.
+     * Recent releases of Windows do not really shut down, but rather
+     * log out and hibernate to make the next startup faster.  Hence,
+     * this manifested as a more serious hang during shutdown with
+     *
+     * Next driver release from 2016 fixed this problem, so working around it
+     * is not a must, but it's easy to do so let's do it here.
+     *
+     * Note: it's safe to update ISR from any thread as it was switched
+     * to an atomic operation.
+     */
+    virtio_set_isr(vq->vdev, 0x1);
+    event_notifier_set(&vq->guest_notifier);
+}
+
+static void virtio_irq(VirtQueue *vq)
+{
+    virtio_set_isr(vq->vdev, 0x1);
+    virtio_notify_vector(vq->vdev, vq->vector);
+}
+
+void virtio_notify(VirtIODevice *vdev, VirtQueue *vq)
+{
+    bool should_notify;
+    rcu_read_lock();
+    should_notify = virtio_should_notify(vdev, vq);
+    rcu_read_unlock();
+
+    if (!should_notify) {
+        return;
+    }
+
+    trace_virtio_notify(vdev, vq);
+    virtio_irq(vq);
+}
+
+void virtio_notify_config(VirtIODevice *vdev)
+{
+    if (!(vdev->status & VIRTIO_CONFIG_S_DRIVER_OK))
+        return;
+
+    virtio_set_isr(vdev, 0x3);
+    vdev->generation++;
+    virtio_notify_vector(vdev, vdev->config_vector);
+}
+
+static bool virtio_device_endian_needed(void *opaque)
+{
+    VirtIODevice *vdev = opaque;
+
+    assert(vdev->device_endian != VIRTIO_DEVICE_ENDIAN_UNKNOWN);
+    if (!virtio_vdev_has_feature(vdev, VIRTIO_F_VERSION_1)) {
+        return vdev->device_endian != virtio_default_endian();
+    }
+    /* Devices conforming to VIRTIO 1.0 or later are always LE. */
+    return vdev->device_endian != VIRTIO_DEVICE_ENDIAN_LITTLE;
+}
+
+static bool virtio_64bit_features_needed(void *opaque)
+{
+    VirtIODevice *vdev = opaque;
+
+    return (vdev->host_features >> 32) != 0;
+}
+
+static bool virtio_virtqueue_needed(void *opaque)
+{
+    VirtIODevice *vdev = opaque;
+
+    return virtio_host_has_feature(vdev, VIRTIO_F_VERSION_1);
+}
+
+static bool virtio_ringsize_needed(void *opaque)
+{
+    VirtIODevice *vdev = opaque;
+    int i;
+
+    for (i = 0; i < VIRTIO_QUEUE_MAX; i++) {
+        if (vdev->vq[i].vring.num != vdev->vq[i].vring.num_default) {
+            return true;
+        }
+    }
+    return false;
+}
+
+static bool virtio_extra_state_needed(void *opaque)
+{
+    VirtIODevice *vdev = opaque;
+    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));
+    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);
+
+    return k->has_extra_state &&
+        k->has_extra_state(qbus->parent);
+}
+
+static bool virtio_broken_needed(void *opaque)
+{
+    VirtIODevice *vdev = opaque;
+
+    return vdev->broken;
+}
+
+static const VMStateDescription vmstate_virtqueue = {
+    .name = "virtqueue_state",
+    .version_id = 1,
+    .minimum_version_id = 1,
+    .fields = (VMStateField[]) {
+        VMSTATE_UINT64(vring.avail, struct VirtQueue),
+        VMSTATE_UINT64(vring.used, struct VirtQueue),
+        VMSTATE_END_OF_LIST()
+    }
+};
+
+static const VMStateDescription vmstate_virtio_virtqueues = {
+    .name = "virtio/virtqueues",
+    .version_id = 1,
+    .minimum_version_id = 1,
+    .needed = &virtio_virtqueue_needed,
+    .fields = (VMStateField[]) {
+        VMSTATE_STRUCT_VARRAY_POINTER_KNOWN(vq, struct VirtIODevice,
+                      VIRTIO_QUEUE_MAX, 0, vmstate_virtqueue, VirtQueue),
+        VMSTATE_END_OF_LIST()
+    }
+};
+
+static const VMStateDescription vmstate_ringsize = {
+    .name = "ringsize_state",
+    .version_id = 1,
+    .minimum_version_id = 1,
+    .fields = (VMStateField[]) {
+        VMSTATE_UINT32(vring.num_default, struct VirtQueue),
+        VMSTATE_END_OF_LIST()
+    }
+};
+
+static const VMStateDescription vmstate_virtio_ringsize = {
+    .name = "virtio/ringsize",
+    .version_id = 1,
+    .minimum_version_id = 1,
+    .needed = &virtio_ringsize_needed,
+    .fields = (VMStateField[]) {
+        VMSTATE_STRUCT_VARRAY_POINTER_KNOWN(vq, struct VirtIODevice,
+                      VIRTIO_QUEUE_MAX, 0, vmstate_ringsize, VirtQueue),
+        VMSTATE_END_OF_LIST()
+    }
+};
+
+static int get_extra_state(QEMUFile *f, void *pv, size_t size,
+                           VMStateField *field)
+{
+    VirtIODevice *vdev = pv;
+    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));
+    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);
+
+    if (!k->load_extra_state) {
+        return -1;
+    } else {
+        return k->load_extra_state(qbus->parent, f);
+    }
+}
+
+static int put_extra_state(QEMUFile *f, void *pv, size_t size,
+                           VMStateField *field, QJSON *vmdesc)
+{
+    VirtIODevice *vdev = pv;
+    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));
+    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);
+
+    k->save_extra_state(qbus->parent, f);
+    return 0;
+}
+
+static const VMStateInfo vmstate_info_extra_state = {
+    .name = "virtqueue_extra_state",
+    .get = get_extra_state,
+    .put = put_extra_state,
+};
+
+static const VMStateDescription vmstate_virtio_extra_state = {
+    .name = "virtio/extra_state",
+    .version_id = 1,
+    .minimum_version_id = 1,
+    .needed = &virtio_extra_state_needed,
+    .fields = (VMStateField[]) {
+        {
+            .name         = "extra_state",
+            .version_id   = 0,
+            .field_exists = NULL,
+            .size         = 0,
+            .info         = &vmstate_info_extra_state,
+            .flags        = VMS_SINGLE,
+            .offset       = 0,
+        },
+        VMSTATE_END_OF_LIST()
+    }
+};
+
+static const VMStateDescription vmstate_virtio_device_endian = {
+    .name = "virtio/device_endian",
+    .version_id = 1,
+    .minimum_version_id = 1,
+    .needed = &virtio_device_endian_needed,
+    .fields = (VMStateField[]) {
+        VMSTATE_UINT8(device_endian, VirtIODevice),
+        VMSTATE_END_OF_LIST()
+    }
+};
+
+static const VMStateDescription vmstate_virtio_64bit_features = {
+    .name = "virtio/64bit_features",
+    .version_id = 1,
+    .minimum_version_id = 1,
+    .needed = &virtio_64bit_features_needed,
+    .fields = (VMStateField[]) {
+        VMSTATE_UINT64(guest_features, VirtIODevice),
+        VMSTATE_END_OF_LIST()
+    }
+};
+
+static const VMStateDescription vmstate_virtio_broken = {
+    .name = "virtio/broken",
+    .version_id = 1,
+    .minimum_version_id = 1,
+    .needed = &virtio_broken_needed,
+    .fields = (VMStateField[]) {
+        VMSTATE_BOOL(broken, VirtIODevice),
+        VMSTATE_END_OF_LIST()
+    }
+};
+
+static const VMStateDescription vmstate_virtio = {
+    .name = "virtio",
+    .version_id = 1,
+    .minimum_version_id = 1,
+    .minimum_version_id_old = 1,
+    .fields = (VMStateField[]) {
+        VMSTATE_END_OF_LIST()
+    },
+    .subsections = (const VMStateDescription*[]) {
+        &vmstate_virtio_device_endian,
+        &vmstate_virtio_64bit_features,
+        &vmstate_virtio_virtqueues,
+        &vmstate_virtio_ringsize,
+        &vmstate_virtio_broken,
+        &vmstate_virtio_extra_state,
+        NULL
+    }
+};
+
+void virtio_save(VirtIODevice *vdev, QEMUFile *f)
+{
+    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));
+    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);
+    VirtioDeviceClass *vdc = VIRTIO_DEVICE_GET_CLASS(vdev);
+    uint32_t guest_features_lo = (vdev->guest_features & 0xffffffff);
+    int i;
+
+    if (k->save_config) {
+        k->save_config(qbus->parent, f);
+    }
+
+    qemu_put_8s(f, &vdev->status);
+    qemu_put_8s(f, &vdev->isr);
+    qemu_put_be16s(f, &vdev->queue_sel);
+    qemu_put_be32s(f, &guest_features_lo);
+    qemu_put_be32(f, vdev->config_len);
+    qemu_put_buffer(f, vdev->config, vdev->config_len);
+
+    for (i = 0; i < VIRTIO_QUEUE_MAX; i++) {
+        if (vdev->vq[i].vring.num == 0)
+            break;
+    }
+
+    qemu_put_be32(f, i);
+
+    for (i = 0; i < VIRTIO_QUEUE_MAX; i++) {
+        if (vdev->vq[i].vring.num == 0)
+            break;
+
+        qemu_put_be32(f, vdev->vq[i].vring.num);
+        if (k->has_variable_vring_alignment) {
+            qemu_put_be32(f, vdev->vq[i].vring.align);
+        }
+        /*
+         * Save desc now, the rest of the ring addresses are saved in
+         * subsections for VIRTIO-1 devices.
+         */
+        qemu_put_be64(f, vdev->vq[i].vring.desc);
+        qemu_put_be16s(f, &vdev->vq[i].last_avail_idx);
+        if (k->save_queue) {
+            k->save_queue(qbus->parent, i, f);
+        }
+    }
+
+    if (vdc->save != NULL) {
+        vdc->save(vdev, f);
+    }
+
+    if (vdc->vmsd) {
+        vmstate_save_state(f, vdc->vmsd, vdev, NULL);
+    }
+
+    /* Subsections */
+    vmstate_save_state(f, &vmstate_virtio, vdev, NULL);
+}
+
+/* A wrapper for use as a VMState .put function */
+static int virtio_device_put(QEMUFile *f, void *opaque, size_t size,
+                              VMStateField *field, QJSON *vmdesc)
+{
+    virtio_save(VIRTIO_DEVICE(opaque), f);
+
+    return 0;
+}
+
+/* A wrapper for use as a VMState .get function */
+static int virtio_device_get(QEMUFile *f, void *opaque, size_t size,
+                             VMStateField *field)
+{
+    VirtIODevice *vdev = VIRTIO_DEVICE(opaque);
+    DeviceClass *dc = DEVICE_CLASS(VIRTIO_DEVICE_GET_CLASS(vdev));
+
+    return virtio_load(vdev, f, dc->vmsd->version_id);
+}
+
+const VMStateInfo  virtio_vmstate_info = {
+    .name = "virtio",
+    .get = virtio_device_get,
+    .put = virtio_device_put,
+};
+
+static int virtio_set_features_nocheck(VirtIODevice *vdev, uint64_t val)
+{
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    bool bad = (val & ~(vdev->host_features)) != 0;
+
+    val &= vdev->host_features;
+    if (k->set_features) {
+        k->set_features(vdev, val);
+    }
+    vdev->guest_features = val;
+    return bad ? -1 : 0;
+}
+
+int virtio_set_features(VirtIODevice *vdev, uint64_t val)
+{
+   /*
+     * The driver must not attempt to set features after feature negotiation
+     * has finished.
+     */
+    if (vdev->status & VIRTIO_CONFIG_S_FEATURES_OK) {
+        return -EINVAL;
+    }
+    return virtio_set_features_nocheck(vdev, val);
+}
+
+int virtio_load(VirtIODevice *vdev, QEMUFile *f, int version_id)
+{
+    int i, ret;
+    int32_t config_len;
+    uint32_t num;
+    uint32_t features;
+    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));
+    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);
+    VirtioDeviceClass *vdc = VIRTIO_DEVICE_GET_CLASS(vdev);
+
+    /*
+     * We poison the endianness to ensure it does not get used before
+     * subsections have been loaded.
+     */
+    vdev->device_endian = VIRTIO_DEVICE_ENDIAN_UNKNOWN;
+
+    if (k->load_config) {
+        ret = k->load_config(qbus->parent, f);
+        if (ret)
+            return ret;
+    }
+
+    qemu_get_8s(f, &vdev->status);
+    qemu_get_8s(f, &vdev->isr);
+    qemu_get_be16s(f, &vdev->queue_sel);
+    if (vdev->queue_sel >= VIRTIO_QUEUE_MAX) {
+        return -1;
+    }
+    qemu_get_be32s(f, &features);
+
+    /*
+     * Temporarily set guest_features low bits - needed by
+     * virtio net load code testing for VIRTIO_NET_F_CTRL_GUEST_OFFLOADS
+     * VIRTIO_NET_F_GUEST_ANNOUNCE and VIRTIO_NET_F_CTRL_VQ.
+     *
+     * Note: devices should always test host features in future - don't create
+     * new dependencies like this.
+     */
+    vdev->guest_features = features;
+
+    config_len = qemu_get_be32(f);
+
+    /*
+     * There are cases where the incoming config can be bigger or smaller
+     * than what we have; so load what we have space for, and skip
+     * any excess that's in the stream.
+     */
+    qemu_get_buffer(f, vdev->config, MIN(config_len, vdev->config_len));
+
+    while (config_len > vdev->config_len) {
+        qemu_get_byte(f);
+        config_len--;
+    }
+
+    num = qemu_get_be32(f);
+
+    if (num > VIRTIO_QUEUE_MAX) {
+        error_report("Invalid number of virtqueues: 0x%x", num);
+        return -1;
+    }
+
+    for (i = 0; i < num; i++) {
+        vdev->vq[i].vring.num = qemu_get_be32(f);
+        if (k->has_variable_vring_alignment) {
+            vdev->vq[i].vring.align = qemu_get_be32(f);
+        }
+        vdev->vq[i].vring.desc = qemu_get_be64(f);
+        qemu_get_be16s(f, &vdev->vq[i].last_avail_idx);
+        vdev->vq[i].signalled_used_valid = false;
+        vdev->vq[i].notification = true;
+
+        if (!vdev->vq[i].vring.desc && vdev->vq[i].last_avail_idx) {
+            error_report("VQ %d address 0x0 "
+                         "inconsistent with Host index 0x%x",
+                         i, vdev->vq[i].last_avail_idx);
+            return -1;
+        }
+        if (k->load_queue) {
+            ret = k->load_queue(qbus->parent, i, f);
+            if (ret)
+                return ret;
+        }
+    }
+
+    virtio_notify_vector(vdev, VIRTIO_NO_VECTOR);
+
+    if (vdc->load != NULL) {
+        ret = vdc->load(vdev, f, version_id);
+        if (ret) {
+            return ret;
+        }
+    }
+
+    if (vdc->vmsd) {
+        ret = vmstate_load_state(f, vdc->vmsd, vdev, version_id);
+        if (ret) {
+            return ret;
+        }
+    }
+
+    /* Subsections */
+    ret = vmstate_load_state(f, &vmstate_virtio, vdev, 1);
+    if (ret) {
+        return ret;
+    }
+
+    if (vdev->device_endian == VIRTIO_DEVICE_ENDIAN_UNKNOWN) {
+        vdev->device_endian = virtio_default_endian();
+    }
+
+    if (virtio_64bit_features_needed(vdev)) {
+        /*
+         * Subsection load filled vdev->guest_features.  Run them
+         * through virtio_set_features to sanity-check them against
+         * host_features.
+         */
+        uint64_t features64 = vdev->guest_features;
+        if (virtio_set_features_nocheck(vdev, features64) < 0) {
+            error_report("Features 0x%" PRIx64 " unsupported. "
+                         "Allowed features: 0x%" PRIx64,
+                         features64, vdev->host_features);
+            return -1;
+        }
+    } else {
+        if (virtio_set_features_nocheck(vdev, features) < 0) {
+            error_report("Features 0x%x unsupported. "
+                         "Allowed features: 0x%" PRIx64,
+                         features, vdev->host_features);
+            return -1;
+        }
+    }
+
+    rcu_read_lock();
+    for (i = 0; i < num; i++) {
+        if (vdev->vq[i].vring.desc) {
+            uint16_t nheads;
+
+            /*
+             * VIRTIO-1 devices migrate desc, used, and avail ring addresses so
+             * only the region cache needs to be set up.  Legacy devices need
+             * to calculate used and avail ring addresses based on the desc
+             * address.
+             */
+            if (virtio_vdev_has_feature(vdev, VIRTIO_F_VERSION_1)) {
+                virtio_init_region_cache(vdev, i);
+            } else {
+                virtio_queue_update_rings(vdev, i);
+            }
+
+            nheads = vring_avail_idx(&vdev->vq[i]) - vdev->vq[i].last_avail_idx;
+            /* Check it isn't doing strange things with descriptor numbers. */
+            if (nheads > vdev->vq[i].vring.num) {
+                error_report("VQ %d size 0x%x Guest index 0x%x "
+                             "inconsistent with Host index 0x%x: delta 0x%x",
+                             i, vdev->vq[i].vring.num,
+                             vring_avail_idx(&vdev->vq[i]),
+                             vdev->vq[i].last_avail_idx, nheads);
+                return -1;
+            }
+            vdev->vq[i].used_idx = vring_used_idx(&vdev->vq[i]);
+            vdev->vq[i].shadow_avail_idx = vring_avail_idx(&vdev->vq[i]);
+
+            /*
+             * Some devices migrate VirtQueueElements that have been popped
+             * from the avail ring but not yet returned to the used ring.
+             * Since max ring size < UINT16_MAX it's safe to use modulo
+             * UINT16_MAX + 1 subtraction.
+             */
+            vdev->vq[i].inuse = (uint16_t)(vdev->vq[i].last_avail_idx -
+                                vdev->vq[i].used_idx);
+            if (vdev->vq[i].inuse > vdev->vq[i].vring.num) {
+                error_report("VQ %d size 0x%x < last_avail_idx 0x%x - "
+                             "used_idx 0x%x",
+                             i, vdev->vq[i].vring.num,
+                             vdev->vq[i].last_avail_idx,
+                             vdev->vq[i].used_idx);
+                return -1;
+            }
+        }
+    }
+    rcu_read_unlock();
+
+    return 0;
+}
+
+void virtio_cleanup(VirtIODevice *vdev)
+{
+    qemu_del_vm_change_state_handler(vdev->vmstate);
+}
+
+static void virtio_vmstate_change(void *opaque, int running, RunState state)
+{
+    VirtIODevice *vdev = opaque;
+    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));
+    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);
+    bool backend_run = running && (vdev->status & VIRTIO_CONFIG_S_DRIVER_OK);
+    vdev->vm_running = running;
+
+    if (backend_run) {
+        virtio_set_status(vdev, vdev->status);
+    }
+
+    if (k->vmstate_change) {
+        k->vmstate_change(qbus->parent, backend_run);
+    }
+
+    if (!backend_run) {
+        virtio_set_status(vdev, vdev->status);
+    }
+}
+
+void virtio_instance_init_common(Object *proxy_obj, void *data,
+                                 size_t vdev_size, const char *vdev_name)
+{
+    DeviceState *vdev = data;
+
+    object_initialize(vdev, vdev_size, vdev_name);
+    object_property_add_child(proxy_obj, "virtio-backend", OBJECT(vdev), NULL);
+    object_unref(OBJECT(vdev));
+    qdev_alias_all_properties(vdev, proxy_obj);
+}
+
+void virtio_init(VirtIODevice *vdev, const char *name,
+                 uint16_t device_id, size_t config_size)
+{
+    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));
+    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);
+    int i;
+    int nvectors = k->query_nvectors ? k->query_nvectors(qbus->parent) : 0;
+
+    if (nvectors) {
+        vdev->vector_queues =
+            g_malloc0(sizeof(*vdev->vector_queues) * nvectors);
+    }
+
+    vdev->device_id = device_id;
+    vdev->status = 0;
+    atomic_set(&vdev->isr, 0);
+    vdev->queue_sel = 0;
+    vdev->config_vector = VIRTIO_NO_VECTOR;
+    vdev->vq = g_malloc0(sizeof(VirtQueue) * VIRTIO_QUEUE_MAX);
+    vdev->vm_running = runstate_is_running();
+    vdev->broken = false;
+    for (i = 0; i < VIRTIO_QUEUE_MAX; i++) {
+        vdev->vq[i].vector = VIRTIO_NO_VECTOR;
+        vdev->vq[i].vdev = vdev;
+        vdev->vq[i].queue_index = i;
+    }
+
+    vdev->name = name;
+    vdev->config_len = config_size;
+    if (vdev->config_len) {
+        vdev->config = g_malloc0(config_size);
+    } else {
+        vdev->config = NULL;
+    }
+    vdev->vmstate = qemu_add_vm_change_state_handler(virtio_vmstate_change,
+                                                     vdev);
+    vdev->device_endian = virtio_default_endian();
+    vdev->use_guest_notifier_mask = true;
+}
+
+hwaddr virtio_queue_get_desc_addr(VirtIODevice *vdev, int n)
+{
+    return vdev->vq[n].vring.desc;
+}
+
+hwaddr virtio_queue_get_avail_addr(VirtIODevice *vdev, int n)
+{
+    return vdev->vq[n].vring.avail;
+}
+
+hwaddr virtio_queue_get_used_addr(VirtIODevice *vdev, int n)
+{
+    return vdev->vq[n].vring.used;
+}
+
+hwaddr virtio_queue_get_desc_size(VirtIODevice *vdev, int n)
+{
+    return sizeof(VRingDesc) * vdev->vq[n].vring.num;
+}
+
+hwaddr virtio_queue_get_avail_size(VirtIODevice *vdev, int n)
+{
+    return offsetof(VRingAvail, ring) +
+        sizeof(uint16_t) * vdev->vq[n].vring.num;
+}
+
+hwaddr virtio_queue_get_used_size(VirtIODevice *vdev, int n)
+{
+    return offsetof(VRingUsed, ring) +
+        sizeof(VRingUsedElem) * vdev->vq[n].vring.num;
+}
+
+uint16_t virtio_queue_get_last_avail_idx(VirtIODevice *vdev, int n)
+{
+    return vdev->vq[n].last_avail_idx;
+}
+
+void virtio_queue_set_last_avail_idx(VirtIODevice *vdev, int n, uint16_t idx)
+{
+    vdev->vq[n].last_avail_idx = idx;
+    vdev->vq[n].shadow_avail_idx = idx;
+}
+
+void virtio_queue_update_used_idx(VirtIODevice *vdev, int n)
+{
+    rcu_read_lock();
+    if (vdev->vq[n].vring.desc) {
+        vdev->vq[n].used_idx = vring_used_idx(&vdev->vq[n]);
+    }
+    rcu_read_unlock();
+}
+
+void virtio_queue_invalidate_signalled_used(VirtIODevice *vdev, int n)
+{
+    vdev->vq[n].signalled_used_valid = false;
+}
+
+VirtQueue *virtio_get_queue(VirtIODevice *vdev, int n)
+{
+    return vdev->vq + n;
+}
+
+uint16_t virtio_get_queue_index(VirtQueue *vq)
+{
+    return vq->queue_index;
+}
+
+static void virtio_queue_guest_notifier_read(EventNotifier *n)
+{
+    VirtQueue *vq = container_of(n, VirtQueue, guest_notifier);
+    if (event_notifier_test_and_clear(n)) {
+        virtio_irq(vq);
+    }
+}
+
+void virtio_queue_set_guest_notifier_fd_handler(VirtQueue *vq, bool assign,
+                                                bool with_irqfd)
+{
+    if (assign && !with_irqfd) {
+        event_notifier_set_handler(&vq->guest_notifier,
+                                   virtio_queue_guest_notifier_read);
+    } else {
+        event_notifier_set_handler(&vq->guest_notifier, NULL);
+    }
+    if (!assign) {
+        /* Test and clear notifier before closing it,
+         * in case poll callback didn't have time to run. */
+        virtio_queue_guest_notifier_read(&vq->guest_notifier);
+    }
+}
+
+EventNotifier *virtio_queue_get_guest_notifier(VirtQueue *vq)
+{
+    return &vq->guest_notifier;
+}
+
+static void virtio_queue_host_notifier_aio_read(EventNotifier *n)
+{
+    VirtQueue *vq = container_of(n, VirtQueue, host_notifier);
+    if (event_notifier_test_and_clear(n)) {
+        virtio_queue_notify_aio_vq(vq);
+    }
+}
+
+static void virtio_queue_host_notifier_aio_poll_begin(EventNotifier *n)
+{
+    VirtQueue *vq = container_of(n, VirtQueue, host_notifier);
+
+    virtio_queue_set_notification(vq, 0);
+}
+
+static bool virtio_queue_host_notifier_aio_poll(void *opaque)
+{
+    EventNotifier *n = opaque;
+    VirtQueue *vq = container_of(n, VirtQueue, host_notifier);
+    bool progress;
+
+    if (!vq->vring.desc || virtio_queue_empty(vq)) {
+        return false;
+    }
+
+    progress = virtio_queue_notify_aio_vq(vq);
+
+    /* In case the handler function re-enabled notifications */
+    virtio_queue_set_notification(vq, 0);
+    return progress;
+}
+
+static void virtio_queue_host_notifier_aio_poll_end(EventNotifier *n)
+{
+    VirtQueue *vq = container_of(n, VirtQueue, host_notifier);
+
+    /* Caller polls once more after this to catch requests that race with us */
+    virtio_queue_set_notification(vq, 1);
+}
+
+void virtio_queue_aio_set_host_notifier_handler(VirtQueue *vq, AioContext *ctx,
+                                                VirtIOHandleAIOOutput handle_output)
+{
+    if (handle_output) {
+        vq->handle_aio_output = handle_output;
+        aio_set_event_notifier(ctx, &vq->host_notifier, true,
+                               virtio_queue_host_notifier_aio_read,
+                               virtio_queue_host_notifier_aio_poll);
+        aio_set_event_notifier_poll(ctx, &vq->host_notifier,
+                                    virtio_queue_host_notifier_aio_poll_begin,
+                                    virtio_queue_host_notifier_aio_poll_end);
+    } else {
+        aio_set_event_notifier(ctx, &vq->host_notifier, true, NULL, NULL);
+        /* Test and clear notifier before after disabling event,
+         * in case poll callback didn't have time to run. */
+        virtio_queue_host_notifier_aio_read(&vq->host_notifier);
+        vq->handle_aio_output = NULL;
+    }
+}
+
+void virtio_queue_host_notifier_read(EventNotifier *n)
+{
+    VirtQueue *vq = container_of(n, VirtQueue, host_notifier);
+    if (event_notifier_test_and_clear(n)) {
+        virtio_queue_notify_vq(vq);
+    }
+}
+
+EventNotifier *virtio_queue_get_host_notifier(VirtQueue *vq)
+{
+    return &vq->host_notifier;
+}
+
+void virtio_device_set_child_bus_name(VirtIODevice *vdev, char *bus_name)
+{
+    g_free(vdev->bus_name);
+    vdev->bus_name = g_strdup(bus_name);
+}
+
+void GCC_FMT_ATTR(2, 3) virtio_error(VirtIODevice *vdev, const char *fmt, ...)
+{
+    va_list ap;
+
+    va_start(ap, fmt);
+    error_vreport(fmt, ap);
+    va_end(ap);
+
+    vdev->broken = true;
+
+    if (virtio_vdev_has_feature(vdev, VIRTIO_F_VERSION_1)) {
+        virtio_set_status(vdev, vdev->status | VIRTIO_CONFIG_S_NEEDS_RESET);
+        virtio_notify_config(vdev);
+    }
+}
+
+static void virtio_memory_listener_commit(MemoryListener *listener)
+{
+    VirtIODevice *vdev = container_of(listener, VirtIODevice, listener);
+    int i;
+
+    for (i = 0; i < VIRTIO_QUEUE_MAX; i++) {
+        if (vdev->vq[i].vring.num == 0) {
+            break;
+        }
+        virtio_init_region_cache(vdev, i);
+    }
+}
+
+static void virtio_device_realize(DeviceState *dev, Error **errp)
+{
+    VirtIODevice *vdev = VIRTIO_DEVICE(dev);
+    VirtioDeviceClass *vdc = VIRTIO_DEVICE_GET_CLASS(dev);
+    Error *err = NULL;
+
+    /* Devices should either use vmsd or the load/save methods */
+    assert(!vdc->vmsd || !vdc->load);
+
+    if (vdc->realize != NULL) {
+        vdc->realize(dev, &err);
+        if (err != NULL) {
+            error_propagate(errp, err);
+            return;
+        }
+    }
+
+    virtio_bus_device_plugged(vdev, &err);
+    if (err != NULL) {
+        error_propagate(errp, err);
+        return;
+    }
+
+    vdev->listener.commit = virtio_memory_listener_commit;
+    memory_listener_register(&vdev->listener, vdev->dma_as);
+}
+
+static void virtio_device_unrealize(DeviceState *dev, Error **errp)
+{
+    VirtIODevice *vdev = VIRTIO_DEVICE(dev);
+    VirtioDeviceClass *vdc = VIRTIO_DEVICE_GET_CLASS(dev);
+    Error *err = NULL;
+
+    virtio_bus_device_unplugged(vdev);
+
+    if (vdc->unrealize != NULL) {
+        vdc->unrealize(dev, &err);
+        if (err != NULL) {
+            error_propagate(errp, err);
+            return;
+        }
+    }
+
+    g_free(vdev->bus_name);
+    vdev->bus_name = NULL;
+}
+
+static void virtio_device_free_virtqueues(VirtIODevice *vdev)
+{
+    int i;
+    if (!vdev->vq) {
+        return;
+    }
+
+    for (i = 0; i < VIRTIO_QUEUE_MAX; i++) {
+        if (vdev->vq[i].vring.num == 0) {
+            break;
+        }
+        virtio_virtqueue_reset_region_cache(&vdev->vq[i]);
+    }
+    g_free(vdev->vq);
+}
+
+static void virtio_device_instance_finalize(Object *obj)
+{
+    VirtIODevice *vdev = VIRTIO_DEVICE(obj);
+
+    memory_listener_unregister(&vdev->listener);
+    virtio_device_free_virtqueues(vdev);
+
+    g_free(vdev->config);
+    g_free(vdev->vector_queues);
+}
+
+static Property virtio_properties[] = {
+    DEFINE_VIRTIO_COMMON_FEATURES(VirtIODevice, host_features),
+    DEFINE_PROP_END_OF_LIST(),
+};
+
+static int virtio_device_start_ioeventfd_impl(VirtIODevice *vdev)
+{
+    VirtioBusState *qbus = VIRTIO_BUS(qdev_get_parent_bus(DEVICE(vdev)));
+    int n, r, err;
+
+    for (n = 0; n < VIRTIO_QUEUE_MAX; n++) {
+        VirtQueue *vq = &vdev->vq[n];
+        if (!virtio_queue_get_num(vdev, n)) {
+            continue;
+        }
+        r = virtio_bus_set_host_notifier(qbus, n, true);
+        if (r < 0) {
+            err = r;
+            goto assign_error;
+        }
+        event_notifier_set_handler(&vq->host_notifier,
+                                   virtio_queue_host_notifier_read);
+    }
+
+    for (n = 0; n < VIRTIO_QUEUE_MAX; n++) {
+        /* Kick right away to begin processing requests already in vring */
+        VirtQueue *vq = &vdev->vq[n];
+        if (!vq->vring.num) {
+            continue;
+        }
+        event_notifier_set(&vq->host_notifier);
+    }
+    return 0;
+
+assign_error:
+    while (--n >= 0) {
+        VirtQueue *vq = &vdev->vq[n];
+        if (!virtio_queue_get_num(vdev, n)) {
+            continue;
+        }
+
+        event_notifier_set_handler(&vq->host_notifier, NULL);
+        r = virtio_bus_set_host_notifier(qbus, n, false);
+        assert(r >= 0);
+    }
+    return err;
+}
+
+int virtio_device_start_ioeventfd(VirtIODevice *vdev)
+{
+    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));
+    VirtioBusState *vbus = VIRTIO_BUS(qbus);
+
+    return virtio_bus_start_ioeventfd(vbus);
+}
+
+static void virtio_device_stop_ioeventfd_impl(VirtIODevice *vdev)
+{
+    VirtioBusState *qbus = VIRTIO_BUS(qdev_get_parent_bus(DEVICE(vdev)));
+    int n, r;
+
+    for (n = 0; n < VIRTIO_QUEUE_MAX; n++) {
+        VirtQueue *vq = &vdev->vq[n];
+
+        if (!virtio_queue_get_num(vdev, n)) {
+            continue;
+        }
+        event_notifier_set_handler(&vq->host_notifier, NULL);
+        r = virtio_bus_set_host_notifier(qbus, n, false);
+        assert(r >= 0);
+    }
+}
+
+void virtio_device_stop_ioeventfd(VirtIODevice *vdev)
+{
+    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));
+    VirtioBusState *vbus = VIRTIO_BUS(qbus);
+
+    virtio_bus_stop_ioeventfd(vbus);
+}
+
+int virtio_device_grab_ioeventfd(VirtIODevice *vdev)
+{
+    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));
+    VirtioBusState *vbus = VIRTIO_BUS(qbus);
+
+    return virtio_bus_grab_ioeventfd(vbus);
+}
+
+void virtio_device_release_ioeventfd(VirtIODevice *vdev)
+{
+    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));
+    VirtioBusState *vbus = VIRTIO_BUS(qbus);
+
+    virtio_bus_release_ioeventfd(vbus);
+}
+
+static void virtio_device_class_init(ObjectClass *klass, void *data)
+{
+    /* Set the default value here. */
+    VirtioDeviceClass *vdc = VIRTIO_DEVICE_CLASS(klass);
+    DeviceClass *dc = DEVICE_CLASS(klass);
+
+    dc->realize = virtio_device_realize;
+    dc->unrealize = virtio_device_unrealize;
+    dc->bus_type = TYPE_VIRTIO_BUS;
+    dc->props = virtio_properties;
+    vdc->start_ioeventfd = virtio_device_start_ioeventfd_impl;
+    vdc->stop_ioeventfd = virtio_device_stop_ioeventfd_impl;
+
+    vdc->legacy_features |= VIRTIO_LEGACY_FEATURES;
+}
+
+bool virtio_device_ioeventfd_enabled(VirtIODevice *vdev)
+{
+    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));
+    VirtioBusState *vbus = VIRTIO_BUS(qbus);
+
+    return virtio_bus_ioeventfd_enabled(vbus);
+}
+
+static const TypeInfo virtio_device_info = {
+    .name = TYPE_VIRTIO_DEVICE,
+    .parent = TYPE_DEVICE,
+    .instance_size = sizeof(VirtIODevice),
+    .class_init = virtio_device_class_init,
+    .instance_finalize = virtio_device_instance_finalize,
+    .abstract = true,
+    .class_size = sizeof(VirtioDeviceClass),
+};
+
+static void virtio_register_types(void)
+{
+    type_register_static(&virtio_device_info);
+}
+
+type_init(virtio_register_types)
diff -rupN /home/prafull/Desktop/qemu-2.9.0/hw/virtio/virtio-pci.c /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/virtio/virtio-pci.c
--- /home/prafull/Desktop/qemu-2.9.0/hw/virtio/virtio-pci.c	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/virtio/virtio-pci.c	2018-05-28 13:06:18.000000000 +0530
@@ -25,6 +25,9 @@
 #include "hw/virtio/virtio-scsi.h"
 #include "hw/virtio/virtio-balloon.h"
 #include "hw/virtio/virtio-input.h"
+/* Added by Bhavesh Singh. 2017.02.03. Begin add */
+#include "hw/virtio/virtio-vssd.h"
+/* Added by Bhavesh Singh. 2017.02.03. End add */
 #include "hw/pci/pci.h"
 #include "qapi/error.h"
 #include "qemu/error-report.h"
@@ -2420,6 +2423,64 @@ static const TypeInfo virtio_rng_pci_inf
     .class_init    = virtio_rng_pci_class_init,
 };
 
+/* Added by Bhavesh Singh. 2017.02.03. Begin add */
+/* virtio-vssd-pci */
+
+static Property virtio_vssd_pci_properties[] = {
+    DEFINE_PROP_END_OF_LIST(),
+};
+
+static void virtio_vssd_pci_realize(VirtIOPCIProxy *vpci_dev, Error **errp)
+{
+    printf("\t virtio_vssd_pci_realize function called \n");
+    VirtIOVssdPCI *vssd = VIRTIO_VSSD_PCI(vpci_dev);
+    DeviceState *vdev = DEVICE(&vssd->vdev);
+
+    qdev_set_parent_bus(vdev, BUS(&vpci_dev->bus));
+    object_property_set_bool(OBJECT(vdev), true, "realized", errp);
+}
+
+static void virtio_vssd_pci_class_init(ObjectClass *klass, void *data)
+{
+    printf("\t virtio_vssd_pci_class_init function called \n");
+
+    DeviceClass *dc = DEVICE_CLASS(klass);
+    VirtioPCIClass *k = VIRTIO_PCI_CLASS(klass);
+    PCIDeviceClass *pcidev_k = PCI_DEVICE_CLASS(klass);
+
+    k->realize = virtio_vssd_pci_realize;
+    dc->props = virtio_vssd_pci_properties;
+    set_bit(DEVICE_CATEGORY_MISC, dc->categories);
+
+    pcidev_k->vendor_id = PCI_VENDOR_ID_REDHAT_QUMRANET;
+    pcidev_k->device_id = PCI_DEVICE_ID_VIRTIO_VSSD;
+    pcidev_k->revision = VIRTIO_PCI_ABI_VERSION;
+    pcidev_k->class_id = PCI_CLASS_OTHERS;
+
+
+}
+
+static void virtio_vssd_pci_instance_init(Object *obj)
+{
+    printf("\t virtio_vssd_pci_instance_init function called \n");
+    
+    VirtIOVssdPCI *dev = VIRTIO_VSSD_PCI(obj);
+
+    virtio_instance_init_common(obj, &dev->vdev, sizeof(dev->vdev),
+                                TYPE_VIRTIO_VSSD);
+    /*object_property_add_alias(obj, "vssd", OBJECT(&dev->vdev), "vssd",
+      &error_abort);*/
+}
+
+static const TypeInfo virtio_vssd_pci_info = {
+    .name          = TYPE_VIRTIO_VSSD_PCI,
+    .parent        = TYPE_VIRTIO_PCI,
+    .instance_size = sizeof(VirtIOVssdPCI),
+    .instance_init = virtio_vssd_pci_instance_init,
+    .class_init    = virtio_vssd_pci_class_init,
+};
+/* Added by Bhavesh Singh. 2017.02.03. End add */
+
 /* virtio-input-pci */
 
 static Property virtio_input_pci_properties[] = {
@@ -2615,6 +2676,9 @@ static void virtio_pci_register_types(vo
 #ifdef CONFIG_VHOST_VSOCK
     type_register_static(&vhost_vsock_pci_info);
 #endif
+    /* Added by Bhavesh Singh. 2017.02.10. Begin add */
+    type_register_static(&virtio_vssd_pci_info);
+    /* Added by Bhavesh Singh. 2017.02.10. End add */
 }
 
 type_init(virtio_pci_register_types)
diff -rupN /home/prafull/Desktop/qemu-2.9.0/hw/virtio/virtio-pci.c.orig /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/virtio/virtio-pci.c.orig
--- /home/prafull/Desktop/qemu-2.9.0/hw/virtio/virtio-pci.c.orig	1970-01-01 05:30:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/virtio/virtio-pci.c.orig	2018-05-28 13:06:18.000000000 +0530
@@ -0,0 +1,2620 @@
+/*
+ * Virtio PCI Bindings
+ *
+ * Copyright IBM, Corp. 2007
+ * Copyright (c) 2009 CodeSourcery
+ *
+ * Authors:
+ *  Anthony Liguori   <aliguori@us.ibm.com>
+ *  Paul Brook        <paul@codesourcery.com>
+ *
+ * This work is licensed under the terms of the GNU GPL, version 2.  See
+ * the COPYING file in the top-level directory.
+ *
+ * Contributions after 2012-01-13 are licensed under the terms of the
+ * GNU GPL, version 2 or (at your option) any later version.
+ */
+
+#include "qemu/osdep.h"
+
+#include "standard-headers/linux/virtio_pci.h"
+#include "hw/virtio/virtio.h"
+#include "hw/virtio/virtio-blk.h"
+#include "hw/virtio/virtio-net.h"
+#include "hw/virtio/virtio-serial.h"
+#include "hw/virtio/virtio-scsi.h"
+#include "hw/virtio/virtio-balloon.h"
+#include "hw/virtio/virtio-input.h"
+#include "hw/pci/pci.h"
+#include "qapi/error.h"
+#include "qemu/error-report.h"
+#include "hw/pci/msi.h"
+#include "hw/pci/msix.h"
+#include "hw/loader.h"
+#include "sysemu/kvm.h"
+#include "sysemu/block-backend.h"
+#include "virtio-pci.h"
+#include "qemu/range.h"
+#include "hw/virtio/virtio-bus.h"
+#include "qapi/visitor.h"
+
+#define VIRTIO_PCI_REGION_SIZE(dev)     VIRTIO_PCI_CONFIG_OFF(msix_present(dev))
+
+#undef VIRTIO_PCI_CONFIG
+
+/* The remaining space is defined by each driver as the per-driver
+ * configuration space */
+#define VIRTIO_PCI_CONFIG_SIZE(dev)     VIRTIO_PCI_CONFIG_OFF(msix_enabled(dev))
+
+static void virtio_pci_bus_new(VirtioBusState *bus, size_t bus_size,
+                               VirtIOPCIProxy *dev);
+static void virtio_pci_reset(DeviceState *qdev);
+
+/* virtio device */
+/* DeviceState to VirtIOPCIProxy. For use off data-path. TODO: use QOM. */
+static inline VirtIOPCIProxy *to_virtio_pci_proxy(DeviceState *d)
+{
+    return container_of(d, VirtIOPCIProxy, pci_dev.qdev);
+}
+
+/* DeviceState to VirtIOPCIProxy. Note: used on datapath,
+ * be careful and test performance if you change this.
+ */
+static inline VirtIOPCIProxy *to_virtio_pci_proxy_fast(DeviceState *d)
+{
+    return container_of(d, VirtIOPCIProxy, pci_dev.qdev);
+}
+
+static void virtio_pci_notify(DeviceState *d, uint16_t vector)
+{
+    VirtIOPCIProxy *proxy = to_virtio_pci_proxy_fast(d);
+
+    if (msix_enabled(&proxy->pci_dev))
+        msix_notify(&proxy->pci_dev, vector);
+    else {
+        VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+        pci_set_irq(&proxy->pci_dev, atomic_read(&vdev->isr) & 1);
+    }
+}
+
+static void virtio_pci_save_config(DeviceState *d, QEMUFile *f)
+{
+    VirtIOPCIProxy *proxy = to_virtio_pci_proxy(d);
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+
+    pci_device_save(&proxy->pci_dev, f);
+    msix_save(&proxy->pci_dev, f);
+    if (msix_present(&proxy->pci_dev))
+        qemu_put_be16(f, vdev->config_vector);
+}
+
+static void virtio_pci_load_modern_queue_state(VirtIOPCIQueue *vq,
+                                               QEMUFile *f)
+{
+    vq->num = qemu_get_be16(f);
+    vq->enabled = qemu_get_be16(f);
+    vq->desc[0] = qemu_get_be32(f);
+    vq->desc[1] = qemu_get_be32(f);
+    vq->avail[0] = qemu_get_be32(f);
+    vq->avail[1] = qemu_get_be32(f);
+    vq->used[0] = qemu_get_be32(f);
+    vq->used[1] = qemu_get_be32(f);
+}
+
+static bool virtio_pci_has_extra_state(DeviceState *d)
+{
+    VirtIOPCIProxy *proxy = to_virtio_pci_proxy(d);
+
+    return proxy->flags & VIRTIO_PCI_FLAG_MIGRATE_EXTRA;
+}
+
+static int get_virtio_pci_modern_state(QEMUFile *f, void *pv, size_t size,
+                                       VMStateField *field)
+{
+    VirtIOPCIProxy *proxy = pv;
+    int i;
+
+    proxy->dfselect = qemu_get_be32(f);
+    proxy->gfselect = qemu_get_be32(f);
+    proxy->guest_features[0] = qemu_get_be32(f);
+    proxy->guest_features[1] = qemu_get_be32(f);
+    for (i = 0; i < VIRTIO_QUEUE_MAX; i++) {
+        virtio_pci_load_modern_queue_state(&proxy->vqs[i], f);
+    }
+
+    return 0;
+}
+
+static void virtio_pci_save_modern_queue_state(VirtIOPCIQueue *vq,
+                                               QEMUFile *f)
+{
+    qemu_put_be16(f, vq->num);
+    qemu_put_be16(f, vq->enabled);
+    qemu_put_be32(f, vq->desc[0]);
+    qemu_put_be32(f, vq->desc[1]);
+    qemu_put_be32(f, vq->avail[0]);
+    qemu_put_be32(f, vq->avail[1]);
+    qemu_put_be32(f, vq->used[0]);
+    qemu_put_be32(f, vq->used[1]);
+}
+
+static int put_virtio_pci_modern_state(QEMUFile *f, void *pv, size_t size,
+                                       VMStateField *field, QJSON *vmdesc)
+{
+    VirtIOPCIProxy *proxy = pv;
+    int i;
+
+    qemu_put_be32(f, proxy->dfselect);
+    qemu_put_be32(f, proxy->gfselect);
+    qemu_put_be32(f, proxy->guest_features[0]);
+    qemu_put_be32(f, proxy->guest_features[1]);
+    for (i = 0; i < VIRTIO_QUEUE_MAX; i++) {
+        virtio_pci_save_modern_queue_state(&proxy->vqs[i], f);
+    }
+
+    return 0;
+}
+
+static const VMStateInfo vmstate_info_virtio_pci_modern_state = {
+    .name = "virtqueue_state",
+    .get = get_virtio_pci_modern_state,
+    .put = put_virtio_pci_modern_state,
+};
+
+static bool virtio_pci_modern_state_needed(void *opaque)
+{
+    VirtIOPCIProxy *proxy = opaque;
+
+    return virtio_pci_modern(proxy);
+}
+
+static const VMStateDescription vmstate_virtio_pci_modern_state = {
+    .name = "virtio_pci/modern_state",
+    .version_id = 1,
+    .minimum_version_id = 1,
+    .needed = &virtio_pci_modern_state_needed,
+    .fields = (VMStateField[]) {
+        {
+            .name         = "modern_state",
+            .version_id   = 0,
+            .field_exists = NULL,
+            .size         = 0,
+            .info         = &vmstate_info_virtio_pci_modern_state,
+            .flags        = VMS_SINGLE,
+            .offset       = 0,
+        },
+        VMSTATE_END_OF_LIST()
+    }
+};
+
+static const VMStateDescription vmstate_virtio_pci = {
+    .name = "virtio_pci",
+    .version_id = 1,
+    .minimum_version_id = 1,
+    .minimum_version_id_old = 1,
+    .fields = (VMStateField[]) {
+        VMSTATE_END_OF_LIST()
+    },
+    .subsections = (const VMStateDescription*[]) {
+        &vmstate_virtio_pci_modern_state,
+        NULL
+    }
+};
+
+static void virtio_pci_save_extra_state(DeviceState *d, QEMUFile *f)
+{
+    VirtIOPCIProxy *proxy = to_virtio_pci_proxy(d);
+
+    vmstate_save_state(f, &vmstate_virtio_pci, proxy, NULL);
+}
+
+static int virtio_pci_load_extra_state(DeviceState *d, QEMUFile *f)
+{
+    VirtIOPCIProxy *proxy = to_virtio_pci_proxy(d);
+
+    return vmstate_load_state(f, &vmstate_virtio_pci, proxy, 1);
+}
+
+static void virtio_pci_save_queue(DeviceState *d, int n, QEMUFile *f)
+{
+    VirtIOPCIProxy *proxy = to_virtio_pci_proxy(d);
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+
+    if (msix_present(&proxy->pci_dev))
+        qemu_put_be16(f, virtio_queue_vector(vdev, n));
+}
+
+static int virtio_pci_load_config(DeviceState *d, QEMUFile *f)
+{
+    VirtIOPCIProxy *proxy = to_virtio_pci_proxy(d);
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+
+    int ret;
+    ret = pci_device_load(&proxy->pci_dev, f);
+    if (ret) {
+        return ret;
+    }
+    msix_unuse_all_vectors(&proxy->pci_dev);
+    msix_load(&proxy->pci_dev, f);
+    if (msix_present(&proxy->pci_dev)) {
+        qemu_get_be16s(f, &vdev->config_vector);
+    } else {
+        vdev->config_vector = VIRTIO_NO_VECTOR;
+    }
+    if (vdev->config_vector != VIRTIO_NO_VECTOR) {
+        return msix_vector_use(&proxy->pci_dev, vdev->config_vector);
+    }
+    return 0;
+}
+
+static int virtio_pci_load_queue(DeviceState *d, int n, QEMUFile *f)
+{
+    VirtIOPCIProxy *proxy = to_virtio_pci_proxy(d);
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+
+    uint16_t vector;
+    if (msix_present(&proxy->pci_dev)) {
+        qemu_get_be16s(f, &vector);
+    } else {
+        vector = VIRTIO_NO_VECTOR;
+    }
+    virtio_queue_set_vector(vdev, n, vector);
+    if (vector != VIRTIO_NO_VECTOR) {
+        return msix_vector_use(&proxy->pci_dev, vector);
+    }
+
+    return 0;
+}
+
+static bool virtio_pci_ioeventfd_enabled(DeviceState *d)
+{
+    VirtIOPCIProxy *proxy = to_virtio_pci_proxy(d);
+
+    return (proxy->flags & VIRTIO_PCI_FLAG_USE_IOEVENTFD) != 0;
+}
+
+#define QEMU_VIRTIO_PCI_QUEUE_MEM_MULT 0x1000
+
+static inline int virtio_pci_queue_mem_mult(struct VirtIOPCIProxy *proxy)
+{
+    return (proxy->flags & VIRTIO_PCI_FLAG_PAGE_PER_VQ) ?
+        QEMU_VIRTIO_PCI_QUEUE_MEM_MULT : 4;
+}
+
+static int virtio_pci_ioeventfd_assign(DeviceState *d, EventNotifier *notifier,
+                                       int n, bool assign)
+{
+    VirtIOPCIProxy *proxy = to_virtio_pci_proxy(d);
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    VirtQueue *vq = virtio_get_queue(vdev, n);
+    bool legacy = virtio_pci_legacy(proxy);
+    bool modern = virtio_pci_modern(proxy);
+    bool fast_mmio = kvm_ioeventfd_any_length_enabled();
+    bool modern_pio = proxy->flags & VIRTIO_PCI_FLAG_MODERN_PIO_NOTIFY;
+    MemoryRegion *modern_mr = &proxy->notify.mr;
+    MemoryRegion *modern_notify_mr = &proxy->notify_pio.mr;
+    MemoryRegion *legacy_mr = &proxy->bar;
+    hwaddr modern_addr = virtio_pci_queue_mem_mult(proxy) *
+                         virtio_get_queue_index(vq);
+    hwaddr legacy_addr = VIRTIO_PCI_QUEUE_NOTIFY;
+
+    if (assign) {
+        if (modern) {
+            if (fast_mmio) {
+                memory_region_add_eventfd(modern_mr, modern_addr, 0,
+                                          false, n, notifier);
+            } else {
+                memory_region_add_eventfd(modern_mr, modern_addr, 2,
+                                          false, n, notifier);
+            }
+            if (modern_pio) {
+                memory_region_add_eventfd(modern_notify_mr, 0, 2,
+                                              true, n, notifier);
+            }
+        }
+        if (legacy) {
+            memory_region_add_eventfd(legacy_mr, legacy_addr, 2,
+                                      true, n, notifier);
+        }
+    } else {
+        if (modern) {
+            if (fast_mmio) {
+                memory_region_del_eventfd(modern_mr, modern_addr, 0,
+                                          false, n, notifier);
+            } else {
+                memory_region_del_eventfd(modern_mr, modern_addr, 2,
+                                          false, n, notifier);
+            }
+            if (modern_pio) {
+                memory_region_del_eventfd(modern_notify_mr, 0, 2,
+                                          true, n, notifier);
+            }
+        }
+        if (legacy) {
+            memory_region_del_eventfd(legacy_mr, legacy_addr, 2,
+                                      true, n, notifier);
+        }
+    }
+    return 0;
+}
+
+static void virtio_pci_start_ioeventfd(VirtIOPCIProxy *proxy)
+{
+    virtio_bus_start_ioeventfd(&proxy->bus);
+}
+
+static void virtio_pci_stop_ioeventfd(VirtIOPCIProxy *proxy)
+{
+    virtio_bus_stop_ioeventfd(&proxy->bus);
+}
+
+static void virtio_ioport_write(void *opaque, uint32_t addr, uint32_t val)
+{
+    VirtIOPCIProxy *proxy = opaque;
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    hwaddr pa;
+
+    switch (addr) {
+    case VIRTIO_PCI_GUEST_FEATURES:
+        /* Guest does not negotiate properly?  We have to assume nothing. */
+        if (val & (1 << VIRTIO_F_BAD_FEATURE)) {
+            val = virtio_bus_get_vdev_bad_features(&proxy->bus);
+        }
+        virtio_set_features(vdev, val);
+        break;
+    case VIRTIO_PCI_QUEUE_PFN:
+        pa = (hwaddr)val << VIRTIO_PCI_QUEUE_ADDR_SHIFT;
+        if (pa == 0) {
+            virtio_pci_reset(DEVICE(proxy));
+        }
+        else
+            virtio_queue_set_addr(vdev, vdev->queue_sel, pa);
+        break;
+    case VIRTIO_PCI_QUEUE_SEL:
+        if (val < VIRTIO_QUEUE_MAX)
+            vdev->queue_sel = val;
+        break;
+    case VIRTIO_PCI_QUEUE_NOTIFY:
+        if (val < VIRTIO_QUEUE_MAX) {
+            virtio_queue_notify(vdev, val);
+        }
+        break;
+    case VIRTIO_PCI_STATUS:
+        if (!(val & VIRTIO_CONFIG_S_DRIVER_OK)) {
+            virtio_pci_stop_ioeventfd(proxy);
+        }
+
+        virtio_set_status(vdev, val & 0xFF);
+
+        if (val & VIRTIO_CONFIG_S_DRIVER_OK) {
+            virtio_pci_start_ioeventfd(proxy);
+        }
+
+        if (vdev->status == 0) {
+            virtio_pci_reset(DEVICE(proxy));
+        }
+
+        /* Linux before 2.6.34 drives the device without enabling
+           the PCI device bus master bit. Enable it automatically
+           for the guest. This is a PCI spec violation but so is
+           initiating DMA with bus master bit clear. */
+        if (val == (VIRTIO_CONFIG_S_ACKNOWLEDGE | VIRTIO_CONFIG_S_DRIVER)) {
+            pci_default_write_config(&proxy->pci_dev, PCI_COMMAND,
+                                     proxy->pci_dev.config[PCI_COMMAND] |
+                                     PCI_COMMAND_MASTER, 1);
+        }
+        break;
+    case VIRTIO_MSI_CONFIG_VECTOR:
+        msix_vector_unuse(&proxy->pci_dev, vdev->config_vector);
+        /* Make it possible for guest to discover an error took place. */
+        if (msix_vector_use(&proxy->pci_dev, val) < 0)
+            val = VIRTIO_NO_VECTOR;
+        vdev->config_vector = val;
+        break;
+    case VIRTIO_MSI_QUEUE_VECTOR:
+        msix_vector_unuse(&proxy->pci_dev,
+                          virtio_queue_vector(vdev, vdev->queue_sel));
+        /* Make it possible for guest to discover an error took place. */
+        if (msix_vector_use(&proxy->pci_dev, val) < 0)
+            val = VIRTIO_NO_VECTOR;
+        virtio_queue_set_vector(vdev, vdev->queue_sel, val);
+        break;
+    default:
+        error_report("%s: unexpected address 0x%x value 0x%x",
+                     __func__, addr, val);
+        break;
+    }
+}
+
+static uint32_t virtio_ioport_read(VirtIOPCIProxy *proxy, uint32_t addr)
+{
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    uint32_t ret = 0xFFFFFFFF;
+
+    switch (addr) {
+    case VIRTIO_PCI_HOST_FEATURES:
+        ret = vdev->host_features;
+        break;
+    case VIRTIO_PCI_GUEST_FEATURES:
+        ret = vdev->guest_features;
+        break;
+    case VIRTIO_PCI_QUEUE_PFN:
+        ret = virtio_queue_get_addr(vdev, vdev->queue_sel)
+              >> VIRTIO_PCI_QUEUE_ADDR_SHIFT;
+        break;
+    case VIRTIO_PCI_QUEUE_NUM:
+        ret = virtio_queue_get_num(vdev, vdev->queue_sel);
+        break;
+    case VIRTIO_PCI_QUEUE_SEL:
+        ret = vdev->queue_sel;
+        break;
+    case VIRTIO_PCI_STATUS:
+        ret = vdev->status;
+        break;
+    case VIRTIO_PCI_ISR:
+        /* reading from the ISR also clears it. */
+        ret = atomic_xchg(&vdev->isr, 0);
+        pci_irq_deassert(&proxy->pci_dev);
+        break;
+    case VIRTIO_MSI_CONFIG_VECTOR:
+        ret = vdev->config_vector;
+        break;
+    case VIRTIO_MSI_QUEUE_VECTOR:
+        ret = virtio_queue_vector(vdev, vdev->queue_sel);
+        break;
+    default:
+        break;
+    }
+
+    return ret;
+}
+
+static uint64_t virtio_pci_config_read(void *opaque, hwaddr addr,
+                                       unsigned size)
+{
+    VirtIOPCIProxy *proxy = opaque;
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    uint32_t config = VIRTIO_PCI_CONFIG_SIZE(&proxy->pci_dev);
+    uint64_t val = 0;
+    if (addr < config) {
+        return virtio_ioport_read(proxy, addr);
+    }
+    addr -= config;
+
+    switch (size) {
+    case 1:
+        val = virtio_config_readb(vdev, addr);
+        break;
+    case 2:
+        val = virtio_config_readw(vdev, addr);
+        if (virtio_is_big_endian(vdev)) {
+            val = bswap16(val);
+        }
+        break;
+    case 4:
+        val = virtio_config_readl(vdev, addr);
+        if (virtio_is_big_endian(vdev)) {
+            val = bswap32(val);
+        }
+        break;
+    }
+    return val;
+}
+
+static void virtio_pci_config_write(void *opaque, hwaddr addr,
+                                    uint64_t val, unsigned size)
+{
+    VirtIOPCIProxy *proxy = opaque;
+    uint32_t config = VIRTIO_PCI_CONFIG_SIZE(&proxy->pci_dev);
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    if (addr < config) {
+        virtio_ioport_write(proxy, addr, val);
+        return;
+    }
+    addr -= config;
+    /*
+     * Virtio-PCI is odd. Ioports are LE but config space is target native
+     * endian.
+     */
+    switch (size) {
+    case 1:
+        virtio_config_writeb(vdev, addr, val);
+        break;
+    case 2:
+        if (virtio_is_big_endian(vdev)) {
+            val = bswap16(val);
+        }
+        virtio_config_writew(vdev, addr, val);
+        break;
+    case 4:
+        if (virtio_is_big_endian(vdev)) {
+            val = bswap32(val);
+        }
+        virtio_config_writel(vdev, addr, val);
+        break;
+    }
+}
+
+static const MemoryRegionOps virtio_pci_config_ops = {
+    .read = virtio_pci_config_read,
+    .write = virtio_pci_config_write,
+    .impl = {
+        .min_access_size = 1,
+        .max_access_size = 4,
+    },
+    .endianness = DEVICE_LITTLE_ENDIAN,
+};
+
+/* Below are generic functions to do memcpy from/to an address space,
+ * without byteswaps, with input validation.
+ *
+ * As regular address_space_* APIs all do some kind of byteswap at least for
+ * some host/target combinations, we are forced to explicitly convert to a
+ * known-endianness integer value.
+ * It doesn't really matter which endian format to go through, so the code
+ * below selects the endian that causes the least amount of work on the given
+ * host.
+ *
+ * Note: host pointer must be aligned.
+ */
+static
+void virtio_address_space_write(AddressSpace *as, hwaddr addr,
+                                const uint8_t *buf, int len)
+{
+    uint32_t val;
+
+    /* address_space_* APIs assume an aligned address.
+     * As address is under guest control, handle illegal values.
+     */
+    addr &= ~(len - 1);
+
+    /* Make sure caller aligned buf properly */
+    assert(!(((uintptr_t)buf) & (len - 1)));
+
+    switch (len) {
+    case 1:
+        val = pci_get_byte(buf);
+        address_space_stb(as, addr, val, MEMTXATTRS_UNSPECIFIED, NULL);
+        break;
+    case 2:
+        val = pci_get_word(buf);
+        address_space_stw_le(as, addr, val, MEMTXATTRS_UNSPECIFIED, NULL);
+        break;
+    case 4:
+        val = pci_get_long(buf);
+        address_space_stl_le(as, addr, val, MEMTXATTRS_UNSPECIFIED, NULL);
+        break;
+    default:
+        /* As length is under guest control, handle illegal values. */
+        break;
+    }
+}
+
+static void
+virtio_address_space_read(AddressSpace *as, hwaddr addr, uint8_t *buf, int len)
+{
+    uint32_t val;
+
+    /* address_space_* APIs assume an aligned address.
+     * As address is under guest control, handle illegal values.
+     */
+    addr &= ~(len - 1);
+
+    /* Make sure caller aligned buf properly */
+    assert(!(((uintptr_t)buf) & (len - 1)));
+
+    switch (len) {
+    case 1:
+        val = address_space_ldub(as, addr, MEMTXATTRS_UNSPECIFIED, NULL);
+        pci_set_byte(buf, val);
+        break;
+    case 2:
+        val = address_space_lduw_le(as, addr, MEMTXATTRS_UNSPECIFIED, NULL);
+        pci_set_word(buf, val);
+        break;
+    case 4:
+        val = address_space_ldl_le(as, addr, MEMTXATTRS_UNSPECIFIED, NULL);
+        pci_set_long(buf, val);
+        break;
+    default:
+        /* As length is under guest control, handle illegal values. */
+        break;
+    }
+}
+
+static void virtio_write_config(PCIDevice *pci_dev, uint32_t address,
+                                uint32_t val, int len)
+{
+    VirtIOPCIProxy *proxy = DO_UPCAST(VirtIOPCIProxy, pci_dev, pci_dev);
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    struct virtio_pci_cfg_cap *cfg;
+
+    pci_default_write_config(pci_dev, address, val, len);
+
+    if (range_covers_byte(address, len, PCI_COMMAND) &&
+        !(pci_dev->config[PCI_COMMAND] & PCI_COMMAND_MASTER)) {
+        virtio_pci_stop_ioeventfd(proxy);
+        virtio_set_status(vdev, vdev->status & ~VIRTIO_CONFIG_S_DRIVER_OK);
+    }
+
+    if (proxy->config_cap &&
+        ranges_overlap(address, len, proxy->config_cap + offsetof(struct virtio_pci_cfg_cap,
+                                                                  pci_cfg_data),
+                       sizeof cfg->pci_cfg_data)) {
+        uint32_t off;
+        uint32_t len;
+
+        cfg = (void *)(proxy->pci_dev.config + proxy->config_cap);
+        off = le32_to_cpu(cfg->cap.offset);
+        len = le32_to_cpu(cfg->cap.length);
+
+        if (len == 1 || len == 2 || len == 4) {
+            assert(len <= sizeof cfg->pci_cfg_data);
+            virtio_address_space_write(&proxy->modern_as, off,
+                                       cfg->pci_cfg_data, len);
+        }
+    }
+}
+
+static uint32_t virtio_read_config(PCIDevice *pci_dev,
+                                   uint32_t address, int len)
+{
+    VirtIOPCIProxy *proxy = DO_UPCAST(VirtIOPCIProxy, pci_dev, pci_dev);
+    struct virtio_pci_cfg_cap *cfg;
+
+    if (proxy->config_cap &&
+        ranges_overlap(address, len, proxy->config_cap + offsetof(struct virtio_pci_cfg_cap,
+                                                                  pci_cfg_data),
+                       sizeof cfg->pci_cfg_data)) {
+        uint32_t off;
+        uint32_t len;
+
+        cfg = (void *)(proxy->pci_dev.config + proxy->config_cap);
+        off = le32_to_cpu(cfg->cap.offset);
+        len = le32_to_cpu(cfg->cap.length);
+
+        if (len == 1 || len == 2 || len == 4) {
+            assert(len <= sizeof cfg->pci_cfg_data);
+            virtio_address_space_read(&proxy->modern_as, off,
+                                      cfg->pci_cfg_data, len);
+        }
+    }
+
+    return pci_default_read_config(pci_dev, address, len);
+}
+
+static int kvm_virtio_pci_vq_vector_use(VirtIOPCIProxy *proxy,
+                                        unsigned int queue_no,
+                                        unsigned int vector)
+{
+    VirtIOIRQFD *irqfd = &proxy->vector_irqfd[vector];
+    int ret;
+
+    if (irqfd->users == 0) {
+        ret = kvm_irqchip_add_msi_route(kvm_state, vector, &proxy->pci_dev);
+        if (ret < 0) {
+            return ret;
+        }
+        irqfd->virq = ret;
+    }
+    irqfd->users++;
+    return 0;
+}
+
+static void kvm_virtio_pci_vq_vector_release(VirtIOPCIProxy *proxy,
+                                             unsigned int vector)
+{
+    VirtIOIRQFD *irqfd = &proxy->vector_irqfd[vector];
+    if (--irqfd->users == 0) {
+        kvm_irqchip_release_virq(kvm_state, irqfd->virq);
+    }
+}
+
+static int kvm_virtio_pci_irqfd_use(VirtIOPCIProxy *proxy,
+                                 unsigned int queue_no,
+                                 unsigned int vector)
+{
+    VirtIOIRQFD *irqfd = &proxy->vector_irqfd[vector];
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    VirtQueue *vq = virtio_get_queue(vdev, queue_no);
+    EventNotifier *n = virtio_queue_get_guest_notifier(vq);
+    return kvm_irqchip_add_irqfd_notifier_gsi(kvm_state, n, NULL, irqfd->virq);
+}
+
+static void kvm_virtio_pci_irqfd_release(VirtIOPCIProxy *proxy,
+                                      unsigned int queue_no,
+                                      unsigned int vector)
+{
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    VirtQueue *vq = virtio_get_queue(vdev, queue_no);
+    EventNotifier *n = virtio_queue_get_guest_notifier(vq);
+    VirtIOIRQFD *irqfd = &proxy->vector_irqfd[vector];
+    int ret;
+
+    ret = kvm_irqchip_remove_irqfd_notifier_gsi(kvm_state, n, irqfd->virq);
+    assert(ret == 0);
+}
+
+static int kvm_virtio_pci_vector_use(VirtIOPCIProxy *proxy, int nvqs)
+{
+    PCIDevice *dev = &proxy->pci_dev;
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    unsigned int vector;
+    int ret, queue_no;
+
+    for (queue_no = 0; queue_no < nvqs; queue_no++) {
+        if (!virtio_queue_get_num(vdev, queue_no)) {
+            break;
+        }
+        vector = virtio_queue_vector(vdev, queue_no);
+        if (vector >= msix_nr_vectors_allocated(dev)) {
+            continue;
+        }
+        ret = kvm_virtio_pci_vq_vector_use(proxy, queue_no, vector);
+        if (ret < 0) {
+            goto undo;
+        }
+        /* If guest supports masking, set up irqfd now.
+         * Otherwise, delay until unmasked in the frontend.
+         */
+        if (vdev->use_guest_notifier_mask && k->guest_notifier_mask) {
+            ret = kvm_virtio_pci_irqfd_use(proxy, queue_no, vector);
+            if (ret < 0) {
+                kvm_virtio_pci_vq_vector_release(proxy, vector);
+                goto undo;
+            }
+        }
+    }
+    return 0;
+
+undo:
+    while (--queue_no >= 0) {
+        vector = virtio_queue_vector(vdev, queue_no);
+        if (vector >= msix_nr_vectors_allocated(dev)) {
+            continue;
+        }
+        if (vdev->use_guest_notifier_mask && k->guest_notifier_mask) {
+            kvm_virtio_pci_irqfd_release(proxy, queue_no, vector);
+        }
+        kvm_virtio_pci_vq_vector_release(proxy, vector);
+    }
+    return ret;
+}
+
+static void kvm_virtio_pci_vector_release(VirtIOPCIProxy *proxy, int nvqs)
+{
+    PCIDevice *dev = &proxy->pci_dev;
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    unsigned int vector;
+    int queue_no;
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+
+    for (queue_no = 0; queue_no < nvqs; queue_no++) {
+        if (!virtio_queue_get_num(vdev, queue_no)) {
+            break;
+        }
+        vector = virtio_queue_vector(vdev, queue_no);
+        if (vector >= msix_nr_vectors_allocated(dev)) {
+            continue;
+        }
+        /* If guest supports masking, clean up irqfd now.
+         * Otherwise, it was cleaned when masked in the frontend.
+         */
+        if (vdev->use_guest_notifier_mask && k->guest_notifier_mask) {
+            kvm_virtio_pci_irqfd_release(proxy, queue_no, vector);
+        }
+        kvm_virtio_pci_vq_vector_release(proxy, vector);
+    }
+}
+
+static int virtio_pci_vq_vector_unmask(VirtIOPCIProxy *proxy,
+                                       unsigned int queue_no,
+                                       unsigned int vector,
+                                       MSIMessage msg)
+{
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    VirtQueue *vq = virtio_get_queue(vdev, queue_no);
+    EventNotifier *n = virtio_queue_get_guest_notifier(vq);
+    VirtIOIRQFD *irqfd;
+    int ret = 0;
+
+    if (proxy->vector_irqfd) {
+        irqfd = &proxy->vector_irqfd[vector];
+        if (irqfd->msg.data != msg.data || irqfd->msg.address != msg.address) {
+            ret = kvm_irqchip_update_msi_route(kvm_state, irqfd->virq, msg,
+                                               &proxy->pci_dev);
+            if (ret < 0) {
+                return ret;
+            }
+            kvm_irqchip_commit_routes(kvm_state);
+        }
+    }
+
+    /* If guest supports masking, irqfd is already setup, unmask it.
+     * Otherwise, set it up now.
+     */
+    if (vdev->use_guest_notifier_mask && k->guest_notifier_mask) {
+        k->guest_notifier_mask(vdev, queue_no, false);
+        /* Test after unmasking to avoid losing events. */
+        if (k->guest_notifier_pending &&
+            k->guest_notifier_pending(vdev, queue_no)) {
+            event_notifier_set(n);
+        }
+    } else {
+        ret = kvm_virtio_pci_irqfd_use(proxy, queue_no, vector);
+    }
+    return ret;
+}
+
+static void virtio_pci_vq_vector_mask(VirtIOPCIProxy *proxy,
+                                             unsigned int queue_no,
+                                             unsigned int vector)
+{
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+
+    /* If guest supports masking, keep irqfd but mask it.
+     * Otherwise, clean it up now.
+     */ 
+    if (vdev->use_guest_notifier_mask && k->guest_notifier_mask) {
+        k->guest_notifier_mask(vdev, queue_no, true);
+    } else {
+        kvm_virtio_pci_irqfd_release(proxy, queue_no, vector);
+    }
+}
+
+static int virtio_pci_vector_unmask(PCIDevice *dev, unsigned vector,
+                                    MSIMessage msg)
+{
+    VirtIOPCIProxy *proxy = container_of(dev, VirtIOPCIProxy, pci_dev);
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    VirtQueue *vq = virtio_vector_first_queue(vdev, vector);
+    int ret, index, unmasked = 0;
+
+    while (vq) {
+        index = virtio_get_queue_index(vq);
+        if (!virtio_queue_get_num(vdev, index)) {
+            break;
+        }
+        if (index < proxy->nvqs_with_notifiers) {
+            ret = virtio_pci_vq_vector_unmask(proxy, index, vector, msg);
+            if (ret < 0) {
+                goto undo;
+            }
+            ++unmasked;
+        }
+        vq = virtio_vector_next_queue(vq);
+    }
+
+    return 0;
+
+undo:
+    vq = virtio_vector_first_queue(vdev, vector);
+    while (vq && unmasked >= 0) {
+        index = virtio_get_queue_index(vq);
+        if (index < proxy->nvqs_with_notifiers) {
+            virtio_pci_vq_vector_mask(proxy, index, vector);
+            --unmasked;
+        }
+        vq = virtio_vector_next_queue(vq);
+    }
+    return ret;
+}
+
+static void virtio_pci_vector_mask(PCIDevice *dev, unsigned vector)
+{
+    VirtIOPCIProxy *proxy = container_of(dev, VirtIOPCIProxy, pci_dev);
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    VirtQueue *vq = virtio_vector_first_queue(vdev, vector);
+    int index;
+
+    while (vq) {
+        index = virtio_get_queue_index(vq);
+        if (!virtio_queue_get_num(vdev, index)) {
+            break;
+        }
+        if (index < proxy->nvqs_with_notifiers) {
+            virtio_pci_vq_vector_mask(proxy, index, vector);
+        }
+        vq = virtio_vector_next_queue(vq);
+    }
+}
+
+static void virtio_pci_vector_poll(PCIDevice *dev,
+                                   unsigned int vector_start,
+                                   unsigned int vector_end)
+{
+    VirtIOPCIProxy *proxy = container_of(dev, VirtIOPCIProxy, pci_dev);
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    int queue_no;
+    unsigned int vector;
+    EventNotifier *notifier;
+    VirtQueue *vq;
+
+    for (queue_no = 0; queue_no < proxy->nvqs_with_notifiers; queue_no++) {
+        if (!virtio_queue_get_num(vdev, queue_no)) {
+            break;
+        }
+        vector = virtio_queue_vector(vdev, queue_no);
+        if (vector < vector_start || vector >= vector_end ||
+            !msix_is_masked(dev, vector)) {
+            continue;
+        }
+        vq = virtio_get_queue(vdev, queue_no);
+        notifier = virtio_queue_get_guest_notifier(vq);
+        if (k->guest_notifier_pending) {
+            if (k->guest_notifier_pending(vdev, queue_no)) {
+                msix_set_pending(dev, vector);
+            }
+        } else if (event_notifier_test_and_clear(notifier)) {
+            msix_set_pending(dev, vector);
+        }
+    }
+}
+
+static int virtio_pci_set_guest_notifier(DeviceState *d, int n, bool assign,
+                                         bool with_irqfd)
+{
+    VirtIOPCIProxy *proxy = to_virtio_pci_proxy(d);
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    VirtioDeviceClass *vdc = VIRTIO_DEVICE_GET_CLASS(vdev);
+    VirtQueue *vq = virtio_get_queue(vdev, n);
+    EventNotifier *notifier = virtio_queue_get_guest_notifier(vq);
+
+    if (assign) {
+        int r = event_notifier_init(notifier, 0);
+        if (r < 0) {
+            return r;
+        }
+        virtio_queue_set_guest_notifier_fd_handler(vq, true, with_irqfd);
+    } else {
+        virtio_queue_set_guest_notifier_fd_handler(vq, false, with_irqfd);
+        event_notifier_cleanup(notifier);
+    }
+
+    if (!msix_enabled(&proxy->pci_dev) &&
+        vdev->use_guest_notifier_mask &&
+        vdc->guest_notifier_mask) {
+        vdc->guest_notifier_mask(vdev, n, !assign);
+    }
+
+    return 0;
+}
+
+static bool virtio_pci_query_guest_notifiers(DeviceState *d)
+{
+    VirtIOPCIProxy *proxy = to_virtio_pci_proxy(d);
+    return msix_enabled(&proxy->pci_dev);
+}
+
+static int virtio_pci_set_guest_notifiers(DeviceState *d, int nvqs, bool assign)
+{
+    VirtIOPCIProxy *proxy = to_virtio_pci_proxy(d);
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
+    int r, n;
+    bool with_irqfd = msix_enabled(&proxy->pci_dev) &&
+        kvm_msi_via_irqfd_enabled();
+
+    nvqs = MIN(nvqs, VIRTIO_QUEUE_MAX);
+
+    /* When deassigning, pass a consistent nvqs value
+     * to avoid leaking notifiers.
+     */
+    assert(assign || nvqs == proxy->nvqs_with_notifiers);
+
+    proxy->nvqs_with_notifiers = nvqs;
+
+    /* Must unset vector notifier while guest notifier is still assigned */
+    if ((proxy->vector_irqfd || k->guest_notifier_mask) && !assign) {
+        msix_unset_vector_notifiers(&proxy->pci_dev);
+        if (proxy->vector_irqfd) {
+            kvm_virtio_pci_vector_release(proxy, nvqs);
+            g_free(proxy->vector_irqfd);
+            proxy->vector_irqfd = NULL;
+        }
+    }
+
+    for (n = 0; n < nvqs; n++) {
+        if (!virtio_queue_get_num(vdev, n)) {
+            break;
+        }
+
+        r = virtio_pci_set_guest_notifier(d, n, assign, with_irqfd);
+        if (r < 0) {
+            goto assign_error;
+        }
+    }
+
+    /* Must set vector notifier after guest notifier has been assigned */
+    if ((with_irqfd || k->guest_notifier_mask) && assign) {
+        if (with_irqfd) {
+            proxy->vector_irqfd =
+                g_malloc0(sizeof(*proxy->vector_irqfd) *
+                          msix_nr_vectors_allocated(&proxy->pci_dev));
+            r = kvm_virtio_pci_vector_use(proxy, nvqs);
+            if (r < 0) {
+                goto assign_error;
+            }
+        }
+        r = msix_set_vector_notifiers(&proxy->pci_dev,
+                                      virtio_pci_vector_unmask,
+                                      virtio_pci_vector_mask,
+                                      virtio_pci_vector_poll);
+        if (r < 0) {
+            goto notifiers_error;
+        }
+    }
+
+    return 0;
+
+notifiers_error:
+    if (with_irqfd) {
+        assert(assign);
+        kvm_virtio_pci_vector_release(proxy, nvqs);
+    }
+
+assign_error:
+    /* We get here on assignment failure. Recover by undoing for VQs 0 .. n. */
+    assert(assign);
+    while (--n >= 0) {
+        virtio_pci_set_guest_notifier(d, n, !assign, with_irqfd);
+    }
+    return r;
+}
+
+static void virtio_pci_vmstate_change(DeviceState *d, bool running)
+{
+    VirtIOPCIProxy *proxy = to_virtio_pci_proxy(d);
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+
+    if (running) {
+        /* Old QEMU versions did not set bus master enable on status write.
+         * Detect DRIVER set and enable it.
+         */
+        if ((proxy->flags & VIRTIO_PCI_FLAG_BUS_MASTER_BUG_MIGRATION) &&
+            (vdev->status & VIRTIO_CONFIG_S_DRIVER) &&
+            !(proxy->pci_dev.config[PCI_COMMAND] & PCI_COMMAND_MASTER)) {
+            pci_default_write_config(&proxy->pci_dev, PCI_COMMAND,
+                                     proxy->pci_dev.config[PCI_COMMAND] |
+                                     PCI_COMMAND_MASTER, 1);
+        }
+        virtio_pci_start_ioeventfd(proxy);
+    } else {
+        virtio_pci_stop_ioeventfd(proxy);
+    }
+}
+
+#ifdef CONFIG_VIRTFS
+static void virtio_9p_pci_realize(VirtIOPCIProxy *vpci_dev, Error **errp)
+{
+    V9fsPCIState *dev = VIRTIO_9P_PCI(vpci_dev);
+    DeviceState *vdev = DEVICE(&dev->vdev);
+
+    qdev_set_parent_bus(vdev, BUS(&vpci_dev->bus));
+    object_property_set_bool(OBJECT(vdev), true, "realized", errp);
+}
+
+static Property virtio_9p_pci_properties[] = {
+    DEFINE_PROP_BIT("ioeventfd", VirtIOPCIProxy, flags,
+                    VIRTIO_PCI_FLAG_USE_IOEVENTFD_BIT, true),
+    DEFINE_PROP_UINT32("vectors", VirtIOPCIProxy, nvectors, 2),
+    DEFINE_PROP_END_OF_LIST(),
+};
+
+static void virtio_9p_pci_class_init(ObjectClass *klass, void *data)
+{
+    DeviceClass *dc = DEVICE_CLASS(klass);
+    PCIDeviceClass *pcidev_k = PCI_DEVICE_CLASS(klass);
+    VirtioPCIClass *k = VIRTIO_PCI_CLASS(klass);
+
+    k->realize = virtio_9p_pci_realize;
+    pcidev_k->vendor_id = PCI_VENDOR_ID_REDHAT_QUMRANET;
+    pcidev_k->device_id = PCI_DEVICE_ID_VIRTIO_9P;
+    pcidev_k->revision = VIRTIO_PCI_ABI_VERSION;
+    pcidev_k->class_id = 0x2;
+    set_bit(DEVICE_CATEGORY_STORAGE, dc->categories);
+    dc->props = virtio_9p_pci_properties;
+}
+
+static void virtio_9p_pci_instance_init(Object *obj)
+{
+    V9fsPCIState *dev = VIRTIO_9P_PCI(obj);
+
+    virtio_instance_init_common(obj, &dev->vdev, sizeof(dev->vdev),
+                                TYPE_VIRTIO_9P);
+}
+
+static const TypeInfo virtio_9p_pci_info = {
+    .name          = TYPE_VIRTIO_9P_PCI,
+    .parent        = TYPE_VIRTIO_PCI,
+    .instance_size = sizeof(V9fsPCIState),
+    .instance_init = virtio_9p_pci_instance_init,
+    .class_init    = virtio_9p_pci_class_init,
+};
+#endif /* CONFIG_VIRTFS */
+
+/*
+ * virtio-pci: This is the PCIDevice which has a virtio-pci-bus.
+ */
+
+static int virtio_pci_query_nvectors(DeviceState *d)
+{
+    VirtIOPCIProxy *proxy = VIRTIO_PCI(d);
+
+    return proxy->nvectors;
+}
+
+static AddressSpace *virtio_pci_get_dma_as(DeviceState *d)
+{
+    VirtIOPCIProxy *proxy = VIRTIO_PCI(d);
+    PCIDevice *dev = &proxy->pci_dev;
+
+    return pci_get_address_space(dev);
+}
+
+static int virtio_pci_add_mem_cap(VirtIOPCIProxy *proxy,
+                                   struct virtio_pci_cap *cap)
+{
+    PCIDevice *dev = &proxy->pci_dev;
+    int offset;
+
+    offset = pci_add_capability(dev, PCI_CAP_ID_VNDR, 0, cap->cap_len);
+    assert(offset > 0);
+
+    assert(cap->cap_len >= sizeof *cap);
+    memcpy(dev->config + offset + PCI_CAP_FLAGS, &cap->cap_len,
+           cap->cap_len - PCI_CAP_FLAGS);
+
+    return offset;
+}
+
+static uint64_t virtio_pci_common_read(void *opaque, hwaddr addr,
+                                       unsigned size)
+{
+    VirtIOPCIProxy *proxy = opaque;
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    uint32_t val = 0;
+    int i;
+
+    switch (addr) {
+    case VIRTIO_PCI_COMMON_DFSELECT:
+        val = proxy->dfselect;
+        break;
+    case VIRTIO_PCI_COMMON_DF:
+        if (proxy->dfselect <= 1) {
+            VirtioDeviceClass *vdc = VIRTIO_DEVICE_GET_CLASS(vdev);
+
+            val = (vdev->host_features & ~vdc->legacy_features) >>
+                (32 * proxy->dfselect);
+        }
+        break;
+    case VIRTIO_PCI_COMMON_GFSELECT:
+        val = proxy->gfselect;
+        break;
+    case VIRTIO_PCI_COMMON_GF:
+        if (proxy->gfselect < ARRAY_SIZE(proxy->guest_features)) {
+            val = proxy->guest_features[proxy->gfselect];
+        }
+        break;
+    case VIRTIO_PCI_COMMON_MSIX:
+        val = vdev->config_vector;
+        break;
+    case VIRTIO_PCI_COMMON_NUMQ:
+        for (i = 0; i < VIRTIO_QUEUE_MAX; ++i) {
+            if (virtio_queue_get_num(vdev, i)) {
+                val = i + 1;
+            }
+        }
+        break;
+    case VIRTIO_PCI_COMMON_STATUS:
+        val = vdev->status;
+        break;
+    case VIRTIO_PCI_COMMON_CFGGENERATION:
+        val = vdev->generation;
+        break;
+    case VIRTIO_PCI_COMMON_Q_SELECT:
+        val = vdev->queue_sel;
+        break;
+    case VIRTIO_PCI_COMMON_Q_SIZE:
+        val = virtio_queue_get_num(vdev, vdev->queue_sel);
+        break;
+    case VIRTIO_PCI_COMMON_Q_MSIX:
+        val = virtio_queue_vector(vdev, vdev->queue_sel);
+        break;
+    case VIRTIO_PCI_COMMON_Q_ENABLE:
+        val = proxy->vqs[vdev->queue_sel].enabled;
+        break;
+    case VIRTIO_PCI_COMMON_Q_NOFF:
+        /* Simply map queues in order */
+        val = vdev->queue_sel;
+        break;
+    case VIRTIO_PCI_COMMON_Q_DESCLO:
+        val = proxy->vqs[vdev->queue_sel].desc[0];
+        break;
+    case VIRTIO_PCI_COMMON_Q_DESCHI:
+        val = proxy->vqs[vdev->queue_sel].desc[1];
+        break;
+    case VIRTIO_PCI_COMMON_Q_AVAILLO:
+        val = proxy->vqs[vdev->queue_sel].avail[0];
+        break;
+    case VIRTIO_PCI_COMMON_Q_AVAILHI:
+        val = proxy->vqs[vdev->queue_sel].avail[1];
+        break;
+    case VIRTIO_PCI_COMMON_Q_USEDLO:
+        val = proxy->vqs[vdev->queue_sel].used[0];
+        break;
+    case VIRTIO_PCI_COMMON_Q_USEDHI:
+        val = proxy->vqs[vdev->queue_sel].used[1];
+        break;
+    default:
+        val = 0;
+    }
+
+    return val;
+}
+
+static void virtio_pci_common_write(void *opaque, hwaddr addr,
+                                    uint64_t val, unsigned size)
+{
+    VirtIOPCIProxy *proxy = opaque;
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+
+    switch (addr) {
+    case VIRTIO_PCI_COMMON_DFSELECT:
+        proxy->dfselect = val;
+        break;
+    case VIRTIO_PCI_COMMON_GFSELECT:
+        proxy->gfselect = val;
+        break;
+    case VIRTIO_PCI_COMMON_GF:
+        if (proxy->gfselect < ARRAY_SIZE(proxy->guest_features)) {
+            proxy->guest_features[proxy->gfselect] = val;
+            virtio_set_features(vdev,
+                                (((uint64_t)proxy->guest_features[1]) << 32) |
+                                proxy->guest_features[0]);
+        }
+        break;
+    case VIRTIO_PCI_COMMON_MSIX:
+        msix_vector_unuse(&proxy->pci_dev, vdev->config_vector);
+        /* Make it possible for guest to discover an error took place. */
+        if (msix_vector_use(&proxy->pci_dev, val) < 0) {
+            val = VIRTIO_NO_VECTOR;
+        }
+        vdev->config_vector = val;
+        break;
+    case VIRTIO_PCI_COMMON_STATUS:
+        if (!(val & VIRTIO_CONFIG_S_DRIVER_OK)) {
+            virtio_pci_stop_ioeventfd(proxy);
+        }
+
+        virtio_set_status(vdev, val & 0xFF);
+
+        if (val & VIRTIO_CONFIG_S_DRIVER_OK) {
+            virtio_pci_start_ioeventfd(proxy);
+        }
+
+        if (vdev->status == 0) {
+            virtio_pci_reset(DEVICE(proxy));
+        }
+
+        break;
+    case VIRTIO_PCI_COMMON_Q_SELECT:
+        if (val < VIRTIO_QUEUE_MAX) {
+            vdev->queue_sel = val;
+        }
+        break;
+    case VIRTIO_PCI_COMMON_Q_SIZE:
+        proxy->vqs[vdev->queue_sel].num = val;
+        break;
+    case VIRTIO_PCI_COMMON_Q_MSIX:
+        msix_vector_unuse(&proxy->pci_dev,
+                          virtio_queue_vector(vdev, vdev->queue_sel));
+        /* Make it possible for guest to discover an error took place. */
+        if (msix_vector_use(&proxy->pci_dev, val) < 0) {
+            val = VIRTIO_NO_VECTOR;
+        }
+        virtio_queue_set_vector(vdev, vdev->queue_sel, val);
+        break;
+    case VIRTIO_PCI_COMMON_Q_ENABLE:
+        virtio_queue_set_num(vdev, vdev->queue_sel,
+                             proxy->vqs[vdev->queue_sel].num);
+        virtio_queue_set_rings(vdev, vdev->queue_sel,
+                       ((uint64_t)proxy->vqs[vdev->queue_sel].desc[1]) << 32 |
+                       proxy->vqs[vdev->queue_sel].desc[0],
+                       ((uint64_t)proxy->vqs[vdev->queue_sel].avail[1]) << 32 |
+                       proxy->vqs[vdev->queue_sel].avail[0],
+                       ((uint64_t)proxy->vqs[vdev->queue_sel].used[1]) << 32 |
+                       proxy->vqs[vdev->queue_sel].used[0]);
+        proxy->vqs[vdev->queue_sel].enabled = 1;
+        break;
+    case VIRTIO_PCI_COMMON_Q_DESCLO:
+        proxy->vqs[vdev->queue_sel].desc[0] = val;
+        break;
+    case VIRTIO_PCI_COMMON_Q_DESCHI:
+        proxy->vqs[vdev->queue_sel].desc[1] = val;
+        break;
+    case VIRTIO_PCI_COMMON_Q_AVAILLO:
+        proxy->vqs[vdev->queue_sel].avail[0] = val;
+        break;
+    case VIRTIO_PCI_COMMON_Q_AVAILHI:
+        proxy->vqs[vdev->queue_sel].avail[1] = val;
+        break;
+    case VIRTIO_PCI_COMMON_Q_USEDLO:
+        proxy->vqs[vdev->queue_sel].used[0] = val;
+        break;
+    case VIRTIO_PCI_COMMON_Q_USEDHI:
+        proxy->vqs[vdev->queue_sel].used[1] = val;
+        break;
+    default:
+        break;
+    }
+}
+
+
+static uint64_t virtio_pci_notify_read(void *opaque, hwaddr addr,
+                                       unsigned size)
+{
+    return 0;
+}
+
+static void virtio_pci_notify_write(void *opaque, hwaddr addr,
+                                    uint64_t val, unsigned size)
+{
+    VirtIODevice *vdev = opaque;
+    VirtIOPCIProxy *proxy = VIRTIO_PCI(DEVICE(vdev)->parent_bus->parent);
+    unsigned queue = addr / virtio_pci_queue_mem_mult(proxy);
+
+    if (queue < VIRTIO_QUEUE_MAX) {
+        virtio_queue_notify(vdev, queue);
+    }
+}
+
+static void virtio_pci_notify_write_pio(void *opaque, hwaddr addr,
+                                        uint64_t val, unsigned size)
+{
+    VirtIODevice *vdev = opaque;
+    unsigned queue = val;
+
+    if (queue < VIRTIO_QUEUE_MAX) {
+        virtio_queue_notify(vdev, queue);
+    }
+}
+
+static uint64_t virtio_pci_isr_read(void *opaque, hwaddr addr,
+                                    unsigned size)
+{
+    VirtIOPCIProxy *proxy = opaque;
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+    uint64_t val = atomic_xchg(&vdev->isr, 0);
+    pci_irq_deassert(&proxy->pci_dev);
+
+    return val;
+}
+
+static void virtio_pci_isr_write(void *opaque, hwaddr addr,
+                                 uint64_t val, unsigned size)
+{
+}
+
+static uint64_t virtio_pci_device_read(void *opaque, hwaddr addr,
+                                       unsigned size)
+{
+    VirtIODevice *vdev = opaque;
+    uint64_t val = 0;
+
+    switch (size) {
+    case 1:
+        val = virtio_config_modern_readb(vdev, addr);
+        break;
+    case 2:
+        val = virtio_config_modern_readw(vdev, addr);
+        break;
+    case 4:
+        val = virtio_config_modern_readl(vdev, addr);
+        break;
+    }
+    return val;
+}
+
+static void virtio_pci_device_write(void *opaque, hwaddr addr,
+                                    uint64_t val, unsigned size)
+{
+    VirtIODevice *vdev = opaque;
+    switch (size) {
+    case 1:
+        virtio_config_modern_writeb(vdev, addr, val);
+        break;
+    case 2:
+        virtio_config_modern_writew(vdev, addr, val);
+        break;
+    case 4:
+        virtio_config_modern_writel(vdev, addr, val);
+        break;
+    }
+}
+
+static void virtio_pci_modern_regions_init(VirtIOPCIProxy *proxy)
+{
+    static const MemoryRegionOps common_ops = {
+        .read = virtio_pci_common_read,
+        .write = virtio_pci_common_write,
+        .impl = {
+            .min_access_size = 1,
+            .max_access_size = 4,
+        },
+        .endianness = DEVICE_LITTLE_ENDIAN,
+    };
+    static const MemoryRegionOps isr_ops = {
+        .read = virtio_pci_isr_read,
+        .write = virtio_pci_isr_write,
+        .impl = {
+            .min_access_size = 1,
+            .max_access_size = 4,
+        },
+        .endianness = DEVICE_LITTLE_ENDIAN,
+    };
+    static const MemoryRegionOps device_ops = {
+        .read = virtio_pci_device_read,
+        .write = virtio_pci_device_write,
+        .impl = {
+            .min_access_size = 1,
+            .max_access_size = 4,
+        },
+        .endianness = DEVICE_LITTLE_ENDIAN,
+    };
+    static const MemoryRegionOps notify_ops = {
+        .read = virtio_pci_notify_read,
+        .write = virtio_pci_notify_write,
+        .impl = {
+            .min_access_size = 1,
+            .max_access_size = 4,
+        },
+        .endianness = DEVICE_LITTLE_ENDIAN,
+    };
+    static const MemoryRegionOps notify_pio_ops = {
+        .read = virtio_pci_notify_read,
+        .write = virtio_pci_notify_write_pio,
+        .impl = {
+            .min_access_size = 1,
+            .max_access_size = 4,
+        },
+        .endianness = DEVICE_LITTLE_ENDIAN,
+    };
+
+
+    memory_region_init_io(&proxy->common.mr, OBJECT(proxy),
+                          &common_ops,
+                          proxy,
+                          "virtio-pci-common",
+                          proxy->common.size);
+
+    memory_region_init_io(&proxy->isr.mr, OBJECT(proxy),
+                          &isr_ops,
+                          proxy,
+                          "virtio-pci-isr",
+                          proxy->isr.size);
+
+    memory_region_init_io(&proxy->device.mr, OBJECT(proxy),
+                          &device_ops,
+                          virtio_bus_get_device(&proxy->bus),
+                          "virtio-pci-device",
+                          proxy->device.size);
+
+    memory_region_init_io(&proxy->notify.mr, OBJECT(proxy),
+                          &notify_ops,
+                          virtio_bus_get_device(&proxy->bus),
+                          "virtio-pci-notify",
+                          proxy->notify.size);
+
+    memory_region_init_io(&proxy->notify_pio.mr, OBJECT(proxy),
+                          &notify_pio_ops,
+                          virtio_bus_get_device(&proxy->bus),
+                          "virtio-pci-notify-pio",
+                          proxy->notify_pio.size);
+}
+
+static void virtio_pci_modern_region_map(VirtIOPCIProxy *proxy,
+                                         VirtIOPCIRegion *region,
+                                         struct virtio_pci_cap *cap,
+                                         MemoryRegion *mr,
+                                         uint8_t bar)
+{
+    memory_region_add_subregion(mr, region->offset, &region->mr);
+
+    cap->cfg_type = region->type;
+    cap->bar = bar;
+    cap->offset = cpu_to_le32(region->offset);
+    cap->length = cpu_to_le32(region->size);
+    virtio_pci_add_mem_cap(proxy, cap);
+
+}
+
+static void virtio_pci_modern_mem_region_map(VirtIOPCIProxy *proxy,
+                                             VirtIOPCIRegion *region,
+                                             struct virtio_pci_cap *cap)
+{
+    virtio_pci_modern_region_map(proxy, region, cap,
+                                 &proxy->modern_bar, proxy->modern_mem_bar_idx);
+}
+
+static void virtio_pci_modern_io_region_map(VirtIOPCIProxy *proxy,
+                                            VirtIOPCIRegion *region,
+                                            struct virtio_pci_cap *cap)
+{
+    virtio_pci_modern_region_map(proxy, region, cap,
+                                 &proxy->io_bar, proxy->modern_io_bar_idx);
+}
+
+static void virtio_pci_modern_mem_region_unmap(VirtIOPCIProxy *proxy,
+                                               VirtIOPCIRegion *region)
+{
+    memory_region_del_subregion(&proxy->modern_bar,
+                                &region->mr);
+}
+
+static void virtio_pci_modern_io_region_unmap(VirtIOPCIProxy *proxy,
+                                              VirtIOPCIRegion *region)
+{
+    memory_region_del_subregion(&proxy->io_bar,
+                                &region->mr);
+}
+
+static void virtio_pci_pre_plugged(DeviceState *d, Error **errp)
+{
+    VirtIOPCIProxy *proxy = VIRTIO_PCI(d);
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+
+    if (virtio_pci_modern(proxy)) {
+        virtio_add_feature(&vdev->host_features, VIRTIO_F_VERSION_1);
+    }
+
+    virtio_add_feature(&vdev->host_features, VIRTIO_F_BAD_FEATURE);
+}
+
+/* This is called by virtio-bus just after the device is plugged. */
+static void virtio_pci_device_plugged(DeviceState *d, Error **errp)
+{
+    VirtIOPCIProxy *proxy = VIRTIO_PCI(d);
+    VirtioBusState *bus = &proxy->bus;
+    bool legacy = virtio_pci_legacy(proxy);
+    bool modern;
+    bool modern_pio = proxy->flags & VIRTIO_PCI_FLAG_MODERN_PIO_NOTIFY;
+    uint8_t *config;
+    uint32_t size;
+    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
+
+    /*
+     * Virtio capabilities present without
+     * VIRTIO_F_VERSION_1 confuses guests
+     */
+    if (!proxy->ignore_backend_features &&
+            !virtio_has_feature(vdev->host_features, VIRTIO_F_VERSION_1)) {
+        virtio_pci_disable_modern(proxy);
+
+        if (!legacy) {
+            error_setg(errp, "Device doesn't support modern mode, and legacy"
+                             " mode is disabled");
+            error_append_hint(errp, "Set disable-legacy to off\n");
+
+            return;
+        }
+    }
+
+    modern = virtio_pci_modern(proxy);
+
+    config = proxy->pci_dev.config;
+    if (proxy->class_code) {
+        pci_config_set_class(config, proxy->class_code);
+    }
+
+    if (legacy) {
+        if (virtio_host_has_feature(vdev, VIRTIO_F_IOMMU_PLATFORM)) {
+            error_setg(errp, "VIRTIO_F_IOMMU_PLATFORM was supported by"
+                       "neither legacy nor transitional device.");
+            return ;
+        }
+        /* legacy and transitional */
+        pci_set_word(config + PCI_SUBSYSTEM_VENDOR_ID,
+                     pci_get_word(config + PCI_VENDOR_ID));
+        pci_set_word(config + PCI_SUBSYSTEM_ID, virtio_bus_get_vdev_id(bus));
+    } else {
+        /* pure virtio-1.0 */
+        pci_set_word(config + PCI_VENDOR_ID,
+                     PCI_VENDOR_ID_REDHAT_QUMRANET);
+        pci_set_word(config + PCI_DEVICE_ID,
+                     0x1040 + virtio_bus_get_vdev_id(bus));
+        pci_config_set_revision(config, 1);
+    }
+    config[PCI_INTERRUPT_PIN] = 1;
+
+
+    if (modern) {
+        struct virtio_pci_cap cap = {
+            .cap_len = sizeof cap,
+        };
+        struct virtio_pci_notify_cap notify = {
+            .cap.cap_len = sizeof notify,
+            .notify_off_multiplier =
+                cpu_to_le32(virtio_pci_queue_mem_mult(proxy)),
+        };
+        struct virtio_pci_cfg_cap cfg = {
+            .cap.cap_len = sizeof cfg,
+            .cap.cfg_type = VIRTIO_PCI_CAP_PCI_CFG,
+        };
+        struct virtio_pci_notify_cap notify_pio = {
+            .cap.cap_len = sizeof notify,
+            .notify_off_multiplier = cpu_to_le32(0x0),
+        };
+
+        struct virtio_pci_cfg_cap *cfg_mask;
+
+        virtio_pci_modern_regions_init(proxy);
+
+        virtio_pci_modern_mem_region_map(proxy, &proxy->common, &cap);
+        virtio_pci_modern_mem_region_map(proxy, &proxy->isr, &cap);
+        virtio_pci_modern_mem_region_map(proxy, &proxy->device, &cap);
+        virtio_pci_modern_mem_region_map(proxy, &proxy->notify, &notify.cap);
+
+        if (modern_pio) {
+            memory_region_init(&proxy->io_bar, OBJECT(proxy),
+                               "virtio-pci-io", 0x4);
+
+            pci_register_bar(&proxy->pci_dev, proxy->modern_io_bar_idx,
+                             PCI_BASE_ADDRESS_SPACE_IO, &proxy->io_bar);
+
+            virtio_pci_modern_io_region_map(proxy, &proxy->notify_pio,
+                                            &notify_pio.cap);
+        }
+
+        pci_register_bar(&proxy->pci_dev, proxy->modern_mem_bar_idx,
+                         PCI_BASE_ADDRESS_SPACE_MEMORY |
+                         PCI_BASE_ADDRESS_MEM_PREFETCH |
+                         PCI_BASE_ADDRESS_MEM_TYPE_64,
+                         &proxy->modern_bar);
+
+        proxy->config_cap = virtio_pci_add_mem_cap(proxy, &cfg.cap);
+        cfg_mask = (void *)(proxy->pci_dev.wmask + proxy->config_cap);
+        pci_set_byte(&cfg_mask->cap.bar, ~0x0);
+        pci_set_long((uint8_t *)&cfg_mask->cap.offset, ~0x0);
+        pci_set_long((uint8_t *)&cfg_mask->cap.length, ~0x0);
+        pci_set_long(cfg_mask->pci_cfg_data, ~0x0);
+    }
+
+    if (proxy->nvectors) {
+        int err = msix_init_exclusive_bar(&proxy->pci_dev, proxy->nvectors,
+                                          proxy->msix_bar_idx, NULL);
+        if (err) {
+            /* Notice when a system that supports MSIx can't initialize it */
+            if (err != -ENOTSUP) {
+                error_report("unable to init msix vectors to %" PRIu32,
+                             proxy->nvectors);
+            }
+            proxy->nvectors = 0;
+        }
+    }
+
+    proxy->pci_dev.config_write = virtio_write_config;
+    proxy->pci_dev.config_read = virtio_read_config;
+
+    if (legacy) {
+        size = VIRTIO_PCI_REGION_SIZE(&proxy->pci_dev)
+            + virtio_bus_get_vdev_config_len(bus);
+        size = pow2ceil(size);
+
+        memory_region_init_io(&proxy->bar, OBJECT(proxy),
+                              &virtio_pci_config_ops,
+                              proxy, "virtio-pci", size);
+
+        pci_register_bar(&proxy->pci_dev, proxy->legacy_io_bar_idx,
+                         PCI_BASE_ADDRESS_SPACE_IO, &proxy->bar);
+    }
+}
+
+static void virtio_pci_device_unplugged(DeviceState *d)
+{
+    VirtIOPCIProxy *proxy = VIRTIO_PCI(d);
+    bool modern = virtio_pci_modern(proxy);
+    bool modern_pio = proxy->flags & VIRTIO_PCI_FLAG_MODERN_PIO_NOTIFY;
+
+    virtio_pci_stop_ioeventfd(proxy);
+
+    if (modern) {
+        virtio_pci_modern_mem_region_unmap(proxy, &proxy->common);
+        virtio_pci_modern_mem_region_unmap(proxy, &proxy->isr);
+        virtio_pci_modern_mem_region_unmap(proxy, &proxy->device);
+        virtio_pci_modern_mem_region_unmap(proxy, &proxy->notify);
+        if (modern_pio) {
+            virtio_pci_modern_io_region_unmap(proxy, &proxy->notify_pio);
+        }
+    }
+}
+
+static void virtio_pci_realize(PCIDevice *pci_dev, Error **errp)
+{
+    VirtIOPCIProxy *proxy = VIRTIO_PCI(pci_dev);
+    VirtioPCIClass *k = VIRTIO_PCI_GET_CLASS(pci_dev);
+    bool pcie_port = pci_bus_is_express(pci_dev->bus) &&
+                     !pci_bus_is_root(pci_dev->bus);
+
+    if (!kvm_has_many_ioeventfds()) {
+        proxy->flags &= ~VIRTIO_PCI_FLAG_USE_IOEVENTFD;
+    }
+
+    /*
+     * virtio pci bar layout used by default.
+     * subclasses can re-arrange things if needed.
+     *
+     *   region 0   --  virtio legacy io bar
+     *   region 1   --  msi-x bar
+     *   region 4+5 --  virtio modern memory (64bit) bar
+     *
+     */
+    proxy->legacy_io_bar_idx  = 0;
+    proxy->msix_bar_idx       = 1;
+    proxy->modern_io_bar_idx  = 2;
+    proxy->modern_mem_bar_idx = 4;
+
+    proxy->common.offset = 0x0;
+    proxy->common.size = 0x1000;
+    proxy->common.type = VIRTIO_PCI_CAP_COMMON_CFG;
+
+    proxy->isr.offset = 0x1000;
+    proxy->isr.size = 0x1000;
+    proxy->isr.type = VIRTIO_PCI_CAP_ISR_CFG;
+
+    proxy->device.offset = 0x2000;
+    proxy->device.size = 0x1000;
+    proxy->device.type = VIRTIO_PCI_CAP_DEVICE_CFG;
+
+    proxy->notify.offset = 0x3000;
+    proxy->notify.size = virtio_pci_queue_mem_mult(proxy) * VIRTIO_QUEUE_MAX;
+    proxy->notify.type = VIRTIO_PCI_CAP_NOTIFY_CFG;
+
+    proxy->notify_pio.offset = 0x0;
+    proxy->notify_pio.size = 0x4;
+    proxy->notify_pio.type = VIRTIO_PCI_CAP_NOTIFY_CFG;
+
+    /* subclasses can enforce modern, so do this unconditionally */
+    memory_region_init(&proxy->modern_bar, OBJECT(proxy), "virtio-pci",
+                       /* PCI BAR regions must be powers of 2 */
+                       pow2ceil(proxy->notify.offset + proxy->notify.size));
+
+    memory_region_init_alias(&proxy->modern_cfg,
+                             OBJECT(proxy),
+                             "virtio-pci-cfg",
+                             &proxy->modern_bar,
+                             0,
+                             memory_region_size(&proxy->modern_bar));
+
+    address_space_init(&proxy->modern_as, &proxy->modern_cfg, "virtio-pci-cfg-as");
+
+    if (proxy->disable_legacy == ON_OFF_AUTO_AUTO) {
+        proxy->disable_legacy = pcie_port ? ON_OFF_AUTO_ON : ON_OFF_AUTO_OFF;
+    }
+
+    if (!virtio_pci_modern(proxy) && !virtio_pci_legacy(proxy)) {
+        error_setg(errp, "device cannot work as neither modern nor legacy mode"
+                   " is enabled");
+        error_append_hint(errp, "Set either disable-modern or disable-legacy"
+                          " to off\n");
+        return;
+    }
+
+    if (pcie_port && pci_is_express(pci_dev)) {
+        int pos;
+
+        pos = pcie_endpoint_cap_init(pci_dev, 0);
+        assert(pos > 0);
+
+        pos = pci_add_capability(pci_dev, PCI_CAP_ID_PM, 0, PCI_PM_SIZEOF);
+        assert(pos > 0);
+        pci_dev->exp.pm_cap = pos;
+
+        /*
+         * Indicates that this function complies with revision 1.2 of the
+         * PCI Power Management Interface Specification.
+         */
+        pci_set_word(pci_dev->config + pos + PCI_PM_PMC, 0x3);
+
+        if (proxy->flags & VIRTIO_PCI_FLAG_INIT_DEVERR) {
+            /* Init error enabling flags */
+            pcie_cap_deverr_init(pci_dev);
+        }
+
+        if (proxy->flags & VIRTIO_PCI_FLAG_INIT_LNKCTL) {
+            /* Init Link Control Register */
+            pcie_cap_lnkctl_init(pci_dev);
+        }
+
+        if (proxy->flags & VIRTIO_PCI_FLAG_INIT_PM) {
+            /* Init Power Management Control Register */
+            pci_set_word(pci_dev->wmask + pos + PCI_PM_CTRL,
+                         PCI_PM_CTRL_STATE_MASK);
+        }
+
+        if (proxy->flags & VIRTIO_PCI_FLAG_ATS) {
+            pcie_ats_init(pci_dev, 256);
+        }
+
+    } else {
+        /*
+         * make future invocations of pci_is_express() return false
+         * and pci_config_size() return PCI_CONFIG_SPACE_SIZE.
+         */
+        pci_dev->cap_present &= ~QEMU_PCI_CAP_EXPRESS;
+    }
+
+    virtio_pci_bus_new(&proxy->bus, sizeof(proxy->bus), proxy);
+    if (k->realize) {
+        k->realize(proxy, errp);
+    }
+}
+
+static void virtio_pci_exit(PCIDevice *pci_dev)
+{
+    VirtIOPCIProxy *proxy = VIRTIO_PCI(pci_dev);
+
+    msix_uninit_exclusive_bar(pci_dev);
+    address_space_destroy(&proxy->modern_as);
+}
+
+static void virtio_pci_reset(DeviceState *qdev)
+{
+    VirtIOPCIProxy *proxy = VIRTIO_PCI(qdev);
+    VirtioBusState *bus = VIRTIO_BUS(&proxy->bus);
+    PCIDevice *dev = PCI_DEVICE(qdev);
+    int i;
+
+    virtio_pci_stop_ioeventfd(proxy);
+    virtio_bus_reset(bus);
+    msix_unuse_all_vectors(&proxy->pci_dev);
+
+    for (i = 0; i < VIRTIO_QUEUE_MAX; i++) {
+        proxy->vqs[i].enabled = 0;
+        proxy->vqs[i].num = 0;
+        proxy->vqs[i].desc[0] = proxy->vqs[i].desc[1] = 0;
+        proxy->vqs[i].avail[0] = proxy->vqs[i].avail[1] = 0;
+        proxy->vqs[i].used[0] = proxy->vqs[i].used[1] = 0;
+    }
+
+    if (pci_is_express(dev)) {
+        pcie_cap_deverr_reset(dev);
+        pcie_cap_lnkctl_reset(dev);
+
+        pci_set_word(dev->config + dev->exp.pm_cap + PCI_PM_CTRL, 0);
+    }
+}
+
+static Property virtio_pci_properties[] = {
+    DEFINE_PROP_BIT("virtio-pci-bus-master-bug-migration", VirtIOPCIProxy, flags,
+                    VIRTIO_PCI_FLAG_BUS_MASTER_BUG_MIGRATION_BIT, false),
+    DEFINE_PROP_ON_OFF_AUTO("disable-legacy", VirtIOPCIProxy, disable_legacy,
+                            ON_OFF_AUTO_AUTO),
+    DEFINE_PROP_BOOL("disable-modern", VirtIOPCIProxy, disable_modern, false),
+    DEFINE_PROP_BIT("migrate-extra", VirtIOPCIProxy, flags,
+                    VIRTIO_PCI_FLAG_MIGRATE_EXTRA_BIT, true),
+    DEFINE_PROP_BIT("modern-pio-notify", VirtIOPCIProxy, flags,
+                    VIRTIO_PCI_FLAG_MODERN_PIO_NOTIFY_BIT, false),
+    DEFINE_PROP_BIT("x-disable-pcie", VirtIOPCIProxy, flags,
+                    VIRTIO_PCI_FLAG_DISABLE_PCIE_BIT, false),
+    DEFINE_PROP_BIT("page-per-vq", VirtIOPCIProxy, flags,
+                    VIRTIO_PCI_FLAG_PAGE_PER_VQ_BIT, false),
+    DEFINE_PROP_BOOL("x-ignore-backend-features", VirtIOPCIProxy,
+                     ignore_backend_features, false),
+    DEFINE_PROP_BIT("ats", VirtIOPCIProxy, flags,
+                    VIRTIO_PCI_FLAG_ATS_BIT, false),
+    DEFINE_PROP_BIT("x-pcie-deverr-init", VirtIOPCIProxy, flags,
+                    VIRTIO_PCI_FLAG_INIT_DEVERR_BIT, true),
+    DEFINE_PROP_BIT("x-pcie-lnkctl-init", VirtIOPCIProxy, flags,
+                    VIRTIO_PCI_FLAG_INIT_LNKCTL_BIT, true),
+    DEFINE_PROP_BIT("x-pcie-pm-init", VirtIOPCIProxy, flags,
+                    VIRTIO_PCI_FLAG_INIT_PM_BIT, true),
+    DEFINE_PROP_END_OF_LIST(),
+};
+
+static void virtio_pci_dc_realize(DeviceState *qdev, Error **errp)
+{
+    VirtioPCIClass *vpciklass = VIRTIO_PCI_GET_CLASS(qdev);
+    VirtIOPCIProxy *proxy = VIRTIO_PCI(qdev);
+    PCIDevice *pci_dev = &proxy->pci_dev;
+
+    if (!(proxy->flags & VIRTIO_PCI_FLAG_DISABLE_PCIE) &&
+        virtio_pci_modern(proxy)) {
+        pci_dev->cap_present |= QEMU_PCI_CAP_EXPRESS;
+    }
+
+    vpciklass->parent_dc_realize(qdev, errp);
+}
+
+static void virtio_pci_class_init(ObjectClass *klass, void *data)
+{
+    DeviceClass *dc = DEVICE_CLASS(klass);
+    PCIDeviceClass *k = PCI_DEVICE_CLASS(klass);
+    VirtioPCIClass *vpciklass = VIRTIO_PCI_CLASS(klass);
+
+    dc->props = virtio_pci_properties;
+    k->realize = virtio_pci_realize;
+    k->exit = virtio_pci_exit;
+    k->vendor_id = PCI_VENDOR_ID_REDHAT_QUMRANET;
+    k->revision = VIRTIO_PCI_ABI_VERSION;
+    k->class_id = PCI_CLASS_OTHERS;
+    vpciklass->parent_dc_realize = dc->realize;
+    dc->realize = virtio_pci_dc_realize;
+    dc->reset = virtio_pci_reset;
+}
+
+static const TypeInfo virtio_pci_info = {
+    .name          = TYPE_VIRTIO_PCI,
+    .parent        = TYPE_PCI_DEVICE,
+    .instance_size = sizeof(VirtIOPCIProxy),
+    .class_init    = virtio_pci_class_init,
+    .class_size    = sizeof(VirtioPCIClass),
+    .abstract      = true,
+};
+
+/* virtio-blk-pci */
+
+static Property virtio_blk_pci_properties[] = {
+    DEFINE_PROP_UINT32("class", VirtIOPCIProxy, class_code, 0),
+    DEFINE_PROP_BIT("ioeventfd", VirtIOPCIProxy, flags,
+                    VIRTIO_PCI_FLAG_USE_IOEVENTFD_BIT, true),
+    DEFINE_PROP_UINT32("vectors", VirtIOPCIProxy, nvectors, 2),
+    DEFINE_PROP_END_OF_LIST(),
+};
+
+static void virtio_blk_pci_realize(VirtIOPCIProxy *vpci_dev, Error **errp)
+{
+    VirtIOBlkPCI *dev = VIRTIO_BLK_PCI(vpci_dev);
+    DeviceState *vdev = DEVICE(&dev->vdev);
+
+    qdev_set_parent_bus(vdev, BUS(&vpci_dev->bus));
+    object_property_set_bool(OBJECT(vdev), true, "realized", errp);
+}
+
+static void virtio_blk_pci_class_init(ObjectClass *klass, void *data)
+{
+    DeviceClass *dc = DEVICE_CLASS(klass);
+    VirtioPCIClass *k = VIRTIO_PCI_CLASS(klass);
+    PCIDeviceClass *pcidev_k = PCI_DEVICE_CLASS(klass);
+
+    set_bit(DEVICE_CATEGORY_STORAGE, dc->categories);
+    dc->props = virtio_blk_pci_properties;
+    k->realize = virtio_blk_pci_realize;
+    pcidev_k->vendor_id = PCI_VENDOR_ID_REDHAT_QUMRANET;
+    pcidev_k->device_id = PCI_DEVICE_ID_VIRTIO_BLOCK;
+    pcidev_k->revision = VIRTIO_PCI_ABI_VERSION;
+    pcidev_k->class_id = PCI_CLASS_STORAGE_SCSI;
+}
+
+static void virtio_blk_pci_instance_init(Object *obj)
+{
+    VirtIOBlkPCI *dev = VIRTIO_BLK_PCI(obj);
+
+    virtio_instance_init_common(obj, &dev->vdev, sizeof(dev->vdev),
+                                TYPE_VIRTIO_BLK);
+    object_property_add_alias(obj, "iothread", OBJECT(&dev->vdev),"iothread",
+                              &error_abort);
+    object_property_add_alias(obj, "bootindex", OBJECT(&dev->vdev),
+                              "bootindex", &error_abort);
+}
+
+static const TypeInfo virtio_blk_pci_info = {
+    .name          = TYPE_VIRTIO_BLK_PCI,
+    .parent        = TYPE_VIRTIO_PCI,
+    .instance_size = sizeof(VirtIOBlkPCI),
+    .instance_init = virtio_blk_pci_instance_init,
+    .class_init    = virtio_blk_pci_class_init,
+};
+
+/* virtio-scsi-pci */
+
+static Property virtio_scsi_pci_properties[] = {
+    DEFINE_PROP_BIT("ioeventfd", VirtIOPCIProxy, flags,
+                    VIRTIO_PCI_FLAG_USE_IOEVENTFD_BIT, true),
+    DEFINE_PROP_UINT32("vectors", VirtIOPCIProxy, nvectors,
+                       DEV_NVECTORS_UNSPECIFIED),
+    DEFINE_PROP_END_OF_LIST(),
+};
+
+static void virtio_scsi_pci_realize(VirtIOPCIProxy *vpci_dev, Error **errp)
+{
+    VirtIOSCSIPCI *dev = VIRTIO_SCSI_PCI(vpci_dev);
+    DeviceState *vdev = DEVICE(&dev->vdev);
+    VirtIOSCSICommon *vs = VIRTIO_SCSI_COMMON(vdev);
+    DeviceState *proxy = DEVICE(vpci_dev);
+    char *bus_name;
+
+    if (vpci_dev->nvectors == DEV_NVECTORS_UNSPECIFIED) {
+        vpci_dev->nvectors = vs->conf.num_queues + 3;
+    }
+
+    /*
+     * For command line compatibility, this sets the virtio-scsi-device bus
+     * name as before.
+     */
+    if (proxy->id) {
+        bus_name = g_strdup_printf("%s.0", proxy->id);
+        virtio_device_set_child_bus_name(VIRTIO_DEVICE(vdev), bus_name);
+        g_free(bus_name);
+    }
+
+    qdev_set_parent_bus(vdev, BUS(&vpci_dev->bus));
+    object_property_set_bool(OBJECT(vdev), true, "realized", errp);
+}
+
+static void virtio_scsi_pci_class_init(ObjectClass *klass, void *data)
+{
+    DeviceClass *dc = DEVICE_CLASS(klass);
+    VirtioPCIClass *k = VIRTIO_PCI_CLASS(klass);
+    PCIDeviceClass *pcidev_k = PCI_DEVICE_CLASS(klass);
+
+    k->realize = virtio_scsi_pci_realize;
+    set_bit(DEVICE_CATEGORY_STORAGE, dc->categories);
+    dc->props = virtio_scsi_pci_properties;
+    pcidev_k->vendor_id = PCI_VENDOR_ID_REDHAT_QUMRANET;
+    pcidev_k->device_id = PCI_DEVICE_ID_VIRTIO_SCSI;
+    pcidev_k->revision = 0x00;
+    pcidev_k->class_id = PCI_CLASS_STORAGE_SCSI;
+}
+
+static void virtio_scsi_pci_instance_init(Object *obj)
+{
+    VirtIOSCSIPCI *dev = VIRTIO_SCSI_PCI(obj);
+
+    virtio_instance_init_common(obj, &dev->vdev, sizeof(dev->vdev),
+                                TYPE_VIRTIO_SCSI);
+    object_property_add_alias(obj, "iothread", OBJECT(&dev->vdev), "iothread",
+                              &error_abort);
+}
+
+static const TypeInfo virtio_scsi_pci_info = {
+    .name          = TYPE_VIRTIO_SCSI_PCI,
+    .parent        = TYPE_VIRTIO_PCI,
+    .instance_size = sizeof(VirtIOSCSIPCI),
+    .instance_init = virtio_scsi_pci_instance_init,
+    .class_init    = virtio_scsi_pci_class_init,
+};
+
+/* vhost-scsi-pci */
+
+#ifdef CONFIG_VHOST_SCSI
+static Property vhost_scsi_pci_properties[] = {
+    DEFINE_PROP_UINT32("vectors", VirtIOPCIProxy, nvectors,
+                       DEV_NVECTORS_UNSPECIFIED),
+    DEFINE_PROP_END_OF_LIST(),
+};
+
+static void vhost_scsi_pci_realize(VirtIOPCIProxy *vpci_dev, Error **errp)
+{
+    VHostSCSIPCI *dev = VHOST_SCSI_PCI(vpci_dev);
+    DeviceState *vdev = DEVICE(&dev->vdev);
+    VirtIOSCSICommon *vs = VIRTIO_SCSI_COMMON(vdev);
+
+    if (vpci_dev->nvectors == DEV_NVECTORS_UNSPECIFIED) {
+        vpci_dev->nvectors = vs->conf.num_queues + 3;
+    }
+
+    qdev_set_parent_bus(vdev, BUS(&vpci_dev->bus));
+    object_property_set_bool(OBJECT(vdev), true, "realized", errp);
+}
+
+static void vhost_scsi_pci_class_init(ObjectClass *klass, void *data)
+{
+    DeviceClass *dc = DEVICE_CLASS(klass);
+    VirtioPCIClass *k = VIRTIO_PCI_CLASS(klass);
+    PCIDeviceClass *pcidev_k = PCI_DEVICE_CLASS(klass);
+    k->realize = vhost_scsi_pci_realize;
+    set_bit(DEVICE_CATEGORY_STORAGE, dc->categories);
+    dc->props = vhost_scsi_pci_properties;
+    pcidev_k->vendor_id = PCI_VENDOR_ID_REDHAT_QUMRANET;
+    pcidev_k->device_id = PCI_DEVICE_ID_VIRTIO_SCSI;
+    pcidev_k->revision = 0x00;
+    pcidev_k->class_id = PCI_CLASS_STORAGE_SCSI;
+}
+
+static void vhost_scsi_pci_instance_init(Object *obj)
+{
+    VHostSCSIPCI *dev = VHOST_SCSI_PCI(obj);
+
+    virtio_instance_init_common(obj, &dev->vdev, sizeof(dev->vdev),
+                                TYPE_VHOST_SCSI);
+    object_property_add_alias(obj, "bootindex", OBJECT(&dev->vdev),
+                              "bootindex", &error_abort);
+}
+
+static const TypeInfo vhost_scsi_pci_info = {
+    .name          = TYPE_VHOST_SCSI_PCI,
+    .parent        = TYPE_VIRTIO_PCI,
+    .instance_size = sizeof(VHostSCSIPCI),
+    .instance_init = vhost_scsi_pci_instance_init,
+    .class_init    = vhost_scsi_pci_class_init,
+};
+#endif
+
+/* vhost-vsock-pci */
+
+#ifdef CONFIG_VHOST_VSOCK
+static Property vhost_vsock_pci_properties[] = {
+    DEFINE_PROP_UINT32("vectors", VirtIOPCIProxy, nvectors, 3),
+    DEFINE_PROP_END_OF_LIST(),
+};
+
+static void vhost_vsock_pci_realize(VirtIOPCIProxy *vpci_dev, Error **errp)
+{
+    VHostVSockPCI *dev = VHOST_VSOCK_PCI(vpci_dev);
+    DeviceState *vdev = DEVICE(&dev->vdev);
+
+    qdev_set_parent_bus(vdev, BUS(&vpci_dev->bus));
+    object_property_set_bool(OBJECT(vdev), true, "realized", errp);
+}
+
+static void vhost_vsock_pci_class_init(ObjectClass *klass, void *data)
+{
+    DeviceClass *dc = DEVICE_CLASS(klass);
+    VirtioPCIClass *k = VIRTIO_PCI_CLASS(klass);
+    PCIDeviceClass *pcidev_k = PCI_DEVICE_CLASS(klass);
+    k->realize = vhost_vsock_pci_realize;
+    set_bit(DEVICE_CATEGORY_MISC, dc->categories);
+    dc->props = vhost_vsock_pci_properties;
+    pcidev_k->vendor_id = PCI_VENDOR_ID_REDHAT_QUMRANET;
+    pcidev_k->device_id = PCI_DEVICE_ID_VIRTIO_VSOCK;
+    pcidev_k->revision = 0x00;
+    pcidev_k->class_id = PCI_CLASS_COMMUNICATION_OTHER;
+}
+
+static void vhost_vsock_pci_instance_init(Object *obj)
+{
+    VHostVSockPCI *dev = VHOST_VSOCK_PCI(obj);
+
+    virtio_instance_init_common(obj, &dev->vdev, sizeof(dev->vdev),
+                                TYPE_VHOST_VSOCK);
+}
+
+static const TypeInfo vhost_vsock_pci_info = {
+    .name          = TYPE_VHOST_VSOCK_PCI,
+    .parent        = TYPE_VIRTIO_PCI,
+    .instance_size = sizeof(VHostVSockPCI),
+    .instance_init = vhost_vsock_pci_instance_init,
+    .class_init    = vhost_vsock_pci_class_init,
+};
+#endif
+
+/* virtio-balloon-pci */
+
+static Property virtio_balloon_pci_properties[] = {
+    DEFINE_PROP_UINT32("class", VirtIOPCIProxy, class_code, 0),
+    DEFINE_PROP_END_OF_LIST(),
+};
+
+static void virtio_balloon_pci_realize(VirtIOPCIProxy *vpci_dev, Error **errp)
+{
+    VirtIOBalloonPCI *dev = VIRTIO_BALLOON_PCI(vpci_dev);
+    DeviceState *vdev = DEVICE(&dev->vdev);
+
+    if (vpci_dev->class_code != PCI_CLASS_OTHERS &&
+        vpci_dev->class_code != PCI_CLASS_MEMORY_RAM) { /* qemu < 1.1 */
+        vpci_dev->class_code = PCI_CLASS_OTHERS;
+    }
+
+    qdev_set_parent_bus(vdev, BUS(&vpci_dev->bus));
+    object_property_set_bool(OBJECT(vdev), true, "realized", errp);
+}
+
+static void virtio_balloon_pci_class_init(ObjectClass *klass, void *data)
+{
+    DeviceClass *dc = DEVICE_CLASS(klass);
+    VirtioPCIClass *k = VIRTIO_PCI_CLASS(klass);
+    PCIDeviceClass *pcidev_k = PCI_DEVICE_CLASS(klass);
+    k->realize = virtio_balloon_pci_realize;
+    set_bit(DEVICE_CATEGORY_MISC, dc->categories);
+    dc->props = virtio_balloon_pci_properties;
+    pcidev_k->vendor_id = PCI_VENDOR_ID_REDHAT_QUMRANET;
+    pcidev_k->device_id = PCI_DEVICE_ID_VIRTIO_BALLOON;
+    pcidev_k->revision = VIRTIO_PCI_ABI_VERSION;
+    pcidev_k->class_id = PCI_CLASS_OTHERS;
+}
+
+static void virtio_balloon_pci_instance_init(Object *obj)
+{
+    VirtIOBalloonPCI *dev = VIRTIO_BALLOON_PCI(obj);
+
+    virtio_instance_init_common(obj, &dev->vdev, sizeof(dev->vdev),
+                                TYPE_VIRTIO_BALLOON);
+    object_property_add_alias(obj, "guest-stats", OBJECT(&dev->vdev),
+                                  "guest-stats", &error_abort);
+    object_property_add_alias(obj, "guest-stats-polling-interval",
+                              OBJECT(&dev->vdev),
+                              "guest-stats-polling-interval", &error_abort);
+}
+
+static const TypeInfo virtio_balloon_pci_info = {
+    .name          = TYPE_VIRTIO_BALLOON_PCI,
+    .parent        = TYPE_VIRTIO_PCI,
+    .instance_size = sizeof(VirtIOBalloonPCI),
+    .instance_init = virtio_balloon_pci_instance_init,
+    .class_init    = virtio_balloon_pci_class_init,
+};
+
+/* virtio-serial-pci */
+
+static void virtio_serial_pci_realize(VirtIOPCIProxy *vpci_dev, Error **errp)
+{
+    VirtIOSerialPCI *dev = VIRTIO_SERIAL_PCI(vpci_dev);
+    DeviceState *vdev = DEVICE(&dev->vdev);
+    DeviceState *proxy = DEVICE(vpci_dev);
+    char *bus_name;
+
+    if (vpci_dev->class_code != PCI_CLASS_COMMUNICATION_OTHER &&
+        vpci_dev->class_code != PCI_CLASS_DISPLAY_OTHER && /* qemu 0.10 */
+        vpci_dev->class_code != PCI_CLASS_OTHERS) {        /* qemu-kvm  */
+            vpci_dev->class_code = PCI_CLASS_COMMUNICATION_OTHER;
+    }
+
+    /* backwards-compatibility with machines that were created with
+       DEV_NVECTORS_UNSPECIFIED */
+    if (vpci_dev->nvectors == DEV_NVECTORS_UNSPECIFIED) {
+        vpci_dev->nvectors = dev->vdev.serial.max_virtserial_ports + 1;
+    }
+
+    /*
+     * For command line compatibility, this sets the virtio-serial-device bus
+     * name as before.
+     */
+    if (proxy->id) {
+        bus_name = g_strdup_printf("%s.0", proxy->id);
+        virtio_device_set_child_bus_name(VIRTIO_DEVICE(vdev), bus_name);
+        g_free(bus_name);
+    }
+
+    qdev_set_parent_bus(vdev, BUS(&vpci_dev->bus));
+    object_property_set_bool(OBJECT(vdev), true, "realized", errp);
+}
+
+static Property virtio_serial_pci_properties[] = {
+    DEFINE_PROP_BIT("ioeventfd", VirtIOPCIProxy, flags,
+                    VIRTIO_PCI_FLAG_USE_IOEVENTFD_BIT, true),
+    DEFINE_PROP_UINT32("vectors", VirtIOPCIProxy, nvectors, 2),
+    DEFINE_PROP_UINT32("class", VirtIOPCIProxy, class_code, 0),
+    DEFINE_PROP_END_OF_LIST(),
+};
+
+static void virtio_serial_pci_class_init(ObjectClass *klass, void *data)
+{
+    DeviceClass *dc = DEVICE_CLASS(klass);
+    VirtioPCIClass *k = VIRTIO_PCI_CLASS(klass);
+    PCIDeviceClass *pcidev_k = PCI_DEVICE_CLASS(klass);
+    k->realize = virtio_serial_pci_realize;
+    set_bit(DEVICE_CATEGORY_INPUT, dc->categories);
+    dc->props = virtio_serial_pci_properties;
+    pcidev_k->vendor_id = PCI_VENDOR_ID_REDHAT_QUMRANET;
+    pcidev_k->device_id = PCI_DEVICE_ID_VIRTIO_CONSOLE;
+    pcidev_k->revision = VIRTIO_PCI_ABI_VERSION;
+    pcidev_k->class_id = PCI_CLASS_COMMUNICATION_OTHER;
+}
+
+static void virtio_serial_pci_instance_init(Object *obj)
+{
+    VirtIOSerialPCI *dev = VIRTIO_SERIAL_PCI(obj);
+
+    virtio_instance_init_common(obj, &dev->vdev, sizeof(dev->vdev),
+                                TYPE_VIRTIO_SERIAL);
+}
+
+static const TypeInfo virtio_serial_pci_info = {
+    .name          = TYPE_VIRTIO_SERIAL_PCI,
+    .parent        = TYPE_VIRTIO_PCI,
+    .instance_size = sizeof(VirtIOSerialPCI),
+    .instance_init = virtio_serial_pci_instance_init,
+    .class_init    = virtio_serial_pci_class_init,
+};
+
+/* virtio-net-pci */
+
+static Property virtio_net_properties[] = {
+    DEFINE_PROP_BIT("ioeventfd", VirtIOPCIProxy, flags,
+                    VIRTIO_PCI_FLAG_USE_IOEVENTFD_BIT, true),
+    DEFINE_PROP_UINT32("vectors", VirtIOPCIProxy, nvectors, 3),
+    DEFINE_PROP_END_OF_LIST(),
+};
+
+static void virtio_net_pci_realize(VirtIOPCIProxy *vpci_dev, Error **errp)
+{
+    DeviceState *qdev = DEVICE(vpci_dev);
+    VirtIONetPCI *dev = VIRTIO_NET_PCI(vpci_dev);
+    DeviceState *vdev = DEVICE(&dev->vdev);
+
+    virtio_net_set_netclient_name(&dev->vdev, qdev->id,
+                                  object_get_typename(OBJECT(qdev)));
+    qdev_set_parent_bus(vdev, BUS(&vpci_dev->bus));
+    object_property_set_bool(OBJECT(vdev), true, "realized", errp);
+}
+
+static void virtio_net_pci_class_init(ObjectClass *klass, void *data)
+{
+    DeviceClass *dc = DEVICE_CLASS(klass);
+    PCIDeviceClass *k = PCI_DEVICE_CLASS(klass);
+    VirtioPCIClass *vpciklass = VIRTIO_PCI_CLASS(klass);
+
+    k->romfile = "efi-virtio.rom";
+    k->vendor_id = PCI_VENDOR_ID_REDHAT_QUMRANET;
+    k->device_id = PCI_DEVICE_ID_VIRTIO_NET;
+    k->revision = VIRTIO_PCI_ABI_VERSION;
+    k->class_id = PCI_CLASS_NETWORK_ETHERNET;
+    set_bit(DEVICE_CATEGORY_NETWORK, dc->categories);
+    dc->props = virtio_net_properties;
+    vpciklass->realize = virtio_net_pci_realize;
+}
+
+static void virtio_net_pci_instance_init(Object *obj)
+{
+    VirtIONetPCI *dev = VIRTIO_NET_PCI(obj);
+
+    virtio_instance_init_common(obj, &dev->vdev, sizeof(dev->vdev),
+                                TYPE_VIRTIO_NET);
+    object_property_add_alias(obj, "bootindex", OBJECT(&dev->vdev),
+                              "bootindex", &error_abort);
+}
+
+static const TypeInfo virtio_net_pci_info = {
+    .name          = TYPE_VIRTIO_NET_PCI,
+    .parent        = TYPE_VIRTIO_PCI,
+    .instance_size = sizeof(VirtIONetPCI),
+    .instance_init = virtio_net_pci_instance_init,
+    .class_init    = virtio_net_pci_class_init,
+};
+
+/* virtio-rng-pci */
+
+static void virtio_rng_pci_realize(VirtIOPCIProxy *vpci_dev, Error **errp)
+{
+    VirtIORngPCI *vrng = VIRTIO_RNG_PCI(vpci_dev);
+    DeviceState *vdev = DEVICE(&vrng->vdev);
+    Error *err = NULL;
+
+    qdev_set_parent_bus(vdev, BUS(&vpci_dev->bus));
+    object_property_set_bool(OBJECT(vdev), true, "realized", &err);
+    if (err) {
+        error_propagate(errp, err);
+        return;
+    }
+
+    object_property_set_link(OBJECT(vrng),
+                             OBJECT(vrng->vdev.conf.rng), "rng",
+                             NULL);
+}
+
+static void virtio_rng_pci_class_init(ObjectClass *klass, void *data)
+{
+    DeviceClass *dc = DEVICE_CLASS(klass);
+    VirtioPCIClass *k = VIRTIO_PCI_CLASS(klass);
+    PCIDeviceClass *pcidev_k = PCI_DEVICE_CLASS(klass);
+
+    k->realize = virtio_rng_pci_realize;
+    set_bit(DEVICE_CATEGORY_MISC, dc->categories);
+
+    pcidev_k->vendor_id = PCI_VENDOR_ID_REDHAT_QUMRANET;
+    pcidev_k->device_id = PCI_DEVICE_ID_VIRTIO_RNG;
+    pcidev_k->revision = VIRTIO_PCI_ABI_VERSION;
+    pcidev_k->class_id = PCI_CLASS_OTHERS;
+}
+
+static void virtio_rng_initfn(Object *obj)
+{
+    VirtIORngPCI *dev = VIRTIO_RNG_PCI(obj);
+
+    virtio_instance_init_common(obj, &dev->vdev, sizeof(dev->vdev),
+                                TYPE_VIRTIO_RNG);
+    object_property_add_alias(obj, "rng", OBJECT(&dev->vdev), "rng",
+                              &error_abort);
+}
+
+static const TypeInfo virtio_rng_pci_info = {
+    .name          = TYPE_VIRTIO_RNG_PCI,
+    .parent        = TYPE_VIRTIO_PCI,
+    .instance_size = sizeof(VirtIORngPCI),
+    .instance_init = virtio_rng_initfn,
+    .class_init    = virtio_rng_pci_class_init,
+};
+
+/* virtio-input-pci */
+
+static Property virtio_input_pci_properties[] = {
+    DEFINE_PROP_UINT32("vectors", VirtIOPCIProxy, nvectors, 2),
+    DEFINE_PROP_END_OF_LIST(),
+};
+
+static void virtio_input_pci_realize(VirtIOPCIProxy *vpci_dev, Error **errp)
+{
+    VirtIOInputPCI *vinput = VIRTIO_INPUT_PCI(vpci_dev);
+    DeviceState *vdev = DEVICE(&vinput->vdev);
+
+    qdev_set_parent_bus(vdev, BUS(&vpci_dev->bus));
+    virtio_pci_force_virtio_1(vpci_dev);
+    object_property_set_bool(OBJECT(vdev), true, "realized", errp);
+}
+
+static void virtio_input_pci_class_init(ObjectClass *klass, void *data)
+{
+    DeviceClass *dc = DEVICE_CLASS(klass);
+    VirtioPCIClass *k = VIRTIO_PCI_CLASS(klass);
+    PCIDeviceClass *pcidev_k = PCI_DEVICE_CLASS(klass);
+
+    dc->props = virtio_input_pci_properties;
+    k->realize = virtio_input_pci_realize;
+    set_bit(DEVICE_CATEGORY_INPUT, dc->categories);
+
+    pcidev_k->class_id = PCI_CLASS_INPUT_OTHER;
+}
+
+static void virtio_input_hid_kbd_pci_class_init(ObjectClass *klass, void *data)
+{
+    PCIDeviceClass *pcidev_k = PCI_DEVICE_CLASS(klass);
+
+    pcidev_k->class_id = PCI_CLASS_INPUT_KEYBOARD;
+}
+
+static void virtio_input_hid_mouse_pci_class_init(ObjectClass *klass,
+                                                  void *data)
+{
+    PCIDeviceClass *pcidev_k = PCI_DEVICE_CLASS(klass);
+
+    pcidev_k->class_id = PCI_CLASS_INPUT_MOUSE;
+}
+
+static void virtio_keyboard_initfn(Object *obj)
+{
+    VirtIOInputHIDPCI *dev = VIRTIO_INPUT_HID_PCI(obj);
+
+    virtio_instance_init_common(obj, &dev->vdev, sizeof(dev->vdev),
+                                TYPE_VIRTIO_KEYBOARD);
+}
+
+static void virtio_mouse_initfn(Object *obj)
+{
+    VirtIOInputHIDPCI *dev = VIRTIO_INPUT_HID_PCI(obj);
+
+    virtio_instance_init_common(obj, &dev->vdev, sizeof(dev->vdev),
+                                TYPE_VIRTIO_MOUSE);
+}
+
+static void virtio_tablet_initfn(Object *obj)
+{
+    VirtIOInputHIDPCI *dev = VIRTIO_INPUT_HID_PCI(obj);
+
+    virtio_instance_init_common(obj, &dev->vdev, sizeof(dev->vdev),
+                                TYPE_VIRTIO_TABLET);
+}
+
+static const TypeInfo virtio_input_pci_info = {
+    .name          = TYPE_VIRTIO_INPUT_PCI,
+    .parent        = TYPE_VIRTIO_PCI,
+    .instance_size = sizeof(VirtIOInputPCI),
+    .class_init    = virtio_input_pci_class_init,
+    .abstract      = true,
+};
+
+static const TypeInfo virtio_input_hid_pci_info = {
+    .name          = TYPE_VIRTIO_INPUT_HID_PCI,
+    .parent        = TYPE_VIRTIO_INPUT_PCI,
+    .instance_size = sizeof(VirtIOInputHIDPCI),
+    .abstract      = true,
+};
+
+static const TypeInfo virtio_keyboard_pci_info = {
+    .name          = TYPE_VIRTIO_KEYBOARD_PCI,
+    .parent        = TYPE_VIRTIO_INPUT_HID_PCI,
+    .class_init    = virtio_input_hid_kbd_pci_class_init,
+    .instance_size = sizeof(VirtIOInputHIDPCI),
+    .instance_init = virtio_keyboard_initfn,
+};
+
+static const TypeInfo virtio_mouse_pci_info = {
+    .name          = TYPE_VIRTIO_MOUSE_PCI,
+    .parent        = TYPE_VIRTIO_INPUT_HID_PCI,
+    .class_init    = virtio_input_hid_mouse_pci_class_init,
+    .instance_size = sizeof(VirtIOInputHIDPCI),
+    .instance_init = virtio_mouse_initfn,
+};
+
+static const TypeInfo virtio_tablet_pci_info = {
+    .name          = TYPE_VIRTIO_TABLET_PCI,
+    .parent        = TYPE_VIRTIO_INPUT_HID_PCI,
+    .instance_size = sizeof(VirtIOInputHIDPCI),
+    .instance_init = virtio_tablet_initfn,
+};
+
+#ifdef CONFIG_LINUX
+static void virtio_host_initfn(Object *obj)
+{
+    VirtIOInputHostPCI *dev = VIRTIO_INPUT_HOST_PCI(obj);
+
+    virtio_instance_init_common(obj, &dev->vdev, sizeof(dev->vdev),
+                                TYPE_VIRTIO_INPUT_HOST);
+}
+
+static const TypeInfo virtio_host_pci_info = {
+    .name          = TYPE_VIRTIO_INPUT_HOST_PCI,
+    .parent        = TYPE_VIRTIO_INPUT_PCI,
+    .instance_size = sizeof(VirtIOInputHostPCI),
+    .instance_init = virtio_host_initfn,
+};
+#endif
+
+/* virtio-pci-bus */
+
+static void virtio_pci_bus_new(VirtioBusState *bus, size_t bus_size,
+                               VirtIOPCIProxy *dev)
+{
+    DeviceState *qdev = DEVICE(dev);
+    char virtio_bus_name[] = "virtio-bus";
+
+    qbus_create_inplace(bus, bus_size, TYPE_VIRTIO_PCI_BUS, qdev,
+                        virtio_bus_name);
+}
+
+static void virtio_pci_bus_class_init(ObjectClass *klass, void *data)
+{
+    BusClass *bus_class = BUS_CLASS(klass);
+    VirtioBusClass *k = VIRTIO_BUS_CLASS(klass);
+    bus_class->max_dev = 1;
+    k->notify = virtio_pci_notify;
+    k->save_config = virtio_pci_save_config;
+    k->load_config = virtio_pci_load_config;
+    k->save_queue = virtio_pci_save_queue;
+    k->load_queue = virtio_pci_load_queue;
+    k->save_extra_state = virtio_pci_save_extra_state;
+    k->load_extra_state = virtio_pci_load_extra_state;
+    k->has_extra_state = virtio_pci_has_extra_state;
+    k->query_guest_notifiers = virtio_pci_query_guest_notifiers;
+    k->set_guest_notifiers = virtio_pci_set_guest_notifiers;
+    k->vmstate_change = virtio_pci_vmstate_change;
+    k->pre_plugged = virtio_pci_pre_plugged;
+    k->device_plugged = virtio_pci_device_plugged;
+    k->device_unplugged = virtio_pci_device_unplugged;
+    k->query_nvectors = virtio_pci_query_nvectors;
+    k->ioeventfd_enabled = virtio_pci_ioeventfd_enabled;
+    k->ioeventfd_assign = virtio_pci_ioeventfd_assign;
+    k->get_dma_as = virtio_pci_get_dma_as;
+}
+
+static const TypeInfo virtio_pci_bus_info = {
+    .name          = TYPE_VIRTIO_PCI_BUS,
+    .parent        = TYPE_VIRTIO_BUS,
+    .instance_size = sizeof(VirtioPCIBusState),
+    .class_init    = virtio_pci_bus_class_init,
+};
+
+static void virtio_pci_register_types(void)
+{
+    type_register_static(&virtio_rng_pci_info);
+    type_register_static(&virtio_input_pci_info);
+    type_register_static(&virtio_input_hid_pci_info);
+    type_register_static(&virtio_keyboard_pci_info);
+    type_register_static(&virtio_mouse_pci_info);
+    type_register_static(&virtio_tablet_pci_info);
+#ifdef CONFIG_LINUX
+    type_register_static(&virtio_host_pci_info);
+#endif
+    type_register_static(&virtio_pci_bus_info);
+    type_register_static(&virtio_pci_info);
+#ifdef CONFIG_VIRTFS
+    type_register_static(&virtio_9p_pci_info);
+#endif
+    type_register_static(&virtio_blk_pci_info);
+    type_register_static(&virtio_scsi_pci_info);
+    type_register_static(&virtio_balloon_pci_info);
+    type_register_static(&virtio_serial_pci_info);
+    type_register_static(&virtio_net_pci_info);
+#ifdef CONFIG_VHOST_SCSI
+    type_register_static(&vhost_scsi_pci_info);
+#endif
+#ifdef CONFIG_VHOST_VSOCK
+    type_register_static(&vhost_vsock_pci_info);
+#endif
+}
+
+type_init(virtio_pci_register_types)
diff -rupN /home/prafull/Desktop/qemu-2.9.0/hw/virtio/virtio-pci.h /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/virtio/virtio-pci.h
--- /home/prafull/Desktop/qemu-2.9.0/hw/virtio/virtio-pci.h	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/virtio/virtio-pci.h	2018-05-28 13:06:18.000000000 +0530
@@ -26,6 +26,9 @@
 #include "hw/virtio/virtio-input.h"
 #include "hw/virtio/virtio-gpu.h"
 #include "hw/virtio/virtio-crypto.h"
+/* Added by Bhavesh Singh. 2017.02.03. Begin add */
+#include "hw/virtio/virtio-vssd.h"
+/* Added by Bhavesh Singh. 2017.02.03. End add */
 
 #ifdef CONFIG_VIRTFS
 #include "hw/9pfs/virtio-9p.h"
@@ -51,6 +54,9 @@ typedef struct VirtIOInputHostPCI VirtIO
 typedef struct VirtIOGPUPCI VirtIOGPUPCI;
 typedef struct VHostVSockPCI VHostVSockPCI;
 typedef struct VirtIOCryptoPCI VirtIOCryptoPCI;
+/* Added by Bhavesh Singh. 2017.02.03. Begin add */
+typedef struct VirtIOVssdPCI VirtIOVssdPCI;
+/* Added by Bhavesh Singh. 2017.02.03. End add */
 
 /* virtio-pci-bus */
 
@@ -382,6 +388,20 @@ struct VirtIOCryptoPCI {
     VirtIOCrypto vdev;
 };
 
+/* Added by Bhavesh Singh. 2017.02.03. Begin add */
+/*
+ * virtio-vssd-pci: This extends VirtioPCIProxy.
+ */
+#define TYPE_VIRTIO_VSSD_PCI "virtio-vssd-pci"
+#define VIRTIO_VSSD_PCI(obj) \
+    OBJECT_CHECK(VirtIOVssdPCI, (obj), TYPE_VIRTIO_VSSD_PCI)
+
+struct VirtIOVssdPCI {
+    VirtIOPCIProxy parent_obj;
+    VirtIOVssd vdev;
+};
+/* Added by Bhavesh Singh. 2017.02.03. End add */
+
 /* Virtio ABI version, if we increment this, we break the guest driver. */
 #define VIRTIO_PCI_ABI_VERSION          0
 
diff -rupN /home/prafull/Desktop/qemu-2.9.0/hw/virtio/virtio-pci.h.orig /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/virtio/virtio-pci.h.orig
--- /home/prafull/Desktop/qemu-2.9.0/hw/virtio/virtio-pci.h.orig	1970-01-01 05:30:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/hw/virtio/virtio-pci.h.orig	2018-05-28 13:06:18.000000000 +0530
@@ -0,0 +1,388 @@
+/*
+ * Virtio PCI Bindings
+ *
+ * Copyright IBM, Corp. 2007
+ * Copyright (c) 2009 CodeSourcery
+ *
+ * Authors:
+ *  Anthony Liguori   <aliguori@us.ibm.com>
+ *  Paul Brook        <paul@codesourcery.com>
+ *
+ * This work is licensed under the terms of the GNU GPL, version 2.  See
+ * the COPYING file in the top-level directory.
+ */
+
+#ifndef QEMU_VIRTIO_PCI_H
+#define QEMU_VIRTIO_PCI_H
+
+#include "hw/pci/msi.h"
+#include "hw/virtio/virtio-blk.h"
+#include "hw/virtio/virtio-net.h"
+#include "hw/virtio/virtio-rng.h"
+#include "hw/virtio/virtio-serial.h"
+#include "hw/virtio/virtio-scsi.h"
+#include "hw/virtio/virtio-balloon.h"
+#include "hw/virtio/virtio-bus.h"
+#include "hw/virtio/virtio-input.h"
+#include "hw/virtio/virtio-gpu.h"
+#include "hw/virtio/virtio-crypto.h"
+
+#ifdef CONFIG_VIRTFS
+#include "hw/9pfs/virtio-9p.h"
+#endif
+#ifdef CONFIG_VHOST_SCSI
+#include "hw/virtio/vhost-scsi.h"
+#endif
+#ifdef CONFIG_VHOST_VSOCK
+#include "hw/virtio/vhost-vsock.h"
+#endif
+
+typedef struct VirtIOPCIProxy VirtIOPCIProxy;
+typedef struct VirtIOBlkPCI VirtIOBlkPCI;
+typedef struct VirtIOSCSIPCI VirtIOSCSIPCI;
+typedef struct VirtIOBalloonPCI VirtIOBalloonPCI;
+typedef struct VirtIOSerialPCI VirtIOSerialPCI;
+typedef struct VirtIONetPCI VirtIONetPCI;
+typedef struct VHostSCSIPCI VHostSCSIPCI;
+typedef struct VirtIORngPCI VirtIORngPCI;
+typedef struct VirtIOInputPCI VirtIOInputPCI;
+typedef struct VirtIOInputHIDPCI VirtIOInputHIDPCI;
+typedef struct VirtIOInputHostPCI VirtIOInputHostPCI;
+typedef struct VirtIOGPUPCI VirtIOGPUPCI;
+typedef struct VHostVSockPCI VHostVSockPCI;
+typedef struct VirtIOCryptoPCI VirtIOCryptoPCI;
+
+/* virtio-pci-bus */
+
+typedef struct VirtioBusState VirtioPCIBusState;
+typedef struct VirtioBusClass VirtioPCIBusClass;
+
+#define TYPE_VIRTIO_PCI_BUS "virtio-pci-bus"
+#define VIRTIO_PCI_BUS(obj) \
+        OBJECT_CHECK(VirtioPCIBusState, (obj), TYPE_VIRTIO_PCI_BUS)
+#define VIRTIO_PCI_BUS_GET_CLASS(obj) \
+        OBJECT_GET_CLASS(VirtioPCIBusClass, obj, TYPE_VIRTIO_PCI_BUS)
+#define VIRTIO_PCI_BUS_CLASS(klass) \
+        OBJECT_CLASS_CHECK(VirtioPCIBusClass, klass, TYPE_VIRTIO_PCI_BUS)
+
+enum {
+    VIRTIO_PCI_FLAG_BUS_MASTER_BUG_MIGRATION_BIT,
+    VIRTIO_PCI_FLAG_USE_IOEVENTFD_BIT,
+    VIRTIO_PCI_FLAG_MIGRATE_EXTRA_BIT,
+    VIRTIO_PCI_FLAG_MODERN_PIO_NOTIFY_BIT,
+    VIRTIO_PCI_FLAG_DISABLE_PCIE_BIT,
+    VIRTIO_PCI_FLAG_PAGE_PER_VQ_BIT,
+    VIRTIO_PCI_FLAG_ATS_BIT,
+    VIRTIO_PCI_FLAG_INIT_DEVERR_BIT,
+    VIRTIO_PCI_FLAG_INIT_LNKCTL_BIT,
+    VIRTIO_PCI_FLAG_INIT_PM_BIT,
+};
+
+/* Need to activate work-arounds for buggy guests at vmstate load. */
+#define VIRTIO_PCI_FLAG_BUS_MASTER_BUG_MIGRATION \
+    (1 << VIRTIO_PCI_FLAG_BUS_MASTER_BUG_MIGRATION_BIT)
+
+/* Performance improves when virtqueue kick processing is decoupled from the
+ * vcpu thread using ioeventfd for some devices. */
+#define VIRTIO_PCI_FLAG_USE_IOEVENTFD   (1 << VIRTIO_PCI_FLAG_USE_IOEVENTFD_BIT)
+
+/* virtio version flags */
+#define VIRTIO_PCI_FLAG_DISABLE_PCIE (1 << VIRTIO_PCI_FLAG_DISABLE_PCIE_BIT)
+
+/* migrate extra state */
+#define VIRTIO_PCI_FLAG_MIGRATE_EXTRA (1 << VIRTIO_PCI_FLAG_MIGRATE_EXTRA_BIT)
+
+/* have pio notification for modern device ? */
+#define VIRTIO_PCI_FLAG_MODERN_PIO_NOTIFY \
+    (1 << VIRTIO_PCI_FLAG_MODERN_PIO_NOTIFY_BIT)
+
+/* page per vq flag to be used by split drivers within guests */
+#define VIRTIO_PCI_FLAG_PAGE_PER_VQ \
+    (1 << VIRTIO_PCI_FLAG_PAGE_PER_VQ_BIT)
+
+/* address space translation service */
+#define VIRTIO_PCI_FLAG_ATS (1 << VIRTIO_PCI_FLAG_ATS_BIT)
+
+/* Init error enabling flags */
+#define VIRTIO_PCI_FLAG_INIT_DEVERR (1 << VIRTIO_PCI_FLAG_INIT_DEVERR_BIT)
+
+/* Init Link Control register */
+#define VIRTIO_PCI_FLAG_INIT_LNKCTL (1 << VIRTIO_PCI_FLAG_INIT_LNKCTL_BIT)
+
+/* Init Power Management */
+#define VIRTIO_PCI_FLAG_INIT_PM (1 << VIRTIO_PCI_FLAG_INIT_PM_BIT)
+
+typedef struct {
+    MSIMessage msg;
+    int virq;
+    unsigned int users;
+} VirtIOIRQFD;
+
+/*
+ * virtio-pci: This is the PCIDevice which has a virtio-pci-bus.
+ */
+#define TYPE_VIRTIO_PCI "virtio-pci"
+#define VIRTIO_PCI_GET_CLASS(obj) \
+        OBJECT_GET_CLASS(VirtioPCIClass, obj, TYPE_VIRTIO_PCI)
+#define VIRTIO_PCI_CLASS(klass) \
+        OBJECT_CLASS_CHECK(VirtioPCIClass, klass, TYPE_VIRTIO_PCI)
+#define VIRTIO_PCI(obj) \
+        OBJECT_CHECK(VirtIOPCIProxy, (obj), TYPE_VIRTIO_PCI)
+
+typedef struct VirtioPCIClass {
+    PCIDeviceClass parent_class;
+    DeviceRealize parent_dc_realize;
+    void (*realize)(VirtIOPCIProxy *vpci_dev, Error **errp);
+} VirtioPCIClass;
+
+typedef struct VirtIOPCIRegion {
+    MemoryRegion mr;
+    uint32_t offset;
+    uint32_t size;
+    uint32_t type;
+} VirtIOPCIRegion;
+
+typedef struct VirtIOPCIQueue {
+  uint16_t num;
+  bool enabled;
+  uint32_t desc[2];
+  uint32_t avail[2];
+  uint32_t used[2];
+} VirtIOPCIQueue;
+
+struct VirtIOPCIProxy {
+    PCIDevice pci_dev;
+    MemoryRegion bar;
+    VirtIOPCIRegion common;
+    VirtIOPCIRegion isr;
+    VirtIOPCIRegion device;
+    VirtIOPCIRegion notify;
+    VirtIOPCIRegion notify_pio;
+    MemoryRegion modern_bar;
+    MemoryRegion io_bar;
+    MemoryRegion modern_cfg;
+    AddressSpace modern_as;
+    uint32_t legacy_io_bar_idx;
+    uint32_t msix_bar_idx;
+    uint32_t modern_io_bar_idx;
+    uint32_t modern_mem_bar_idx;
+    int config_cap;
+    uint32_t flags;
+    bool disable_modern;
+    bool ignore_backend_features;
+    OnOffAuto disable_legacy;
+    uint32_t class_code;
+    uint32_t nvectors;
+    uint32_t dfselect;
+    uint32_t gfselect;
+    uint32_t guest_features[2];
+    VirtIOPCIQueue vqs[VIRTIO_QUEUE_MAX];
+
+    VirtIOIRQFD *vector_irqfd;
+    int nvqs_with_notifiers;
+    VirtioBusState bus;
+};
+
+static inline bool virtio_pci_modern(VirtIOPCIProxy *proxy)
+{
+    return !proxy->disable_modern;
+}
+
+static inline bool virtio_pci_legacy(VirtIOPCIProxy *proxy)
+{
+    return proxy->disable_legacy == ON_OFF_AUTO_OFF;
+}
+
+static inline void virtio_pci_force_virtio_1(VirtIOPCIProxy *proxy)
+{
+    proxy->disable_modern = false;
+    proxy->disable_legacy = ON_OFF_AUTO_ON;
+}
+
+static inline void virtio_pci_disable_modern(VirtIOPCIProxy *proxy)
+{
+    proxy->disable_modern = true;
+}
+
+/*
+ * virtio-scsi-pci: This extends VirtioPCIProxy.
+ */
+#define TYPE_VIRTIO_SCSI_PCI "virtio-scsi-pci"
+#define VIRTIO_SCSI_PCI(obj) \
+        OBJECT_CHECK(VirtIOSCSIPCI, (obj), TYPE_VIRTIO_SCSI_PCI)
+
+struct VirtIOSCSIPCI {
+    VirtIOPCIProxy parent_obj;
+    VirtIOSCSI vdev;
+};
+
+#ifdef CONFIG_VHOST_SCSI
+/*
+ * vhost-scsi-pci: This extends VirtioPCIProxy.
+ */
+#define TYPE_VHOST_SCSI_PCI "vhost-scsi-pci"
+#define VHOST_SCSI_PCI(obj) \
+        OBJECT_CHECK(VHostSCSIPCI, (obj), TYPE_VHOST_SCSI_PCI)
+
+struct VHostSCSIPCI {
+    VirtIOPCIProxy parent_obj;
+    VHostSCSI vdev;
+};
+#endif
+
+/*
+ * virtio-blk-pci: This extends VirtioPCIProxy.
+ */
+#define TYPE_VIRTIO_BLK_PCI "virtio-blk-pci"
+#define VIRTIO_BLK_PCI(obj) \
+        OBJECT_CHECK(VirtIOBlkPCI, (obj), TYPE_VIRTIO_BLK_PCI)
+
+struct VirtIOBlkPCI {
+    VirtIOPCIProxy parent_obj;
+    VirtIOBlock vdev;
+};
+
+/*
+ * virtio-balloon-pci: This extends VirtioPCIProxy.
+ */
+#define TYPE_VIRTIO_BALLOON_PCI "virtio-balloon-pci"
+#define VIRTIO_BALLOON_PCI(obj) \
+        OBJECT_CHECK(VirtIOBalloonPCI, (obj), TYPE_VIRTIO_BALLOON_PCI)
+
+struct VirtIOBalloonPCI {
+    VirtIOPCIProxy parent_obj;
+    VirtIOBalloon vdev;
+};
+
+/*
+ * virtio-serial-pci: This extends VirtioPCIProxy.
+ */
+#define TYPE_VIRTIO_SERIAL_PCI "virtio-serial-pci"
+#define VIRTIO_SERIAL_PCI(obj) \
+        OBJECT_CHECK(VirtIOSerialPCI, (obj), TYPE_VIRTIO_SERIAL_PCI)
+
+struct VirtIOSerialPCI {
+    VirtIOPCIProxy parent_obj;
+    VirtIOSerial vdev;
+};
+
+/*
+ * virtio-net-pci: This extends VirtioPCIProxy.
+ */
+#define TYPE_VIRTIO_NET_PCI "virtio-net-pci"
+#define VIRTIO_NET_PCI(obj) \
+        OBJECT_CHECK(VirtIONetPCI, (obj), TYPE_VIRTIO_NET_PCI)
+
+struct VirtIONetPCI {
+    VirtIOPCIProxy parent_obj;
+    VirtIONet vdev;
+};
+
+/*
+ * virtio-9p-pci: This extends VirtioPCIProxy.
+ */
+
+#ifdef CONFIG_VIRTFS
+
+#define TYPE_VIRTIO_9P_PCI "virtio-9p-pci"
+#define VIRTIO_9P_PCI(obj) \
+        OBJECT_CHECK(V9fsPCIState, (obj), TYPE_VIRTIO_9P_PCI)
+
+typedef struct V9fsPCIState {
+    VirtIOPCIProxy parent_obj;
+    V9fsVirtioState vdev;
+} V9fsPCIState;
+
+#endif
+
+/*
+ * virtio-rng-pci: This extends VirtioPCIProxy.
+ */
+#define TYPE_VIRTIO_RNG_PCI "virtio-rng-pci"
+#define VIRTIO_RNG_PCI(obj) \
+        OBJECT_CHECK(VirtIORngPCI, (obj), TYPE_VIRTIO_RNG_PCI)
+
+struct VirtIORngPCI {
+    VirtIOPCIProxy parent_obj;
+    VirtIORNG vdev;
+};
+
+/*
+ * virtio-input-pci: This extends VirtioPCIProxy.
+ */
+#define TYPE_VIRTIO_INPUT_PCI "virtio-input-pci"
+#define VIRTIO_INPUT_PCI(obj) \
+        OBJECT_CHECK(VirtIOInputPCI, (obj), TYPE_VIRTIO_INPUT_PCI)
+
+struct VirtIOInputPCI {
+    VirtIOPCIProxy parent_obj;
+    VirtIOInput vdev;
+};
+
+#define TYPE_VIRTIO_INPUT_HID_PCI "virtio-input-hid-pci"
+#define TYPE_VIRTIO_KEYBOARD_PCI  "virtio-keyboard-pci"
+#define TYPE_VIRTIO_MOUSE_PCI     "virtio-mouse-pci"
+#define TYPE_VIRTIO_TABLET_PCI    "virtio-tablet-pci"
+#define VIRTIO_INPUT_HID_PCI(obj) \
+        OBJECT_CHECK(VirtIOInputHIDPCI, (obj), TYPE_VIRTIO_INPUT_HID_PCI)
+
+struct VirtIOInputHIDPCI {
+    VirtIOPCIProxy parent_obj;
+    VirtIOInputHID vdev;
+};
+
+#ifdef CONFIG_LINUX
+
+#define TYPE_VIRTIO_INPUT_HOST_PCI "virtio-input-host-pci"
+#define VIRTIO_INPUT_HOST_PCI(obj) \
+        OBJECT_CHECK(VirtIOInputHostPCI, (obj), TYPE_VIRTIO_INPUT_HOST_PCI)
+
+struct VirtIOInputHostPCI {
+    VirtIOPCIProxy parent_obj;
+    VirtIOInputHost vdev;
+};
+
+#endif
+
+/*
+ * virtio-gpu-pci: This extends VirtioPCIProxy.
+ */
+#define TYPE_VIRTIO_GPU_PCI "virtio-gpu-pci"
+#define VIRTIO_GPU_PCI(obj) \
+        OBJECT_CHECK(VirtIOGPUPCI, (obj), TYPE_VIRTIO_GPU_PCI)
+
+struct VirtIOGPUPCI {
+    VirtIOPCIProxy parent_obj;
+    VirtIOGPU vdev;
+};
+
+#ifdef CONFIG_VHOST_VSOCK
+/*
+ * vhost-vsock-pci: This extends VirtioPCIProxy.
+ */
+#define TYPE_VHOST_VSOCK_PCI "vhost-vsock-pci"
+#define VHOST_VSOCK_PCI(obj) \
+        OBJECT_CHECK(VHostVSockPCI, (obj), TYPE_VHOST_VSOCK_PCI)
+
+struct VHostVSockPCI {
+    VirtIOPCIProxy parent_obj;
+    VHostVSock vdev;
+};
+#endif
+
+/*
+ * virtio-crypto-pci: This extends VirtioPCIProxy.
+ */
+#define TYPE_VIRTIO_CRYPTO_PCI "virtio-crypto-pci"
+#define VIRTIO_CRYPTO_PCI(obj) \
+        OBJECT_CHECK(VirtIOCryptoPCI, (obj), TYPE_VIRTIO_CRYPTO_PCI)
+
+struct VirtIOCryptoPCI {
+    VirtIOPCIProxy parent_obj;
+    VirtIOCrypto vdev;
+};
+
+/* Virtio ABI version, if we increment this, we break the guest driver. */
+#define VIRTIO_PCI_ABI_VERSION          0
+
+#endif
diff -rupN /home/prafull/Desktop/qemu-2.9.0/include/hw/pci/pci.h /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/hw/pci/pci.h
--- /home/prafull/Desktop/qemu-2.9.0/include/hw/pci/pci.h	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/hw/pci/pci.h	2018-05-28 13:06:18.000000000 +0530
@@ -84,6 +84,14 @@
 #define PCI_DEVICE_ID_VIRTIO_9P          0x1009
 #define PCI_DEVICE_ID_VIRTIO_VSOCK       0x1012
 
+/* Added by Bhavesh Singh. 2017.03.03. Begin add */
+/* This ID does not even matter, since ultimately, the
+ * linux kernel virtio frontend will replace this by the
+ * subdevice id, which is set to the VirtIO device id. */
+#define PCI_DEVICE_ID_VIRTIO_VSSD        0x1036
+/* Older Virtio device numbers started from 0x1041 */
+/* Added by Bhavesh Singh. 2017.03.03. End add */
+
 #define PCI_VENDOR_ID_REDHAT             0x1b36
 #define PCI_DEVICE_ID_REDHAT_BRIDGE      0x0001
 #define PCI_DEVICE_ID_REDHAT_SERIAL      0x0002
diff -rupN /home/prafull/Desktop/qemu-2.9.0/include/hw/virtio/vhost-backend.h /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/hw/virtio/vhost-backend.h
--- /home/prafull/Desktop/qemu-2.9.0/include/hw/virtio/vhost-backend.h	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/hw/virtio/vhost-backend.h	2018-05-28 13:06:18.000000000 +0530
@@ -14,10 +14,10 @@
 #include "exec/memory.h"
 
 typedef enum VhostBackendType {
-    VHOST_BACKEND_TYPE_NONE = 0,
-    VHOST_BACKEND_TYPE_KERNEL = 1,
-    VHOST_BACKEND_TYPE_USER = 2,
-    VHOST_BACKEND_TYPE_MAX = 3,
+	VHOST_BACKEND_TYPE_NONE = 0,
+	VHOST_BACKEND_TYPE_KERNEL = 1,
+	VHOST_BACKEND_TYPE_USER = 2,
+	VHOST_BACKEND_TYPE_MAX = 3,
 } VhostBackendType;
 
 struct vhost_dev;
@@ -33,100 +33,100 @@ typedef int (*vhost_backend_cleanup)(str
 typedef int (*vhost_backend_memslots_limit)(struct vhost_dev *dev);
 
 typedef int (*vhost_net_set_backend_op)(struct vhost_dev *dev,
-                                struct vhost_vring_file *file);
+		struct vhost_vring_file *file);
 typedef int (*vhost_net_set_mtu_op)(struct vhost_dev *dev, uint16_t mtu);
 typedef int (*vhost_scsi_set_endpoint_op)(struct vhost_dev *dev,
-                                  struct vhost_scsi_target *target);
+		struct vhost_scsi_target *target);
 typedef int (*vhost_scsi_clear_endpoint_op)(struct vhost_dev *dev,
-                                    struct vhost_scsi_target *target);
+		struct vhost_scsi_target *target);
 typedef int (*vhost_scsi_get_abi_version_op)(struct vhost_dev *dev,
-                                             int *version);
+		int *version);
 typedef int (*vhost_set_log_base_op)(struct vhost_dev *dev, uint64_t base,
-                                     struct vhost_log *log);
+		struct vhost_log *log);
 typedef int (*vhost_set_mem_table_op)(struct vhost_dev *dev,
-                                      struct vhost_memory *mem);
+		struct vhost_memory *mem);
 typedef int (*vhost_set_vring_addr_op)(struct vhost_dev *dev,
-                                       struct vhost_vring_addr *addr);
+		struct vhost_vring_addr *addr);
 typedef int (*vhost_set_vring_endian_op)(struct vhost_dev *dev,
-                                         struct vhost_vring_state *ring);
+		struct vhost_vring_state *ring);
 typedef int (*vhost_set_vring_num_op)(struct vhost_dev *dev,
-                                      struct vhost_vring_state *ring);
+		struct vhost_vring_state *ring);
 typedef int (*vhost_set_vring_base_op)(struct vhost_dev *dev,
-                                       struct vhost_vring_state *ring);
+		struct vhost_vring_state *ring);
 typedef int (*vhost_get_vring_base_op)(struct vhost_dev *dev,
-                                       struct vhost_vring_state *ring);
+		struct vhost_vring_state *ring);
 typedef int (*vhost_set_vring_kick_op)(struct vhost_dev *dev,
-                                       struct vhost_vring_file *file);
+		struct vhost_vring_file *file);
 typedef int (*vhost_set_vring_call_op)(struct vhost_dev *dev,
-                                       struct vhost_vring_file *file);
+		struct vhost_vring_file *file);
 typedef int (*vhost_set_vring_busyloop_timeout_op)(struct vhost_dev *dev,
-                                                   struct vhost_vring_state *r);
+		struct vhost_vring_state *r);
 typedef int (*vhost_set_features_op)(struct vhost_dev *dev,
-                                     uint64_t features);
+		uint64_t features);
 typedef int (*vhost_get_features_op)(struct vhost_dev *dev,
-                                     uint64_t *features);
+		uint64_t *features);
 typedef int (*vhost_set_owner_op)(struct vhost_dev *dev);
 typedef int (*vhost_reset_device_op)(struct vhost_dev *dev);
 typedef int (*vhost_get_vq_index_op)(struct vhost_dev *dev, int idx);
 typedef int (*vhost_set_vring_enable_op)(struct vhost_dev *dev,
-                                         int enable);
+		int enable);
 typedef bool (*vhost_requires_shm_log_op)(struct vhost_dev *dev);
 typedef int (*vhost_migration_done_op)(struct vhost_dev *dev,
-                                       char *mac_addr);
+		char *mac_addr);
 typedef bool (*vhost_backend_can_merge_op)(struct vhost_dev *dev,
-                                           uint64_t start1, uint64_t size1,
-                                           uint64_t start2, uint64_t size2);
+		uint64_t start1, uint64_t size1,
+		uint64_t start2, uint64_t size2);
 typedef int (*vhost_vsock_set_guest_cid_op)(struct vhost_dev *dev,
-                                            uint64_t guest_cid);
+		uint64_t guest_cid);
 typedef int (*vhost_vsock_set_running_op)(struct vhost_dev *dev, int start);
 typedef void (*vhost_set_iotlb_callback_op)(struct vhost_dev *dev,
-                                           int enabled);
+		int enabled);
 typedef int (*vhost_update_device_iotlb_op)(struct vhost_dev *dev,
-                                            uint64_t iova, uint64_t uaddr,
-                                            uint64_t len,
-                                            IOMMUAccessFlags perm);
+		uint64_t iova, uint64_t uaddr,
+		uint64_t len,
+		IOMMUAccessFlags perm);
 typedef int (*vhost_invalidate_device_iotlb_op)(struct vhost_dev *dev,
-                                                uint64_t iova, uint64_t len);
+		uint64_t iova, uint64_t len);
 
 typedef struct VhostOps {
-    VhostBackendType backend_type;
-    vhost_backend_init vhost_backend_init;
-    vhost_backend_cleanup vhost_backend_cleanup;
-    vhost_backend_memslots_limit vhost_backend_memslots_limit;
-    vhost_net_set_backend_op vhost_net_set_backend;
-    vhost_net_set_mtu_op vhost_net_set_mtu;
-    vhost_scsi_set_endpoint_op vhost_scsi_set_endpoint;
-    vhost_scsi_clear_endpoint_op vhost_scsi_clear_endpoint;
-    vhost_scsi_get_abi_version_op vhost_scsi_get_abi_version;
-    vhost_set_log_base_op vhost_set_log_base;
-    vhost_set_mem_table_op vhost_set_mem_table;
-    vhost_set_vring_addr_op vhost_set_vring_addr;
-    vhost_set_vring_endian_op vhost_set_vring_endian;
-    vhost_set_vring_num_op vhost_set_vring_num;
-    vhost_set_vring_base_op vhost_set_vring_base;
-    vhost_get_vring_base_op vhost_get_vring_base;
-    vhost_set_vring_kick_op vhost_set_vring_kick;
-    vhost_set_vring_call_op vhost_set_vring_call;
-    vhost_set_vring_busyloop_timeout_op vhost_set_vring_busyloop_timeout;
-    vhost_set_features_op vhost_set_features;
-    vhost_get_features_op vhost_get_features;
-    vhost_set_owner_op vhost_set_owner;
-    vhost_reset_device_op vhost_reset_device;
-    vhost_get_vq_index_op vhost_get_vq_index;
-    vhost_set_vring_enable_op vhost_set_vring_enable;
-    vhost_requires_shm_log_op vhost_requires_shm_log;
-    vhost_migration_done_op vhost_migration_done;
-    vhost_backend_can_merge_op vhost_backend_can_merge;
-    vhost_vsock_set_guest_cid_op vhost_vsock_set_guest_cid;
-    vhost_vsock_set_running_op vhost_vsock_set_running;
-    vhost_set_iotlb_callback_op vhost_set_iotlb_callback;
-    vhost_update_device_iotlb_op vhost_update_device_iotlb;
-    vhost_invalidate_device_iotlb_op vhost_invalidate_device_iotlb;
+	VhostBackendType backend_type;
+	vhost_backend_init vhost_backend_init;
+	vhost_backend_cleanup vhost_backend_cleanup;
+	vhost_backend_memslots_limit vhost_backend_memslots_limit;
+	vhost_net_set_backend_op vhost_net_set_backend;
+	vhost_net_set_mtu_op vhost_net_set_mtu;
+	vhost_scsi_set_endpoint_op vhost_scsi_set_endpoint;
+	vhost_scsi_clear_endpoint_op vhost_scsi_clear_endpoint;
+	vhost_scsi_get_abi_version_op vhost_scsi_get_abi_version;
+	vhost_set_log_base_op vhost_set_log_base;
+	vhost_set_mem_table_op vhost_set_mem_table;
+	vhost_set_vring_addr_op vhost_set_vring_addr;
+	vhost_set_vring_endian_op vhost_set_vring_endian;
+	vhost_set_vring_num_op vhost_set_vring_num;
+	vhost_set_vring_base_op vhost_set_vring_base;
+	vhost_get_vring_base_op vhost_get_vring_base;
+	vhost_set_vring_kick_op vhost_set_vring_kick;
+	vhost_set_vring_call_op vhost_set_vring_call;
+	vhost_set_vring_busyloop_timeout_op vhost_set_vring_busyloop_timeout;
+	vhost_set_features_op vhost_set_features;
+	vhost_get_features_op vhost_get_features;
+	vhost_set_owner_op vhost_set_owner;
+	vhost_reset_device_op vhost_reset_device;
+	vhost_get_vq_index_op vhost_get_vq_index;
+	vhost_set_vring_enable_op vhost_set_vring_enable;
+	vhost_requires_shm_log_op vhost_requires_shm_log;
+	vhost_migration_done_op vhost_migration_done;
+	vhost_backend_can_merge_op vhost_backend_can_merge;
+	vhost_vsock_set_guest_cid_op vhost_vsock_set_guest_cid;
+	vhost_vsock_set_running_op vhost_vsock_set_running;
+	vhost_set_iotlb_callback_op vhost_set_iotlb_callback;
+	vhost_update_device_iotlb_op vhost_update_device_iotlb;
+	vhost_invalidate_device_iotlb_op vhost_invalidate_device_iotlb;
 } VhostOps;
 
 extern const VhostOps user_ops;
 
 int vhost_set_backend_type(struct vhost_dev *dev,
-                           VhostBackendType backend_type);
+		VhostBackendType backend_type);
 
 #endif /* VHOST_BACKEND_H */
diff -rupN /home/prafull/Desktop/qemu-2.9.0/include/hw/virtio/virtio-blk.h /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/hw/virtio/virtio-blk.h
--- /home/prafull/Desktop/qemu-2.9.0/include/hw/virtio/virtio-blk.h	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/hw/virtio/virtio-blk.h	2018-05-28 13:06:18.000000000 +0530
@@ -44,7 +44,8 @@ struct VirtIOBlkConf
 struct VirtIOBlockDataPlane;
 
 struct VirtIOBlockReq;
-typedef struct VirtIOBlock {
+typedef struct VirtIOBlock 
+{
     VirtIODevice parent_obj;
     BlockBackend *blk;
     void *rq;
@@ -56,9 +57,14 @@ typedef struct VirtIOBlock {
     bool dataplane_disabled;
     bool dataplane_started;
     struct VirtIOBlockDataPlane *dataplane;
+
+    /*  unaisp  */
+    int vssd_device;
+    /*  end*/
 } VirtIOBlock;
 
-typedef struct VirtIOBlockReq {
+typedef struct VirtIOBlockReq 
+{
     VirtQueueElement elem;
     int64_t sector_num;
     VirtIOBlock *dev;
diff -rupN /home/prafull/Desktop/qemu-2.9.0/include/hw/virtio/virtio.h /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/hw/virtio/virtio.h
--- /home/prafull/Desktop/qemu-2.9.0/include/hw/virtio/virtio.h	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/hw/virtio/virtio.h	2018-05-28 13:06:18.000000000 +0530
@@ -245,6 +245,8 @@ int virtio_set_features(VirtIODevice *vd
 
 /* Base devices.  */
 typedef struct VirtIOBlkConf VirtIOBlkConf;
+typedef struct VirtIOVssdConf VirtIOVssdConf;
+
 struct virtio_net_conf;
 typedef struct virtio_serial_conf virtio_serial_conf;
 typedef struct virtio_input_conf virtio_input_conf;
diff -rupN /home/prafull/Desktop/qemu-2.9.0/include/hw/virtio/virtio-vssd.h /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/hw/virtio/virtio-vssd.h
--- /home/prafull/Desktop/qemu-2.9.0/include/hw/virtio/virtio-vssd.h	1970-01-01 05:30:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/hw/virtio/virtio-vssd.h	2018-05-28 13:06:18.000000000 +0530
@@ -0,0 +1,188 @@
+/*
+ * Virtio Block Device
+ *
+ * Copyright IBM, Corp. 2007
+ *
+ * Authors:
+ *  Anthony Liguori   <aliguori@us.ibm.com>
+ *
+ * This work is licensed under the terms of the GNU GPL, version 2.  See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#ifndef QEMU_VIRTIO_VSSD_H
+#define QEMU_VIRTIO_VSSD_H
+
+#define SSD_BALLOON_UNIT 2046
+#define SSD_BALLOON_INFLATION '-'
+#define SSD_BALLOON_DEFLATION '+'
+#define SSD_BLOCK_SIZE 2048    // 1 block = 2048 sectors.  ToDo: Find best block size. 
+#define VSSD_DEBUG_MODE 0
+
+
+#include "hw/virtio/virtio.h"
+#include "hw/block/block.h"
+#include "sysemu/iothread.h"
+#include "sysemu/block-backend.h"
+#include "standard-headers/linux/virtio_vssd.h"
+
+#define TYPE_VIRTIO_VSSD "virtio-vssd-device"
+#define VIRTIO_VSSD(obj) \
+        OBJECT_CHECK(VirtIOVssd, (obj), TYPE_VIRTIO_VSSD)
+
+/* This is the last element of the write scatter-gather list */
+struct virtio_vssd_inhdr
+{
+    unsigned char status;
+};
+
+/*
+ * This comes first in the read scatter-gather list.
+ * For legacy virtio, if VIRTIO_F_ANY_LAYOUT is not negotiated,
+ * this is the first element of the read scatter-gather list.
+ */
+struct virtio_vssd_outhdr {
+    /* VIRTIO_VSSD_T* */
+    __virtio32 type;
+    /* io priority. */
+    __virtio32 ioprio;
+    /* Sector (ie. 512 byte offset) */
+    __virtio64 sector;
+};
+
+
+struct VirtIOVssdConf
+{
+    BlockConf conf;
+    IOThread *iothread;
+    char *serial;
+    uint32_t scsi;
+    uint32_t config_wce;
+    uint32_t request_merging;
+    uint16_t num_queues;
+};
+
+struct VirtIOVssdDataPlane;
+
+
+typedef struct VirtIOVssd 
+{
+    VirtIODevice parent_obj;
+    BlockBackend *blk;
+    void *rq;
+    QEMUBH *bh;
+    VirtIOVssdConf conf;
+    unsigned short sector_mask;
+    bool original_wce;
+    VMChangeStateEntry *change;
+    bool dataplane_disabled;
+    bool dataplane_started;
+    struct VirtIOVssdDataPlane *dataplane;
+
+    // new variables
+    uint64_t capacity;  //  Capacity of front-end device(vssd) in sectors;
+    uint64_t current_capacity;  // in sectors
+    uint32_t *bitmap;   //  Allocate memory on realize
+                        //  Set / unset on realize
+                        //  copy the list conetent to virtio_vssd_config on update_config.
+                        //  Do I need to copy?  YES (:
+                        //  can virtio_vssd_config->bitmap point to vssd->bitmap ??  NO 
+    VirtQueue *ctrl_vq;
+    VirtQueueElement *ctrl_vq_elem;
+    int64_t command;
+    uint64_t *block_list;
+
+    //new variables to communicate with cm
+    int vm_id;
+    char vm_name[25];
+    void *message;
+    pthread_t listener_thread_ID;
+    FILE *log_file;
+    FILE *summary_file;
+
+    //Variables for debugging
+    pthread_mutex_t lock;
+
+
+} VirtIOVssd;
+
+struct statistics
+{
+    //  Related to one VirtioVSSD-request
+    unsigned long int qiov_size;
+    unsigned long int qiov_iov_count;
+
+    //  After merging multiple requests
+    unsigned long int qiov_merge_count;
+    unsigned long int qiov_mereg_size;          //  Number of QIOVs merged 
+    unsigned long int qiov_merg_iov_count;
+
+    //  After dividing merged requests
+    unsigned long int sub_qiov_count;   // 1 = Not crossing the boundary
+
+    //  Time frame
+    unsigned long long request_pull_time;              //  First event
+    unsigned long long adding_to_mrb_time;
+    unsigned long long callback_time;                  //  Time at which call back for last sub-qiov received
+    unsigned long long request_push_time;              //  Final event
+
+    unsigned long long qiov_merging_start_time;
+    unsigned long long qiov_division_start_time;       //  Merging end time
+    unsigned long long first_sub_qiov_send_time;
+    unsigned long long last_sub_qiov_send_time;        //  Division end time
+    unsigned long long first_sub_qiov_callback_time;
+    unsigned long long last_sub_qiov_callback_time;    //  Request call back time
+};
+
+struct VirtIOVssdReq;
+typedef struct VirtIOVssdReq 
+{
+    VirtQueueElement elem;
+    int64_t sector_num;
+    VirtIOVssd *dev;
+    VirtQueue *vq;
+    struct virtio_vssd_inhdr *in;
+    struct virtio_vssd_outhdr out;
+    QEMUIOVector qiov;
+    size_t in_len;
+    struct VirtIOVssdReq *next;
+    struct VirtIOVssdReq *mr_next;
+    BlockAcctCookie acct;
+
+    //New variables
+    int sub_qiov_count;
+    int sub_qiov_finished;
+    pthread_spinlock_t lock;       //  Lock is used to protect the above two variables
+    struct statistics stats;
+
+} VirtIOVssdReq;
+
+#define VIRTIO_VSSD_MAX_MERGE_REQS 32
+
+typedef struct MultiVssdReqBuffer 
+{
+    VirtIOVssdReq *reqs[VIRTIO_VSSD_MAX_MERGE_REQS];
+    unsigned int num_reqs;
+    bool is_write;
+} MultiVssdReqBuffer;
+
+typedef struct VirtIOVssdResizeInfo 
+{
+    int32_t status;
+    int32_t ack;
+    int64_t block_list[SSD_BALLOON_UNIT];
+
+    int32_t total_requested_blocks;
+    int32_t current_requested_blocks;
+    int32_t given_blocks;
+    int32_t remaining_blocks;
+    char operation;      //      SSD_BALLOON_DELATION, SSD_BALLOON_INFLATION
+    int flag;
+
+} VirtIOVssdResizeInfo;
+
+
+bool virtio_vssd_handle_vq(VirtIOVssd *s, VirtQueue *vq);
+
+#endif
diff -rupN /home/prafull/Desktop/qemu-2.9.0/include/qemu/osdep.h /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/qemu/osdep.h
--- /home/prafull/Desktop/qemu-2.9.0/include/qemu/osdep.h	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/qemu/osdep.h	2018-05-28 13:06:18.000000000 +0530
@@ -35,6 +35,8 @@
 #endif
 #include "qemu/compiler.h"
 
+
+
 /* Older versions of C++ don't get definitions of various macros from
  * stdlib.h unless we define these macros before first inclusion of
  * that system header.
@@ -386,6 +388,16 @@ static inline void qemu_timersub(const s
 #define qemu_timersub timersub
 #endif
 
+/*  Added by Unais*/
+extern bool vssd_persist_full;
+extern char vssd_vm_name[25];
+extern int vssd_current_allocate;
+extern int vssd_current_persist;
+extern int vssd_size;
+extern int vssd_vm_id;
+
+/* End*/
+
 void qemu_set_cloexec(int fd);
 
 /* Starting on QEMU 2.5, qemu_hw_version() returns "2.5+" by default
@@ -468,3 +480,5 @@ char *qemu_get_pid_name(pid_t pid);
 pid_t qemu_fork(Error **errp);
 
 #endif
+
+
diff -rupN /home/prafull/Desktop/qemu-2.9.0/include/standard-headers/linux/virtio_ids.h /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/standard-headers/linux/virtio_ids.h
--- /home/prafull/Desktop/qemu-2.9.0/include/standard-headers/linux/virtio_ids.h	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/standard-headers/linux/virtio_ids.h	2018-05-28 13:06:18.000000000 +0530
@@ -43,5 +43,12 @@
 #define VIRTIO_ID_INPUT        18 /* virtio input */
 #define VIRTIO_ID_VSOCK        19 /* virtio vsock transport */
 #define VIRTIO_ID_CRYPTO       20 /* virtio crypto */
+/* Added by Bhavesh Singh. 2017.02.03. Begin add */
+/* It might not be immediately apparent here but this is used to set
+ * the Virtio Device Id, which, in turn, is used to set the PCI subsystem ID
+ * which is used by the frontend linux virtio driver to set the PCI device id
+ * ignoring the PCI device ID, it uses the subsystem ID. */
+#define VIRTIO_ID_VSSD         54 /* virtio virtual ssd */
+/* Added by Bhavesh Singh. 2017.02.03. End add */
 
 #endif /* _LINUX_VIRTIO_IDS_H */
diff -rupN /home/prafull/Desktop/qemu-2.9.0/include/standard-headers/linux/virtio_ids.h.orig /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/standard-headers/linux/virtio_ids.h.orig
--- /home/prafull/Desktop/qemu-2.9.0/include/standard-headers/linux/virtio_ids.h.orig	1970-01-01 05:30:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/standard-headers/linux/virtio_ids.h.orig	2018-05-28 13:06:18.000000000 +0530
@@ -0,0 +1,47 @@
+#ifndef _LINUX_VIRTIO_IDS_H
+#define _LINUX_VIRTIO_IDS_H
+/*
+ * Virtio IDs
+ *
+ * This header is BSD licensed so anyone can use the definitions to implement
+ * compatible drivers/servers.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the name of IBM nor the names of its contributors
+ *    may be used to endorse or promote products derived from this software
+ *    without specific prior written permission.
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL IBM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE. */
+
+#define VIRTIO_ID_NET		1 /* virtio net */
+#define VIRTIO_ID_BLOCK		2 /* virtio block */
+#define VIRTIO_ID_CONSOLE	3 /* virtio console */
+#define VIRTIO_ID_RNG		4 /* virtio rng */
+#define VIRTIO_ID_BALLOON	5 /* virtio balloon */
+#define VIRTIO_ID_RPMSG		7 /* virtio remote processor messaging */
+#define VIRTIO_ID_SCSI		8 /* virtio scsi */
+#define VIRTIO_ID_9P		9 /* 9p virtio console */
+#define VIRTIO_ID_RPROC_SERIAL 11 /* virtio remoteproc serial link */
+#define VIRTIO_ID_CAIF	       12 /* Virtio caif */
+#define VIRTIO_ID_GPU          16 /* virtio GPU */
+#define VIRTIO_ID_INPUT        18 /* virtio input */
+#define VIRTIO_ID_VSOCK        19 /* virtio vsock transport */
+#define VIRTIO_ID_CRYPTO       20 /* virtio crypto */
+
+#endif /* _LINUX_VIRTIO_IDS_H */
diff -rupN /home/prafull/Desktop/qemu-2.9.0/include/standard-headers/linux/virtio_ids.h.rej /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/standard-headers/linux/virtio_ids.h.rej
--- /home/prafull/Desktop/qemu-2.9.0/include/standard-headers/linux/virtio_ids.h.rej	1970-01-01 05:30:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/standard-headers/linux/virtio_ids.h.rej	2018-05-28 13:06:18.000000000 +0530
@@ -0,0 +1,17 @@
+--- include/standard-headers/linux/virtio_ids.h	2016-12-20 21:46:42.000000000 +0530
++++ qemu/qemu-2.9.0/include/standard-headers/linux/virtio_ids.h	2017-03-20 11:53:21.195951114 +0530
+@@ -41,6 +41,14 @@
+ #define VIRTIO_ID_CAIF	       12 /* Virtio caif */
+ #define VIRTIO_ID_GPU          16 /* virtio GPU */
+ #define VIRTIO_ID_INPUT        18 /* virtio input */
+ #define VIRTIO_ID_VSOCK        19 /* virtio vsock transport */
+ #define VIRTIO_ID_CRYPTO       20 /* virtio crypto */
++
++/* Added by Bhavesh Singh. 2017.02.03. Begin add */
++/* It might not be immediately apparent here but this is used to set
++ * the Virtio Device Id, which, in turn, is used to set the PCI subsystem ID
++ * which is used by the frontend linux virtio driver to set the PCI device id
++ * ignoring the PCI device ID, it uses the subsystem ID. */
++#define VIRTIO_ID_VSSD         54 /* virtio virtual ssd */
++/* Added by Bhavesh Singh. 2017.02.03. End add */
+ #endif /* _LINUX_VIRTIO_IDS_H */
diff -rupN /home/prafull/Desktop/qemu-2.9.0/include/standard-headers/linux/virtio_vssd.h /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/standard-headers/linux/virtio_vssd.h
--- /home/prafull/Desktop/qemu-2.9.0/include/standard-headers/linux/virtio_vssd.h	1970-01-01 05:30:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/standard-headers/linux/virtio_vssd.h	2018-05-28 13:06:18.000000000 +0530
@@ -0,0 +1,155 @@
+#ifndef _LINUX_VIRTIO_BLK_H
+#define _LINUX_VIRTIO_BLK_H
+/* This header is BSD licensed so anyone can use the definitions to implement
+ * compatible drivers/servers.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the name of IBM nor the names of its contributors
+ *    may be used to endorse or promote products derived from this software
+ *    without specific prior written permission.
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL IBM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE. */
+#include "standard-headers/linux/types.h"
+#include "standard-headers/linux/virtio_ids.h"
+#include "standard-headers/linux/virtio_config.h"
+#include "standard-headers/linux/virtio_types.h"
+
+/* Feature bits */
+#define VIRTIO_VSSD_F_SIZE_MAX	1	/* Indicates maximum segment size */
+#define VIRTIO_VSSD_F_SEG_MAX	2	/* Indicates maximum # of segments */
+#define VIRTIO_VSSD_F_GEOMETRY	4	/* Legacy geometry available  */
+#define VIRTIO_VSSD_F_RO		5	/* Disk is read-only */
+#define VIRTIO_VSSD_F_BLK_SIZE	6	/* Block size of disk is available*/
+#define VIRTIO_VSSD_F_TOPOLOGY	10	/* Topology information is available */
+#define VIRTIO_VSSD_F_MQ		12	/* support more than one vq */
+
+/* Legacy feature bits */
+#ifndef VIRTIO_BLK_NO_LEGACY
+#define VIRTIO_VSSD_F_BARRIER	0	/* Does host support barriers? */
+#define VIRTIO_VSSD_F_SCSI	7	/* Supports scsi command passthru */
+#define VIRTIO_VSSD_F_FLUSH	9	/* Flush command supported */
+#define VIRTIO_VSSD_F_CONFIG_WCE	11	/* Writeback mode available in config */
+/* Old (deprecated) name for VIRTIO_VSSD_F_FLUSH. */
+#define VIRTIO_VSSD_F_WCE VIRTIO_VSSD_F_FLUSH
+#endif /* !VIRTIO_BLK_NO_LEGACY */
+
+#define VIRTIO_VSSD_ID_BYTES	20	/* ID string length */
+
+#define SECTORS_IN_A_GB 2097152
+
+struct virtio_vssd_config {
+	/* The capacity (in 512-byte sectors). */
+	uint64_t capacity;
+	/* The maximum segment size (if VIRTIO_VSSD_F_SIZE_MAX) */
+	uint32_t size_max;
+	/* The maximum number of segments (if VIRTIO_VSSD_F_SEG_MAX) */
+	uint32_t seg_max;
+	/* geometry of the device (if VIRTIO_VSSD_F_GEOMETRY) */
+	struct virtio_vssd_geometry {
+		uint16_t cylinders;
+		uint8_t heads;
+		uint8_t sectors;
+	} geometry;
+
+	/* block size of device (if VIRTIO_VSSD_F_BLK_SIZE) */
+	uint32_t blk_size;
+
+	/* the next 4 entries are guarded by VIRTIO_VSSD_F_TOPOLOGY  */
+	/* exponent for physical block per logical block. */
+	uint8_t physical_block_exp;
+	/* alignment offset in logical blocks. */
+	uint8_t alignment_offset;
+	/* minimum I/O size without performance penalty in logical blocks. */
+	uint16_t min_io_size;
+	/* optimal sustained I/O size in logical blocks. */
+	uint32_t opt_io_size;
+
+	/* writeback mode (if VIRTIO_VSSD_F_CONFIG_WCE) */
+	uint8_t wce;
+	uint8_t unused;
+
+	/* number of vqs, only available when VIRTIO_VSSD_F_MQ is set */
+	uint16_t num_queues;
+
+	/*New Varialble unais*/
+	uint64_t current_capacity;
+	//uint32_t bitmap[1* SECTORS_IN_A_GB/32];
+	//uint32_t bitmap[8192];
+	int32_t command;
+
+	// struct vssd_bitmap
+	// {
+	uint32_t bitmap_offset;
+	uint32_t bitmap_reading;
+	uint32_t bitmap_value;		//400 Bytes.	3200 Blocks
+	uint32_t current_request_id;
+	// } bitmap;
+
+
+} QEMU_PACKED;
+
+/*
+ * Command types
+ *
+ * Usage is a bit tricky as some bits are used as flags and some are not.
+ *
+ * Rules:
+ *   VIRTIO_VSSD_T_OUT may be combined with VIRTIO_VSSD_T_SCSI_CMD or
+ *   VIRTIO_VSSD_T_BARRIER.  VIRTIO_VSSD_T_FLUSH is a command of its own
+ *   and may not be combined with any of the other flags.
+ */
+
+/* These two define direction. */
+#define VIRTIO_VSSD_T_IN		0
+#define VIRTIO_VSSD_T_OUT	1
+
+#ifndef VIRTIO_VSSD_NO_LEGACY
+/* This bit says it's a scsi command, not an actual read or write. */
+#define VIRTIO_VSSD_T_SCSI_CMD	2
+#endif /* VIRTIO_BLK_NO_LEGACY */
+
+/* Cache flush command */
+#define VIRTIO_VSSD_T_FLUSH	4
+
+/* Get device ID command */
+#define VIRTIO_VSSD_T_GET_ID    8
+
+#ifndef VIRTIO_VSSD_NO_LEGACY
+/* Barrier before this op. */
+#define VIRTIO_VSSD_T_BARRIER	0x80000000
+#endif /* !VIRTIO_BLK_NO_LEGACY */
+
+
+
+
+
+#ifndef VIRTIO_VSSD_NO_LEGACY
+struct virtio_scsi_inhdr {
+	__virtio32 errors;
+	__virtio32 data_len;
+	__virtio32 sense_len;
+	__virtio32 residual;
+};
+#endif /* !VIRTIO_BLK_NO_LEGACY */
+
+/* And this is the final byte of the write scatter-gather list. */
+#define VIRTIO_VSSD_S_OK		0
+#define VIRTIO_VSSD_S_IOERR	1
+#define VIRTIO_VSSD_S_UNSUPP	2
+#endif /* _LINUX_VIRTIO_BLK_H */
diff -rupN /home/prafull/Desktop/qemu-2.9.0/include/sysemu/balloon.h /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/sysemu/balloon.h
--- /home/prafull/Desktop/qemu-2.9.0/include/sysemu/balloon.h	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/include/sysemu/balloon.h	2018-05-28 13:06:18.000000000 +0530
@@ -17,6 +17,13 @@
 #include "qapi-types.h"
 
 typedef void (QEMUBalloonEvent)(void *opaque, ram_addr_t target);
+
+/* Added by Bhavesh Singh. 2017.06.02. Begin add */
+typedef void (QEMUSSDBalloonEvent)(void *opaque, int64_t target);
+int qemu_add_ssd_balloon_handler(QEMUSSDBalloonEvent *event_func, void *opaque);
+void qemu_remove_ssd_balloon_handler(void *opaque);
+/* Added by Bhavesh Singh. 2017.06.02. End add */
+
 typedef void (QEMUBalloonStatus)(void *opaque, BalloonInfo *info);
 
 int qemu_add_balloon_handler(QEMUBalloonEvent *event_func,
diff -rupN /home/prafull/Desktop/qemu-2.9.0/Makefile /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/Makefile
--- /home/prafull/Desktop/qemu-2.9.0/Makefile	2017-04-20 20:27:00.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/Makefile	2018-05-28 13:06:05.000000000 +0530
@@ -1,5 +1,4 @@
 # Makefile for QEMU.
-
 # Always point to the root of the build tree (needs GNU make).
 BUILD_DIR=$(CURDIR)
 
diff -rupN /home/prafull/Desktop/qemu-2.9.0/qdev-monitor.c /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/qdev-monitor.c
--- /home/prafull/Desktop/qemu-2.9.0/qdev-monitor.c	2017-04-20 20:27:01.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/qdev-monitor.c	2018-05-28 13:06:05.000000000 +0530
@@ -74,6 +74,9 @@ static const QDevAlias qdev_alias_table[
     { "virtio-serial-pci", "virtio-serial", QEMU_ARCH_ALL & ~QEMU_ARCH_S390X },
     { "virtio-tablet-ccw", "virtio-tablet", QEMU_ARCH_S390X },
     { "virtio-tablet-pci", "virtio-tablet", QEMU_ARCH_ALL & ~QEMU_ARCH_S390X },
+    /* Added by Bhavesh Singh. 2017.02.05. Begin add */
+    {"virtio-vssd-pci", "virtio-vssd", QEMU_ARCH_ALL & ~QEMU_ARCH_S390X },
+    /* Added by Bhavesh Singh. 2017.02.05. End add */
     { }
 };
 
@@ -567,6 +570,7 @@ DeviceState *qdev_device_add(QemuOpts *o
     Error *err = NULL;
 
     driver = qemu_opt_get(opts, "driver");
+    printf("\tdriver = %s \n", driver);
     if (!driver) {
         error_setg(errp, QERR_MISSING_PARAMETER, "driver");
         return NULL;
diff -rupN /home/prafull/Desktop/qemu-2.9.0/vl.c /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/vl.c
--- /home/prafull/Desktop/qemu-2.9.0/vl.c	2017-04-20 20:27:01.000000000 +0530
+++ /home/prafull/Desktop/prafull/vssd_stats/qemu-2.9.0/vl.c	2018-05-28 13:06:05.000000000 +0530
@@ -133,6 +133,16 @@ int main(int argc, char **argv)
 #define MAX_VIRTIO_CONSOLES 1
 #define MAX_SCLP_CONSOLES 1
 
+/*Added by unais*/
+int vssd_vm_id;
+int vssd_size;
+char vssd_vm_name[25];
+int vssd_current_allocate;
+int vssd_current_persist;
+bool vssd_persist_full;
+/*ENnd */
+
+
 static const char *data_dir[16];
 static int data_dir_idx;
 const char *bios_name = NULL;
